{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.8.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.12.43)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.41)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2019.8.19)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.43.0)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers) (1.15.43)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import tokenizers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla V100-SXM2-16GB\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    MAX_LEN = 128 # no text is longer than 128\n",
    "    TRAIN_BATCH_SIZE = 64\n",
    "    VALID_BATCH_SIZE = 16\n",
    "    EPOCHS = 5\n",
    "    BASE_PATH = Path(\"../\")\n",
    "    BERT_PATH = BASE_PATH  / \"bert-base-uncased\"\n",
    "    BERT_NAME = \"bert-base-uncased\"\n",
    "    MODEL_PATH = BASE_PATH  / \"model_save/model_0415-1.bin\"\n",
    "    TRAINING_FILE = BASE_PATH / \"input/train.csv\"\n",
    "    TESTING_FILE = BASE_PATH  / \"input/test.csv\"\n",
    "    TOKENIZER = tokenizers.BertWordPieceTokenizer(\n",
    "        os.path.join(BERT_PATH, 'bert-base-uncased-vocab.txt'),\n",
    "        lowercase=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_NAME)\n",
    "        #self.bert_drop = nn.Dropout(0.3)\n",
    "        self.l0 = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "    #def forward(self, ids, mask, token_type_ids):\n",
    "        # not using sentiment at all\n",
    "        sequence_output, pooled_output = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # sequence_output:  (batch_size, num_tokens or MAX_LEN?, 768)\n",
    "        # pooled_output: Last layer hidden-state of the first token of the sequence (classification token)\n",
    "        logits = self.l0(sequence_output)\n",
    "        # (batch_size, num_tokens, 2)\n",
    "\n",
    "        # (batch_size, num_tokens, 1), (batch_size, num_tokens, 1)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        # (batch_size, num_tokens), (batch_size, num_tokens)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, sentiment, selected_text):\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.selected_text = selected_text\n",
    "        self.max_len = config.MAX_LEN\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, item): # item is index value\n",
    "        # remove redundant spaces\n",
    "        tweet = \" \".join(str(self.tweet[item]).split())\n",
    "        selected_text = \" \".join(str(self.selected_text[item]).split())\n",
    "\n",
    "        len_sel_text = len(selected_text)\n",
    "        idx0 = -1\n",
    "        idx1 = -1\n",
    "        # is it possible that there are two matches?\n",
    "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n",
    "            if tweet[ind: ind + len_sel_text] == selected_text:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_sel_text - 1\n",
    "                break\n",
    "\n",
    "        # [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != -1 and idx1 != -1:\n",
    "            for j in range(idx0, idx1 + 1):\n",
    "                if tweet[j] != \" \":\n",
    "                    char_targets[j] = 1\n",
    "        # [0,0,0,0,0,1,1,1,1,0,1,1,1,0,0,0,0,0,0,0]\n",
    "\n",
    "        # https://github.com/huggingface/tokenizers\n",
    "        tok_tweet = self.tokenizer.encode(tweet)\n",
    "        tok_tweet_tokens = tok_tweet.tokens\n",
    "        tok_tweet_ids = tok_tweet.ids\n",
    "        tok_tweet_offsets = tok_tweet.offsets[1:-1]\n",
    "\n",
    "        # [Not clear] partial match?\n",
    "        # need to check if there are partial words in selected_text\n",
    "        targets = [0] * (len(tok_tweet_tokens) - 2) # -2 for cls, sep\n",
    "        # [0,0,0,0,0,0,0]\n",
    "        for j, (offset1, offset2) in enumerate(tok_tweet_offsets):\n",
    "            if sum(char_targets[offset1:offset2]) > 0:\n",
    "                targets[j] = 1\n",
    "        # [0,1,1,1,0,0,0]\n",
    "        targets = [0] + targets + [0] # cls, sep\n",
    "        targets_start = [0] * len(targets) # [0,1,0,0,0,0,0]\n",
    "        targets_end = [0] * len(targets)   # [0,0,0,1,0,0,0]\n",
    "\n",
    "        non_zero = np.nonzero(targets)[0]\n",
    "        if len(non_zero) > 0:\n",
    "            targets_start[non_zero[0]] = 1\n",
    "            targets_end[non_zero[-1]] = 1\n",
    "\n",
    "        mask = [1] * len(tok_tweet_ids)\n",
    "        token_type_ids = [0] * len(tok_tweet_ids)\n",
    "\n",
    "        padding_len = self.max_len - len(tok_tweet_ids)\n",
    "        ids = tok_tweet_ids + [0] * padding_len\n",
    "        mask = mask + [0] * padding_len\n",
    "        token_type_ids = token_type_ids + [0] * padding_len\n",
    "        targets = targets + [0] * padding_len\n",
    "        targets_start = targets_start + [0] * padding_len\n",
    "        targets_end  = targets_end + [0] * padding_len\n",
    "\n",
    "        sentiment = [1, 0, 0]\n",
    "        if self.sentiment[item] == 'positive':\n",
    "            sentiment = [0, 0, 1]\n",
    "        if self.sentiment[item] == 'negative':\n",
    "            sentiment = [0, 1, 0]\n",
    "\n",
    "        return {\n",
    "            'ids':torch.tensor(ids, dtype=torch.long),\n",
    "            'mask':torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids':torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets':torch.tensor(targets, dtype=torch.long),\n",
    "            'targets_start':torch.tensor(targets_start, dtype=torch.long),\n",
    "            'targets_end':torch.tensor(targets_end, dtype=torch.long),\n",
    "            'padding_len':torch.tensor(padding_len, dtype=torch.long),\n",
    "            'tweet_tokens':\" \".join(tok_tweet_tokens),\n",
    "            'orig_tweet':self.tweet[item],\n",
    "            'sentiment':torch.tensor(sentiment, dtype=torch.long),\n",
    "            'orig_sentiment':self.sentiment[item],\n",
    "            'orig_selected':self.selected_text[item]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(o1, o2, t1, t2): # start_logits, end_logits, targets_start, targets_end\n",
    "    l1 = nn.BCEWithLogitsLoss()(o1, t1)\n",
    "    l2 = nn.BCEWithLogitsLoss()(o2, t2)\n",
    "    return l1 + l2\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.float)\n",
    "        targets_end = targets_end.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        start_logits, end_logits = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(start_logits, end_logits, targets_start, targets_end)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        tk0.set_postfix(loss=losses.avg)\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_outputs_start = []\n",
    "    fin_outputs_end = []\n",
    "    fin_padding_lens = []\n",
    "    fin_tweet_tokens = []\n",
    "    fin_orig_sentiment = []\n",
    "    fin_orig_selected = []\n",
    "    fin_orig_tweet = []\n",
    "\n",
    "    for bi, d in enumerate(data_loader):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        tweet_tokens = d[\"tweet_tokens\"]\n",
    "        padding_len = d[\"padding_len\"]\n",
    "        orig_sentiment = d[\"orig_sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "\n",
    "        start_logits, end_logits = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        fin_outputs_start.append(torch.sigmoid(start_logits).cpu().detach().numpy())\n",
    "        fin_outputs_end.append(torch.sigmoid(end_logits).cpu().detach().numpy())\n",
    "        fin_padding_lens.extend(padding_len.cpu().detach().numpy().tolist())\n",
    "\n",
    "        fin_tweet_tokens.extend(tweet_tokens)\n",
    "        fin_orig_sentiment.extend(orig_sentiment)\n",
    "        fin_orig_selected.extend(orig_selected)\n",
    "        fin_orig_tweet.extend(orig_tweet)\n",
    "\n",
    "    fin_outputs_start = np.vstack(fin_outputs_start)\n",
    "    fin_outputs_end = np.vstack(fin_outputs_end)\n",
    "\n",
    "    threshold = 0.3\n",
    "    jaccards = []\n",
    "    for j in range(len(fin_tweet_tokens)):\n",
    "        target_string = fin_orig_selected[j]\n",
    "        tweet_tokens = fin_tweet_tokens[j]\n",
    "        padding_len = fin_padding_lens[j]\n",
    "        original_tweet = fin_orig_tweet[j]\n",
    "        sentiment = fin_orig_sentiment[j]\n",
    "\n",
    "        if padding_len > 0:\n",
    "            mask_start = fin_outputs_start[j,:][:-padding_len] >= threshold\n",
    "            mask_end = fin_outputs_end[j,:][:-padding_len] >= threshold\n",
    "        else:\n",
    "            mask_start = fin_outputs_start[j,:] >= threshold\n",
    "            mask_end = fin_outputs_end[j,:] >= threshold\n",
    "\n",
    "        mask = [0]*len(mask_start)\n",
    "        idx_start = np.nonzero(mask_start)[0]\n",
    "        idx_end = np.nonzero(mask_end)[0]\n",
    "\n",
    "        if len(idx_start) > 0:\n",
    "            idx_start = idx_start[0]\n",
    "            if len(idx_end) > 0:\n",
    "                idx_end = idx_end[0]\n",
    "            else:\n",
    "                idx_end = idx_start\n",
    "        else:\n",
    "            idx_start = 0\n",
    "            idx_end = 0\n",
    "\n",
    "        for mj in range(idx_start, idx_end + 1):\n",
    "            mask[mj] = 1\n",
    "\n",
    "        output_tokens = [x for p,x in enumerate(tweet_tokens.split()) if mask[p] == 1]\n",
    "        output_tokens = [x for x in output_tokens if x not in ('[CLS]', '[SEP]')]\n",
    "\n",
    "        final_output = \"\"\n",
    "        for ot in output_tokens:\n",
    "            if ot.startswith('##'):\n",
    "                final_output = final_output + ot[2:]\n",
    "            elif len(ot) == 1 and ot in string.punctuation:\n",
    "                final_output = final_output + ot\n",
    "            else: final_output = final_output + \" \" + ot\n",
    "        final_output = final_output.strip()\n",
    "        # this can be improved \n",
    "        if sentiment == 'neutral' or len(original_tweet.split()) < 4:\n",
    "            final_output = original_tweet\n",
    "        jac = jaccard(target_string.strip(), final_output.strip())\n",
    "        jaccards.append(jac)\n",
    "    mean_jac = np.mean(jaccards)\n",
    "    return mean_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    #dfx = pd.read_csv(config.TRAINING_FILE, nrows=100).dropna().reset_index(drop=True)\n",
    "    dfx = pd.read_csv(config.TRAINING_FILE).dropna().reset_index(drop=True)\n",
    "\n",
    "    df_train, df_valid = model_selection.train_test_split(\n",
    "        dfx,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=dfx.sentiment.values\n",
    "    )\n",
    "\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TweetDataset(\n",
    "        tweet=df_train.text.values,\n",
    "        sentiment=df_train.sentiment.values,\n",
    "        selected_text=df_train.selected_text.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = TweetDataset(\n",
    "        tweet=df_valid.text.values,\n",
    "        sentiment=df_valid.sentiment.values,\n",
    "        selected_text=df_valid.selected_text.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = BERTBaseUncased()\n",
    "    model.to(device)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    best_jaccard = 0\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        jaccard = eval_fn(valid_data_loader, model, device)\n",
    "        print(f\"Jaccard Score = {jaccard}\")\n",
    "        if jaccard > best_jaccard:\n",
    "            torch.save(model.state_dict(), config.MODEL_PATH)\n",
    "            best_jaccard = jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [02:45<00:00,  2.34it/s, loss=0.0557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Score = 0.5830396511523096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [02:45<00:00,  2.34it/s, loss=0.0298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Score = 0.6100185495117904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [02:45<00:00,  2.34it/s, loss=0.0271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Score = 0.6146267574918401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [02:45<00:00,  2.34it/s, loss=0.0256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Score = 0.6223177109121496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387/387 [02:45<00:00,  2.34it/s, loss=0.0244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Score = 0.6270922695832994\n",
      "CPU times: user 10min 38s, sys: 3min 52s, total: 14min 30s\n",
      "Wall time: 14min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
