{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"XLNet_model_0602_2.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7jaLISF2magZ","colab_type":"text"},"source":["Code adapted from:\n","* https://www.youtube.com/watch?v=U51ranzJBpY [ TOKENISER ]\n","* https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch [ TRAINING ]\n","* https://www.kaggle.com/abhishek/roberta-inference-5-folds [ INFERENCE ] \n","* https://www.kaggle.com/masterscrat/detect-if-notebook-is-running-interactively [ CHECK WHERE NOTEBOOK IS RUNNING ]\n","* https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/141502 [ SUBMISSION ]\n","\n","Notes \n","This model is a basic implementation of XLNet 0508_1 but  \n","* Run with train_folds_2.csv, based on updated input data (June 2nd)\n","* Seeding of random number generator\n","* Generates prediction for training data"]},{"cell_type":"code","metadata":{"id":"VjKIw2dP1wuj","outputId":"215c9dda-2c75-4e1d-c44f-d4cd41524238","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1591280697757,"user_tz":-120,"elapsed":15528,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["!pip install transformers\n","!pip install tokenizers\n","!pip install protobuf"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n","Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.7.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (3.10.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf) (47.1.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1RjWp8i3_R4y","outputId":"30c3cb2b-6ba3-41b2-e756-161bc5b86c04","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1591280703023,"user_tz":-120,"elapsed":20727,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["try:\n","    from google.colab import drive\n","    IN_COLAB = True\n","    drive.mount('/content/drive')\n","    !wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n","    !wget https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model\n","except:\n","    IN_COLAB = False\n","    \n","    import sys\n","    sys.path.append('/kaggle/input/sentencepiece-pb2/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","--2020-06-04 14:24:58--  https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7382 (7.2K) [text/plain]\n","Saving to: ‘sentencepiece_pb2.py.1’\n","\n","sentencepiece_pb2.p 100%[===================>]   7.21K  --.-KB/s    in 0s      \n","\n","2020-06-04 14:24:58 (65.5 MB/s) - ‘sentencepiece_pb2.py.1’ saved [7382/7382]\n","\n","--2020-06-04 14:25:00--  https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.39.230\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.39.230|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 798011 (779K) [binary/octet-stream]\n","Saving to: ‘xlnet-base-cased-spiece.model.1’\n","\n","xlnet-base-cased-sp 100%[===================>] 779.31K  1.63MB/s    in 0.5s    \n","\n","2020-06-04 14:25:01 (1.63 MB/s) - ‘xlnet-base-cased-spiece.model.1’ saved [798011/798011]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NBiazpTm1wvB","colab_type":"text"},"source":["## Import library"]},{"cell_type":"code","metadata":{"id":"vEBAM8Yn1wvC","trusted":true,"colab_type":"code","colab":{}},"source":["from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import os\n","import tokenizers\n","import string\n","import torch\n","import transformers\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","import re\n","import sentencepiece as spm\n","import sentencepiece_pb2\n","import random\n","from sklearn import model_selection\n","import gc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"76RJVzb_1wvF","outputId":"2713cebc-6e1b-400c-9030-eed7b70f8628","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1591280707539,"user_tz":-120,"elapsed":25169,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P0NlI3nJ6Kw4","trusted":true,"colab_type":"code","colab":{}},"source":["class SentencePieceTokenizer:\n","    def __init__(self, model_name):\n","        self.sp = spm.SentencePieceProcessor()\n","        self.sp.load(model_name)\n","    \n","    def encode(self, sentence):\n","        spt = sentencepiece_pb2.SentencePieceText()\n","        spt.ParseFromString(self.sp.encode_as_serialized_proto(sentence))\n","        offsets = []\n","        ids = []\n","        for piece in spt.pieces:\n","            ids.append(piece.id)\n","            offsets.append((piece.begin, piece.end))\n","        return {'ids' : ids,\n","                'offsets' : offsets}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiUv9iHz1wvu","trusted":true,"colab_type":"code","colab":{}},"source":["class config:\n","    MAX_LEN = 128\n","    TRAIN_BATCH_SIZE = 32 #64\n","    VALID_BATCH_SIZE =  16\n","    EPOCHS = 10\n","    \n","    MODEL_CONFIG = transformers.XLNetConfig\n","    MODEL = transformers.XLNetForQuestionAnswering\n","    if IN_COLAB:\n","        \n","        BASE_PATH = Path.cwd() / \"drive\" / \"My Drive\" / \"kaggle\" / \"tweet_sentiment_extraction\"\n","        PRETRAINED_MODEL_DIR = BASE_PATH / \"input\" / \"xlnetbasecased\"\n","        TOKENIZER = SentencePieceTokenizer(str(PRETRAINED_MODEL_DIR / 'xlnet-base-cased-spiece.model'))\n","        MODEL_PATH = BASE_PATH  / \"model_save\" / \"model_0602_2\"\n","        FOLDED_TRAINING_FILE = BASE_PATH / \"input\" / \"train-5fold\" / \"train_folds.csv\"\n","        TRAINING_FILE = BASE_PATH / \"input\" / \"train.csv\"\n","        TESTING_FILE = BASE_PATH  / \"input\" / \"test.csv\"\n","        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"sample_submission.csv\"\n","        SUBMISSION_FILE = BASE_PATH / \"input\" / \"submission.csv\"\n","    else:\n","        BASE_PATH = Path('/kaggle')\n","        PRETRAINED_MODEL_DIR = BASE_PATH / \"input\" / \"xlnetbasecased\"\n","        TOKENIZER = SentencePieceTokenizer( str(PRETRAINED_MODEL_DIR / \"xlnet-base-cased-spiece.model\"))\n","        MODEL_PATH = BASE_PATH  / \"input\" / \"xlnetmodel06022\"\n","        FOLDED_TRAINING_FILE = BASE_PATH / \"input\" / \"trainfolds\" / \"train_folds.csv\"\n","        TRAINING_FILE = BASE_PATH  / \"input\" / \"tweet-sentiment-extraction\" / \"train.csv\"\n","        TESTING_FILE = BASE_PATH  / \"input\" / \"tweet-sentiment-extraction\" / \"test.csv\"\n","        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"tweet-sentiment-extraction\" / \"sample_submission.csv\"\n","        SUBMISSION_FILE = BASE_PATH / \"working\" / \"submission.csv\"\n","\n","\n","  PRETRAINED_MODEL_DIR = BASE_PATH / \"input\" / \"xlnetbasecased\"\n","  TOKENIZER = SentencePieceTokenizer(str(PRETRAINED_MODEL_DIR / 'xlnet-base-cased-spiece.model'))\n","  SLANG_DICT = pd.read_csv(SLANG_FILE, header=None, names=['slang', 'normalised']).set_index('slang').to_dict()['normalised']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AciDwLjC8G77","outputId":"2b62904a-a7ea-4d01-823d-852b596d155d","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1591280707543,"user_tz":-120,"elapsed":25078,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["[config.TOKENIZER.sp.id_to_piece(x) for x in range(0,10)]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<unk>',\n"," '<s>',\n"," '</s>',\n"," '<cls>',\n"," '<sep>',\n"," '<pad>',\n"," '<mask>',\n"," '<eod>',\n"," '<eop>',\n"," '.']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"UMoIyYf39Zct","outputId":"c15021f0-0117-4d2b-a6f7-b9e5e506be51","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1591280707544,"user_tz":-120,"elapsed":25066,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["[config.TOKENIZER.sp.piece_to_id(x) for x in ['positive', 'negative', 'neutral']]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[19036, 25976, 24734]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"cbpVX6MIlynx","colab_type":"code","colab":{}},"source":["def seed_everything(seed_value):\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    os.environ['PYTHONHASHSEED'] = str(seed_value)\n","    \n","    if torch.cuda.is_available(): \n","        torch.cuda.manual_seed(seed_value)\n","        torch.cuda.manual_seed_all(seed_value)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = True\n","\n","seed = 42\n","seed_everything(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EXBSMEFxl3be","colab_type":"code","outputId":"5e313c31-bf8a-42e5-ed00-dd2f06198d55","executionInfo":{"status":"ok","timestamp":1591280708109,"user_tz":-120,"elapsed":25547,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["def create_train_folds():\n","    df = pd.read_csv(config.TRAINING_FILE)\n","    df = df.dropna().reset_index(drop=True)\n","    df[\"kfold\"] = -1\n","\n","    df = df.sample(frac=1).reset_index(drop=True)\n","\n","    kf = model_selection.StratifiedKFold(n_splits=5, random_state=seed)\n","\n","    for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.sentiment.values)):\n","        print(len(trn_), len(val_))\n","        df.loc[val_, 'kfold'] = fold\n","\n","    df.to_csv(config.FOLDED_TRAINING_FILE, index=False)\n","\n","create_train_folds()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n","  FutureWarning\n"],"name":"stderr"},{"output_type":"stream","text":["21984 5496\n","21984 5496\n","21984 5496\n","21984 5496\n","21984 5496\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2hXHgCXG1wv2","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"wLmNjNd41wv3","trusted":true,"colab_type":"code","colab":{}},"source":["class AverageMeter:\n","    \"\"\"\n","    Computes and stores the average and current value\n","    \"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def jaccard(str1, str2):\n","    a = set(str1.lower().split())\n","    b = set(str2.lower().split())\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","\n","class EarlyStopping:\n","    # https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","\n","    def __call__(self, val_loss, model, name):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model, name)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model, name)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model, name):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), name)\n","        self.val_loss_min = val_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EZ8fCbd41wv6","colab_type":"text"},"source":["## Data processing"]},{"cell_type":"code","metadata":{"id":"MUCcTAMr1wv7","trusted":true,"colab_type":"code","colab":{}},"source":["def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n","    \"\"\"\n","    Preprocessing the data to the XLNet model formatting\n","    \"\"\"\n","    tweet = \" \" + \" \".join(str(tweet).split())\n","    selected_text = \" \" + \" \".join(str(selected_text).split())\n","\n","    # find start and indices of selected_text in tweet\n","    len_st = len(selected_text) - 1\n","    idx0 = None\n","    idx1 = None\n","\n","    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n","        if \" \" + tweet[ind: ind+len_st] == selected_text:\n","            idx0 = ind\n","            idx1 = ind + len_st - 1\n","            break\n","\n","    # create character mask for selected_text in tweet\n","    char_targets = [0] * len(tweet)\n","    if idx0 != None and idx1 != None:\n","        for ct in range(idx0, idx1 + 1):\n","            char_targets[ct] = 1\n","\n","    tok_tweet = tokenizer.encode(tweet)\n","    \n","    input_ids_orig = tok_tweet['ids']\n","    tweet_offsets = tok_tweet['offsets']\n","    \n","    target_idx = []\n","    for j, (offset1, offset2) in enumerate(tweet_offsets):\n","        if sum(char_targets[offset1: offset2]) > 0:\n","            target_idx.append(j)\n","    \n","    targets_start = target_idx[0]\n","    targets_end = target_idx[-1]\n","\n","    #######\n","    sentiment_id = {\n","        'positive': 19036,\n","        'negative': 25976,\n","        'neutral': 24734\n","    }\n","    #######\n","    \n","    # https://huggingface.co/transformers/model_doc/xlnet.html#transformers.XLNetTokenizer.build_inputs_with_special_tokens\n","    input_ids = [sentiment_id[sentiment]] + [4] + input_ids_orig + [4] + [3]\n","    #input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n","    token_type_ids = [0]*2 + [1] * (len(input_ids_orig)+1) + [2]\n","    mask = [1] * len(token_type_ids)\n","    tweet_offsets = [(0, 0)] * 2 + tweet_offsets + [(0, 0)] * 2\n","    targets_start += 2\n","    targets_end += 2\n","\n","    padding_length = max_len - len(input_ids)\n","    if padding_length > 0:\n","        input_ids = input_ids + ([5] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n","    \n","    return {\n","        'ids': input_ids,\n","        'mask': mask,\n","        'token_type_ids': token_type_ids,\n","        'targets_start': targets_start,\n","        'targets_end': targets_end,\n","        'orig_tweet': tweet,\n","        'orig_selected': selected_text,\n","        'sentiment': sentiment,\n","        'offsets': tweet_offsets\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y-eRe4x11wv-","colab_type":"text"},"source":["## Data loader"]},{"cell_type":"code","metadata":{"id":"4s9Id3hW1wv-","trusted":true,"colab_type":"code","colab":{}},"source":["class TweetDataset:\n","    def __init__(self, tweet, sentiment, selected_text):\n","        self.tweet = tweet\n","        self.sentiment = sentiment\n","        self.selected_text = selected_text\n","        self.tokenizer = config.TOKENIZER\n","        self.max_len = config.MAX_LEN\n","    \n","    def __len__(self):\n","        return len(self.tweet)\n","\n","    def __getitem__(self, item):\n","        data = process_data(\n","            self.tweet[item], \n","            self.selected_text[item], \n","            self.sentiment[item],\n","            self.tokenizer,\n","            self.max_len\n","        )\n","\n","        return {\n","            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n","            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n","            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n","            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n","            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n","            'orig_tweet': data[\"orig_tweet\"],\n","            'orig_selected': data[\"orig_selected\"],\n","            'sentiment': data[\"sentiment\"],\n","            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n","        }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXJ2VElJ1wwI","colab_type":"text"},"source":["## Loss function"]},{"cell_type":"code","metadata":{"id":"hFNXnE3u1wwJ","trusted":true,"colab_type":"code","colab":{}},"source":["# def loss_fn(start_logits, end_logits, start_positions, end_positions):\n","#     loss_fct = nn.CrossEntropyLoss()\n","#     start_loss = loss_fct(start_logits, start_positions)\n","#     end_loss = loss_fct(end_logits, end_positions)\n","#     total_loss = (start_loss + end_loss)\n","#     return total_loss\n","\n","def loss_fn(start_logprobs, end_logprobs, start_positions, end_positions):\n","    loss_fct = nn.NLLLoss()\n","    start_loss = loss_fct(start_logits, start_positions)\n","    end_loss = loss_fct(end_logits, end_positions)\n","    total_loss = (start_loss + end_loss)\n","    return total_loss\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCxxX5Jw1wwM","colab_type":"text"},"source":["## Training function"]},{"cell_type":"code","metadata":{"id":"NL1vcjOa1wwN","trusted":true,"colab_type":"code","colab":{}},"source":["def train_fn(data_loader, model, optimizer, device, scheduler=None):\n","    model.train()\n","    losses = AverageMeter()\n","    jaccards = AverageMeter()\n","\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    \n","    for bi, d in enumerate(tk0):\n","\n","        ids = d[\"ids\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        mask = d[\"mask\"]\n","        targets_start = d[\"targets_start\"]\n","        targets_end = d[\"targets_end\"]\n","        sentiment = d[\"sentiment\"]\n","        orig_selected = d[\"orig_selected\"]\n","        orig_tweet = d[\"orig_tweet\"]\n","        targets_start = d[\"targets_start\"]\n","        targets_end = d[\"targets_end\"]\n","        offsets = d[\"offsets\"]\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        targets_start = targets_start.to(device, dtype=torch.long)\n","        targets_end = targets_end.to(device, dtype=torch.long)\n","\n","        model.zero_grad()\n","        \n","        # outputs_start, outputs_end = model(\n","        #     ids=ids,\n","        #     mask=mask,\n","        #     token_type_ids=token_type_ids,\n","        # )\n","        # loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n","        # loss.backward()\n","\n","        outputs = model(\n","            input_ids=ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids,\n","            start_positions=targets_start, \n","            end_positions=targets_end\n","        )\n","        \n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n","        # outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n","        \n","        # jaccard_scores = []\n","        # for px, tweet in enumerate(orig_tweet):\n","        #     selected_tweet = orig_selected[px]\n","        #     tweet_sentiment = sentiment[px]\n","        #     jaccard_score, _ = calculate_jaccard_score(\n","        #         original_tweet=tweet,\n","        #         target_string=selected_tweet,\n","        #         sentiment_val=tweet_sentiment,\n","        #         idx_start=np.argmax(outputs_start[px, :]),\n","        #         idx_end=np.argmax(outputs_end[px, :]),\n","        #         offsets=offsets[px]\n","        #     )\n","        #     jaccard_scores.append(jaccard_score)\n","\n","        # jaccards.update(np.mean(jaccard_scores), ids.size(0))\n","        losses.update(loss.item(), ids.size(0))\n","        tk0.set_postfix(loss=losses.avg)#, jaccard=jaccards.avg)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jalt8e9B1wwQ","colab_type":"text"},"source":["## Evaluation function"]},{"cell_type":"code","metadata":{"id":"chIyiuLB1wwR","trusted":true,"colab_type":"code","colab":{}},"source":["def calculate_jaccard_score(\n","    original_tweet, \n","    target_string, \n","    sentiment_val, \n","    idx_start, \n","    idx_end, \n","    offsets,\n","    verbose=False):\n","    \n","    if idx_end < idx_start:\n","        idx_end = idx_start\n","    \n","    filtered_output  = \"\"\n","    for ix in range(idx_start, idx_end + 1):\n","        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n","\n","        # add spacing to output\n","        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n","            filtered_output += \" \"\n","\n","    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n","        filtered_output = original_tweet\n","\n","    if sentiment_val != \"neutral\" and verbose == True:\n","        if filtered_output.strip().lower() != target_string.strip().lower():\n","            print(\"********************************\")\n","            print(f\"Output= {filtered_output.strip()}\")\n","            print(f\"Target= {target_string.strip()}\")\n","            print(f\"Tweet= {original_tweet.strip()}\")\n","            print(\"********************************\")\n","\n","    jac = jaccard(target_string.strip(), filtered_output.strip())\n","    return jac, filtered_output\n","\n","\n","def eval_fn(data_loader, model, device):\n","    model.eval()\n","    losses = AverageMeter()\n","    jaccards = AverageMeter()\n","    \n","    with torch.no_grad():\n","        tk0 = tqdm(data_loader, total=len(data_loader))\n","        for bi, d in enumerate(tk0):\n","            ids = d[\"ids\"]\n","            token_type_ids = d[\"token_type_ids\"]\n","            mask = d[\"mask\"]\n","            sentiment = d[\"sentiment\"]\n","            orig_selected = d[\"orig_selected\"]\n","            orig_tweet = d[\"orig_tweet\"]\n","            targets_start = d[\"targets_start\"]\n","            targets_end = d[\"targets_end\"]\n","            offsets = d[\"offsets\"]\n","\n","            ids = ids.to(device, dtype=torch.long)\n","            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","            mask = mask.to(device, dtype=torch.long)\n","            targets_start = targets_start.to(device, dtype=torch.long)\n","            targets_end = targets_end.to(device, dtype=torch.long)\n","\n","            outputs = model(\n","                input_ids=ids,\n","                attention_mask=mask,\n","                token_type_ids=token_type_ids,\n","                start_positions=targets_start,\n","                end_positions=targets_end\n","            )\n","            loss = outputs[0]\n","            \n","            # run it again to get the probabilities\n","            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n","            outputs = model(\n","                input_ids=ids,\n","                attention_mask=mask,\n","                token_type_ids=token_type_ids\n","            )\n","            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n","            start_top_probs = outputs[0]  \n","          \n","            # start_top_probs contain those positions' probabilities\n","            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n","            start_top_index = outputs[1] \n","\n","            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n","            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n","            # this can be verified by checking summation to unity\n","            end_top_probs = outputs[2] \n","            end_top_index = outputs[3] \n","            \n","            # calculate joint probability of start, end position tuples\n","            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n","\n","            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n","            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n","            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n","\n","            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n","                      indices]\n","\n","            # get (flat) position in sequence of highest probability tuple\n","            top_start_end_probs_sorted = start_end_probs_sorted.argmax(dim=1)\n","\n","            # convert flat position to separate start and end positions\n","            start_top_positions = (top_start_end_probs_sorted % torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n","            end_top_positions = (top_start_end_probs_sorted // torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n","            \n","            jaccard_scores = []\n","            for px, tweet in enumerate(orig_tweet):\n","                selected_tweet = orig_selected[px]\n","                tweet_sentiment = sentiment[px]\n","                start_top_position = start_top_positions[px]\n","                end_top_position = end_top_positions[px]\n","            \n","                jaccard_score, _ = calculate_jaccard_score(\n","                    original_tweet=tweet,\n","                    target_string=selected_tweet,\n","                    sentiment_val=tweet_sentiment,\n","                    idx_start=start_top_position,\n","                    idx_end=end_top_position,\n","                    offsets=offsets[px]\n","                )\n","                jaccard_scores.append(jaccard_score)\n","\n","            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n","            losses.update(loss.item(), ids.size(0))\n","            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n","    \n","    print(f\"Jaccard = {jaccards.avg}\")\n","    print(f\"Loss = {losses.avg}\")\n","    return jaccards.avg, losses.avg"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sWiozMAq1wwW","colab_type":"text"},"source":["## Training "]},{"cell_type":"code","metadata":{"id":"el1CDathhxr8","trusted":true,"colab_type":"code","colab":{}},"source":["def init_model(config):\n","    model_config = config.MODEL_CONFIG.from_pretrained(config.PRETRAINED_MODEL_DIR )#/ \"config.json\")\n","    model_config.output_hidden_states = True\n","    model_config.start_n_top = config.MAX_LEN\n","    model_config.end_n_top = config.MAX_LEN\n","    #'/kaggle/input/xlnet-base-tf/xlnet-base-cased'\n","    model = config.MODEL.from_pretrained(config.PRETRAINED_MODEL_DIR, config=model_config)#, state_dict='/kaggle/input/xlnetmodel05081/model_3.bin')\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0tHJ24G1wwW","trusted":true,"colab_type":"code","colab":{}},"source":["def run_fold(fold):\n","\n","    dfx = pd.read_csv(config.FOLDED_TRAINING_FILE)\n","\n","    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n","    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n","\n","    train_dataset = TweetDataset(\n","        tweet=df_train.text.values,\n","        sentiment=df_train.sentiment.values,\n","        selected_text=df_train.selected_text.values\n","    )\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.TRAIN_BATCH_SIZE,\n","        num_workers=4\n","    )\n","\n","    valid_dataset = TweetDataset(\n","        tweet=df_valid.text.values,\n","        sentiment=df_valid.sentiment.values,\n","        selected_text=df_valid.selected_text.values\n","    )\n","\n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=config.VALID_BATCH_SIZE,\n","        num_workers=2\n","    )\n","    \n","    device = torch.device(\"cuda\")\n","\n","    # initialise model\n","    model = init_model(config)\n","    \n","    model.to(device)\n","\n","    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, \n","        num_warmup_steps=0, \n","        num_training_steps=num_train_steps\n","    )\n","\n","    es = EarlyStopping(patience=2, verbose=True)\n","    print(f\"Training is Starting for fold={fold}\")\n","    \n","    for epoch in range(config.EPOCHS):\n","        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n","        jaccard, loss = eval_fn(valid_data_loader, model, device)\n","        print(f\"Jaccard Score = {jaccard}\")\n","        print(f\"Loss score = {loss}\")\n","        es(loss, model, name=config.MODEL_PATH / f\"model_{fold}.bin\")\n","        \n","        if es.early_stop:\n","            print(\"Early stopping\")\n","            break\n","  \n","    return es.val_loss_min"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2uAP683Z1wwZ","colab_type":"text"},"source":["## Run training"]},{"cell_type":"code","metadata":{"id":"mUwMNkb1Hq3w","trusted":true,"colab_type":"code","colab":{}},"source":["def run_training():\n","  if not os.path.exists(config.MODEL_PATH):\n","    os.mkdir(config.MODEL_PATH)\n","  val_loss = []\n","  for ifold in range(2,5):\n","      q = run_fold(ifold)\n","      val_loss.append(q)\n","  print(f'Mean val loss: {np.mean(val_loss)}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iyui81xMHLqQ","colab_type":"text"},"source":["## Predict test set"]},{"cell_type":"code","metadata":{"id":"pW0jJHfUkzTs","trusted":true,"colab_type":"code","colab":{}},"source":["def predict_test():\n","  df_test = pd.read_csv(config.TESTING_FILE)\n","  df_test.loc[:, \"selected_text\"] = df_test.text.values\n","\n","  models = []\n","\n","  for mf in os.listdir(config.MODEL_PATH):\n","    m = init_model(config)\n","    \n","    m.load_state_dict(torch.load(config.MODEL_PATH / mf))\n","    print(config.MODEL_PATH / mf)\n","    m.eval()\n","    # ensure we get output probabilities for all combinations of start and end position\n","    m.start_n_top = config.MAX_LEN\n","    m.end_n_top = config.MAX_LEN\n","    m.to(device)\n","\n","    models.append(m)\n","\n","  test_dataset = TweetDataset(\n","          tweet=df_test.text.values,\n","          sentiment=df_test.sentiment.values,\n","          selected_text=df_test.selected_text.values\n","      )\n","\n","  test_data_loader = torch.utils.data.DataLoader(\n","      test_dataset,\n","      shuffle=False,\n","      batch_size=config.VALID_BATCH_SIZE,\n","      num_workers=1\n","  )\n","\n","  final_output = []\n","\n","  with torch.no_grad():\n","      tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n","      for bi, d in enumerate(tk0):\n","          ids = d[\"ids\"]\n","          token_type_ids = d[\"token_type_ids\"]\n","          mask = d[\"mask\"]\n","          sentiment = d[\"sentiment\"]\n","          orig_selected = d[\"orig_selected\"]\n","          orig_tweet = d[\"orig_tweet\"]\n","          targets_start = d[\"targets_start\"]\n","          targets_end = d[\"targets_end\"]\n","          offsets = d[\"offsets\"].numpy()\n","\n","          ids = ids.to(device, dtype=torch.long)\n","          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","          mask = mask.to(device, dtype=torch.long)\n","          targets_start = targets_start.to(device, dtype=torch.long)\n","          targets_end = targets_end.to(device, dtype=torch.long)\n","          \n","          summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n","\n","          for model in models: \n","            # run it again to get the probabilities\n","            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n","            outputs = model(\n","                input_ids=ids,\n","                attention_mask=mask,\n","                token_type_ids=token_type_ids\n","            )\n","\n","            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n","            start_top_probs = outputs[0]  \n","          \n","            # start_top_probs contain those positions' probabilities\n","            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n","            start_top_index = outputs[1] \n","\n","            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n","            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n","            # this can be verified by checking summation to unity\n","            end_top_probs = outputs[2] \n","            end_top_index = outputs[3] \n","            \n","            # calculate joint probability of start, end position tuples\n","            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n","\n","            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n","            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n","            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n","\n","            #start_end_probs_sorted = start_end_probs[torch.arange(start_end_probs.shape[0]), indices]\n","            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n","                      indices]\n","\n","            summed_start_end_probs_sorted += start_end_probs_sorted\n","\n","          avg_start_end_probs_sorted = summed_start_end_probs_sorted/torch.tensor(len(models))\n","\n","          # get (flat) position in sequence of highest probability tuple\n","          top_avg_start_end_probs_sorted = avg_start_end_probs_sorted.argmax(dim=1)\n","\n","          # convert flat position to separate start and end positions\n","          start_top_positions = (top_avg_start_end_probs_sorted % torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n","          end_top_positions = (top_avg_start_end_probs_sorted // torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n","          \n","          jaccard_scores = []\n","          for px, tweet in enumerate(orig_tweet):\n","              selected_tweet = orig_selected[px]\n","              tweet_sentiment = sentiment[px]\n","              _, output_sentence = calculate_jaccard_score(\n","                  original_tweet=tweet,\n","                  target_string=selected_tweet,\n","                  sentiment_val=tweet_sentiment,\n","                  idx_start=start_top_positions[px],\n","                  idx_end=end_top_positions[px],\n","                  offsets=offsets[px],\n","                  verbose=True\n","              )\n","              final_output.append(output_sentence)\n","\n","\n","  sample = pd.read_csv(config.SAMPLE_SUBMISSION_FILE)\n","  sample.loc[:, 'selected_text'] = final_output\n","  sample.to_csv(\"submission.csv\", index=False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WzGdGgpPp_Pi","colab_type":"code","colab":{}},"source":["def predict_train(n_sample=None):\n","  df_train = pd.read_csv(config.FOLDED_TRAINING_FILE)\n","  \n","  if n_sample:\n","    df_train = df_train.sample(n_sample)\n","  \n","  final_output = []\n","\n","  for mf in os.listdir(config.MODEL_PATH):\n","    if not mf.endswith('.bin'):\n","      continue\n","\n","    model = init_model(config)\n","    \n","    model.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n","    print(config.MODEL_PATH / mf)\n","    model.eval()\n","    # ensure we get output probabilities for all combinations of start and end position\n","    model.start_n_top = config.MAX_LEN\n","    model.end_n_top = config.MAX_LEN\n","    model.to(device)\n","\n","    fold = int(re.findall('model_(\\d).bin', mf)[0])\n","    \n","    if df_train.pipe(lambda x:x[x.kfold==fold]).shape[0]==0:\n","      continue\n","    \n","    train_dataset = TweetDataset(\n","            tweet=df_train.pipe(lambda x:x[x.kfold==fold]).text.values,\n","            sentiment=df_train.pipe(lambda x:x[x.kfold==fold]).sentiment.values,\n","            selected_text=df_train.pipe(lambda x:x[x.kfold==fold]).selected_text.values\n","        )\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        shuffle=False,\n","        batch_size=8, #config.VALID_BATCH_SIZE,\n","        num_workers=0\n","    )\n","\n","    tk0 = tqdm(train_data_loader, total=len(train_data_loader))\n","    for bi, d in enumerate(tk0):\n","        ids = d[\"ids\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        mask = d[\"mask\"]\n","        sentiment = d[\"sentiment\"]\n","        orig_selected = d[\"orig_selected\"]\n","        orig_tweet = d[\"orig_tweet\"]\n","        targets_start = d[\"targets_start\"]\n","        targets_end = d[\"targets_end\"]\n","        offsets = d[\"offsets\"]\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        targets_start = targets_start.to(device, dtype=torch.long)\n","        targets_end = targets_end.to(device, dtype=torch.long)\n","\n","        # run it again to get the probabilities\n","        # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n","        outputs = model(\n","            input_ids=ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids\n","        )\n","        # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n","        start_top_probs = outputs[0]  \n","      \n","        # start_top_probs contain those positions' probabilities\n","        # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n","        start_top_index = outputs[1] \n","\n","        # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n","        # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n","        # this can be verified by checking summation to unity\n","        end_top_probs = outputs[2] \n","        end_top_index = outputs[3] \n","        \n","        # calculate joint probability of start, end position tuples\n","        start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n","\n","        # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n","        mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n","        _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n","\n","        start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n","                  indices]\n","\n","        # get (flat) position in sequence of highest probability tuple\n","        top_start_end_probs_sorted = start_end_probs_sorted.argmax(dim=1)\n","\n","        # convert flat position to separate start and end positions\n","        start_top_positions = (top_start_end_probs_sorted % torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n","        end_top_positions = (top_start_end_probs_sorted // torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n","        \n","        jaccard_scores = []\n","        for px, tweet in enumerate(orig_tweet):\n","            selected_tweet = orig_selected[px]\n","            tweet_sentiment = sentiment[px]\n","            start_top_position = start_top_positions[px]\n","            end_top_position = end_top_positions[px]\n","            \n","            _, output_sentence = calculate_jaccard_score(\n","                original_tweet=tweet,\n","                target_string=selected_tweet,\n","                sentiment_val=tweet_sentiment,\n","                idx_start=start_top_position,\n","                idx_end=end_top_position,\n","                offsets=offsets[px]\n","            )\n","            final_output.append({'text':tweet, 'prediction':output_sentence})\n","     \n","    del model, train_dataset, train_data_loader\n","    gc.collect()\n","\n","  df_train = df_train.merge(pd.DataFrame(final_output), on='text', how='inner')\n","\n","  return df_train\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sy0b2gnexnQB","outputId":"4a4e39dd-586a-4bfa-e0fb-d9af7eed996d","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1591211049296,"user_tz":-120,"elapsed":106387,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["IN_KAGGLE_COMMIT = False\n","if (not IN_COLAB) and ('runtime' not in get_ipython().config.IPKernelApp.connection_file):\n","   IN_KAGGLE_COMMIT = True\n","\n","\n","print(IN_KAGGLE_COMMIT)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WSqxmPyXYbhZ","trusted":true,"colab_type":"code","outputId":"a720383f-88ee-4551-cb13-a31f48c048e3","executionInfo":{"status":"ok","timestamp":1591224670842,"user_tz":-120,"elapsed":13601527,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":[" %%time\n"," \n","if IN_COLAB:\n","    run_training()\n","\n","if IN_KAGGLE_COMMIT:\n","    predict_test()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/687 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training is Starting for fold=2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [12:59<00:00,  1.13s/it, loss=1.16]\n","100%|██████████| 344/344 [04:14<00:00,  1.35it/s, jaccard=0.688, loss=0.847]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.6881574461168406\n","Loss = 0.8469397480744635\n","Jaccard Score = 0.6881574461168406\n","Loss score = 0.8469397480744635\n","Validation loss decreased (inf --> 0.846940).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:07<00:00,  1.15s/it, loss=0.808]\n","100%|██████████| 344/344 [04:15<00:00,  1.35it/s, jaccard=0.699, loss=0.784]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.699178611541649\n","Loss = 0.7844986148840128\n","Jaccard Score = 0.699178611541649\n","Loss score = 0.7844986148840128\n","Validation loss decreased (0.846940 --> 0.784499).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:06<00:00,  1.15s/it, loss=0.705]\n","100%|██████████| 344/344 [04:15<00:00,  1.35it/s, jaccard=0.706, loss=0.797]\n","  0%|          | 0/687 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7058376872776575\n","Loss = 0.7969098837918763\n","Jaccard Score = 0.7058376872776575\n","Loss score = 0.7969098837918763\n","EarlyStopping counter: 1 out of 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:07<00:00,  1.15s/it, loss=0.624]\n","100%|██████████| 344/344 [04:16<00:00,  1.34it/s, jaccard=0.703, loss=0.833]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7032756170278647\n","Loss = 0.8333517816432873\n","Jaccard Score = 0.7032756170278647\n","Loss score = 0.8333517816432873\n","EarlyStopping counter: 2 out of 2\n","Early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/687 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training is Starting for fold=3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:06<00:00,  1.15s/it, loss=1.15]\n","100%|██████████| 344/344 [04:15<00:00,  1.34it/s, jaccard=0.692, loss=0.881]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.6921551349471375\n","Loss = 0.8813899087420024\n","Jaccard Score = 0.6921551349471375\n","Loss score = 0.8813899087420024\n","Validation loss decreased (inf --> 0.881390).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:07<00:00,  1.15s/it, loss=0.811]\n","100%|██████████| 344/344 [04:16<00:00,  1.34it/s, jaccard=0.701, loss=0.819]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7011193989616508\n","Loss = 0.818517075181875\n","Jaccard Score = 0.7011193989616508\n","Loss score = 0.818517075181875\n","Validation loss decreased (0.881390 --> 0.818517).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:07<00:00,  1.15s/it, loss=0.708]\n","100%|██████████| 344/344 [04:16<00:00,  1.34it/s, jaccard=0.704, loss=0.826]\n","  0%|          | 0/687 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7037199330689328\n","Loss = 0.8260110733810991\n","Jaccard Score = 0.7037199330689328\n","Loss score = 0.8260110733810991\n","EarlyStopping counter: 1 out of 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:08<00:00,  1.15s/it, loss=0.617]\n","100%|██████████| 344/344 [04:16<00:00,  1.34it/s, jaccard=0.707, loss=0.844]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7065561916115967\n","Loss = 0.8439583608764054\n","Jaccard Score = 0.7065561916115967\n","Loss score = 0.8439583608764054\n","EarlyStopping counter: 2 out of 2\n","Early stopping\n"],"name":"stdout"},{"output_type":"stream","text":["\r  0%|          | 0/687 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training is Starting for fold=4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:07<00:00,  1.15s/it, loss=1.18]\n","100%|██████████| 344/344 [04:15<00:00,  1.35it/s, jaccard=0.691, loss=0.858]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.6912228656549559\n","Loss = 0.8580306866023217\n","Jaccard Score = 0.6912228656549559\n","Loss score = 0.8580306866023217\n","Validation loss decreased (inf --> 0.858031).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:07<00:00,  1.15s/it, loss=0.817]\n","100%|██████████| 344/344 [04:15<00:00,  1.35it/s, jaccard=0.704, loss=0.809]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7044407146682441\n","Loss = 0.8094697693116106\n","Jaccard Score = 0.7044407146682441\n","Loss score = 0.8094697693116106\n","Validation loss decreased (0.858031 --> 0.809470).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:06<00:00,  1.14s/it, loss=0.716]\n","100%|██████████| 344/344 [04:14<00:00,  1.35it/s, jaccard=0.71, loss=0.8]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7096976859332754\n","Loss = 0.8002354904380254\n","Jaccard Score = 0.7096976859332754\n","Loss score = 0.8002354904380254\n","Validation loss decreased (0.809470 --> 0.800235).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:06<00:00,  1.14s/it, loss=0.635]\n","100%|██████████| 344/344 [04:15<00:00,  1.35it/s, jaccard=0.71, loss=0.815]\n","  0%|          | 0/687 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7099133414456748\n","Loss = 0.8153451571818523\n","Jaccard Score = 0.7099133414456748\n","Loss score = 0.8153451571818523\n","EarlyStopping counter: 1 out of 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 687/687 [13:05<00:00,  1.14s/it, loss=0.557]\n","100%|██████████| 344/344 [04:15<00:00,  1.35it/s, jaccard=0.714, loss=0.876]"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7138256637549594\n","Loss = 0.8755604223045893\n","Jaccard Score = 0.7138256637549594\n","Loss score = 0.8755604223045893\n","EarlyStopping counter: 2 out of 2\n","Early stopping\n","Mean val loss: 0.8010837268346377\n","CPU times: user 2h 5min 53s, sys: 1h 38min 55s, total: 3h 44min 48s\n","Wall time: 3h 46min 40s\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ewVQ0tPaqCUY","colab_type":"code","outputId":"0fdb92b9-3047-4232-c6e7-994096163614","executionInfo":{"status":"ok","timestamp":1591282553123,"user_tz":-120,"elapsed":57346,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["output = predict_train(1000)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/27 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0602_2/model_0.bin\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 27/27 [00:05<00:00,  4.81it/s]\n","  0%|          | 0/22 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0602_2/model_1.bin\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 22/22 [00:04<00:00,  4.66it/s]\n","  0%|          | 0/26 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0602_2/model_2.bin\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 26/26 [00:05<00:00,  4.67it/s]\n","  0%|          | 0/26 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0602_2/model_3.bin\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 26/26 [00:05<00:00,  4.73it/s]\n","  0%|          | 0/25 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0602_2/model_4.bin\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 25/25 [00:05<00:00,  4.56it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"r9Xikf_F2XG0","colab_type":"code","outputId":"ee767bf4-4b75-42d9-e6bd-e4fcc74a7bfe","executionInfo":{"status":"ok","timestamp":1591282895887,"user_tz":-120,"elapsed":1083,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["output.apply(lambda x: jaccard(x['selected_text'], x['prediction']),axis=1).mean()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7330258621844067"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"mD5pL-K41Ot7","colab_type":"code","outputId":"10142ad2-0cc2-4d51-ccff-6a2a08d3fb92","executionInfo":{"status":"ok","timestamp":1591282616043,"user_tz":-120,"elapsed":1285,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["output.assign(match=lambda x: x['selected_text']==x['prediction'])['match'].value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False    219\n","True      24\n","Name: match, dtype: int64"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"vMd7sqzPqEHe","colab_type":"code","outputId":"b5515d44-d178-4676-8ce2-0ceb56f5d595","executionInfo":{"status":"ok","timestamp":1591279700318,"user_tz":-120,"elapsed":1069,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["output.pipe(lambda x:x[x.prediction.notnull()])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>textID</th>\n","      <th>text</th>\n","      <th>selected_text</th>\n","      <th>sentiment</th>\n","      <th>kfold</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>4</th>\n","      <td>4a9f28d722</td>\n","      <td>I saw slater on some MTV show, So you think y...</td>\n","      <td>some **** like that.</td>\n","      <td>negative</td>\n","      <td>3</td>\n","      <td>**</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>4148a10b84</td>\n","      <td>Btw happy mom`s day to hannah and your mom an...</td>\n","      <td>happy</td>\n","      <td>positive</td>\n","      <td>3</td>\n","      <td>happy</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>991b0b54a1</td>\n","      <td>I called but you didnt answer</td>\n","      <td>I called but you didnt answer</td>\n","      <td>neutral</td>\n","      <td>0</td>\n","      <td>I called but you didnt answer</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>b8d2f3cbfb</td>\n","      <td>Think of the prize at the end. So sorry to he...</td>\n","      <td>So sorry to hear that though.</td>\n","      <td>negative</td>\n","      <td>0</td>\n","      <td>So sorry to hear that though.</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>cf405b31c1</td>\n","      <td>bloody Feds, they lost last statement and r h...</td>\n","      <td>bloody Feds,</td>\n","      <td>negative</td>\n","      <td>2</td>\n","      <td>bloody</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>969</th>\n","      <td>b4acad2b3c</td>\n","      <td>Please wish your wife Evelyn a Happy Mother`s...</td>\n","      <td>I hope that she has a wonderful day.</td>\n","      <td>positive</td>\n","      <td>4</td>\n","      <td>Happy</td>\n","    </tr>\n","    <tr>\n","      <th>972</th>\n","      <td>a0e2fdc0e2</td>\n","      <td>in june. the 6th. a sat!</td>\n","      <td>in june. the 6th. a sat!</td>\n","      <td>neutral</td>\n","      <td>4</td>\n","      <td>in june. the 6th. a sat!</td>\n","    </tr>\n","    <tr>\n","      <th>979</th>\n","      <td>101dba7c07</td>\n","      <td>the closest is a good two hours</td>\n","      <td>the closest is a good two hours</td>\n","      <td>neutral</td>\n","      <td>0</td>\n","      <td>the closest is a good two hours</td>\n","    </tr>\n","    <tr>\n","      <th>984</th>\n","      <td>b104b9ebd8</td>\n","      <td>I would like to be able to go just to ONE of ...</td>\n","      <td>I would like to be able to go just to ONE of t...</td>\n","      <td>neutral</td>\n","      <td>3</td>\n","      <td>I would like to be able to go just to ONE of ...</td>\n","    </tr>\n","    <tr>\n","      <th>990</th>\n","      <td>be9d13a65c</td>\n","      <td>go to vegas, but don`t spend all ur money!!!!</td>\n","      <td>go to vegas, but don`t spend all ur money!!!!</td>\n","      <td>neutral</td>\n","      <td>4</td>\n","      <td>go to vegas, but don`t spend all ur money!!!!</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>246 rows × 6 columns</p>\n","</div>"],"text/plain":["         textID  ...                                         prediction\n","4    4a9f28d722  ...                                                 **\n","14   4148a10b84  ...                                              happy\n","16   991b0b54a1  ...                      I called but you didnt answer\n","18   b8d2f3cbfb  ...                      So sorry to hear that though.\n","27   cf405b31c1  ...                                             bloody\n","..          ...  ...                                                ...\n","969  b4acad2b3c  ...                                              Happy\n","972  a0e2fdc0e2  ...                           in june. the 6th. a sat!\n","979  101dba7c07  ...                    the closest is a good two hours\n","984  b104b9ebd8  ...   I would like to be able to go just to ONE of ...\n","990  be9d13a65c  ...      go to vegas, but don`t spend all ur money!!!!\n","\n","[246 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"AHQPAJaBpz1z","colab_type":"code","outputId":"eaffc654-c9e6-4c4d-c23e-fa23e9e4cd35","executionInfo":{"status":"ok","timestamp":1591279616992,"user_tz":-120,"elapsed":1439,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["output.groupby('kfold')['prediction'].apply(lambda x:x.isnull().value_counts()).unstack(level=1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>True</th>\n","      <th>False</th>\n","    </tr>\n","    <tr>\n","      <th>kfold</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>158</td>\n","      <td>48</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>154</td>\n","      <td>41</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>130</td>\n","      <td>59</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>151</td>\n","      <td>39</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>161</td>\n","      <td>59</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       True   False\n","kfold              \n","0        158     48\n","1        154     41\n","2        130     59\n","3        151     39\n","4        161     59"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"m5LzpeWVpdSL","colab_type":"code","outputId":"b45bf598-61f6-4725-e0a1-eaf7ceb53159","executionInfo":{"status":"ok","timestamp":1591279502746,"user_tz":-120,"elapsed":1227,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["output.prediction.isnull().value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True     754\n","False    246\n","Name: prediction, dtype: int64"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"FJg0fq5jlMZw","trusted":false,"colab_type":"code","colab":{}},"source":["# test_df = pd.read_csv(config.TESTING_FILE).set_index(\"textID\")\n","\n","# sub_df = pd.read_csv(config.SUBMISSION_FILE).set_index(\"textID\")\n","\n","# # Everything not presented in the public set \n","# # will take a value of the original text\n","# test_df[\"selected_text\"] = test_df.text\n","\n","# # Get the public ids and assign them\n","# public_idxs = sub_df.index.values\n","# test_df.loc[public_idxs, \"selected_text\"] = sub_df.selected_text.values\n","# test_df[[\"selected_text\"]].to_csv(\"submission.csv\")"],"execution_count":0,"outputs":[]}]}