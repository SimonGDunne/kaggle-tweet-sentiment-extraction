{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"xlnet-model-0613-6.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"7jaLISF2magZ","colab_type":"text"},"source":["Code taken/adapted from:\n","* https://www.youtube.com/watch?v=U51ranzJBpY [ TOKENISER ]\n","* https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch [ TRAINING ]\n","* https://www.kaggle.com/abhishek/roberta-inference-5-folds [ INFERENCE ] \n","* https://www.kaggle.com/masterscrat/detect-if-notebook-is-running-interactively [ CHECK WHERE NOTEBOOK IS RUNNING ]\n","* https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/141502 [ SUBMISSION ]\n","\n","Notes \n","This model is based on XLNet 0605_5 but  \n","* Adds back the leading whitespace to the tweets that I had removed\n"]},{"cell_type":"code","metadata":{"id":"VjKIw2dP1wuj","outputId":"35fe4da6-e131-489d-d843-5756b170c78d","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1592226649229,"user_tz":-120,"elapsed":16995,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":683}},"source":["!pip install transformers\n","!pip install tokenizers\n","!pip install protobuf"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n","\u001b[K     |████████████████████████████████| 675kB 2.8MB/s \n","\u001b[?25hCollecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 13.4MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 42.9MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 47.7MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=97f9b41e7d68d693d2c3e3a3653b2829f3ea1547322819fe20515bfccfc5c1bb\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.7.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (3.10.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf) (1.12.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf) (47.1.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1RjWp8i3_R4y","outputId":"4d75b482-e6a1-41ea-b0e7-e116b60f88a6","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1592226747370,"user_tz":-120,"elapsed":115118,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["try:\n","    from google.colab import drive\n","    IN_COLAB = True\n","    drive.mount('/content/drive')\n","    !wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n","    !wget https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model\n","except:\n","    IN_COLAB = False\n","    \n","    import sys\n","    sys.path.append('/kaggle/input/sentencepiece-pb2/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","--2020-06-15 13:12:23--  https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 7382 (7.2K) [text/plain]\n","Saving to: ‘sentencepiece_pb2.py’\n","\n","sentencepiece_pb2.p 100%[===================>]   7.21K  --.-KB/s    in 0s      \n","\n","2020-06-15 13:12:23 (92.5 MB/s) - ‘sentencepiece_pb2.py’ saved [7382/7382]\n","\n","--2020-06-15 13:12:25--  https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.64.126\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.64.126|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 798011 (779K) [binary/octet-stream]\n","Saving to: ‘xlnet-base-cased-spiece.model’\n","\n","xlnet-base-cased-sp 100%[===================>] 779.31K  --.-KB/s    in 0.06s   \n","\n","2020-06-15 13:12:25 (11.7 MB/s) - ‘xlnet-base-cased-spiece.model’ saved [798011/798011]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NBiazpTm1wvB","colab_type":"text"},"source":["## Import library"]},{"cell_type":"code","metadata":{"id":"vEBAM8Yn1wvC","trusted":true,"colab_type":"code","colab":{}},"source":["import os\n","\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import os\n","import tokenizers\n","import string\n","import torch\n","import transformers\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","import re\n","import sentencepiece as spm\n","import sentencepiece_pb2\n","import gc\n","import html\n","import random\n","from sklearn import model_selection"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"76RJVzb_1wvF","outputId":"8220f49e-fb2a-4834-b4d9-4e3f1b65239f","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1592226752123,"user_tz":-120,"elapsed":119819,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P0NlI3nJ6Kw4","trusted":true,"colab_type":"code","colab":{}},"source":["class SentencePieceTokenizer:\n","    def __init__(self, model_name):\n","        self.sp = spm.SentencePieceProcessor()\n","        self.sp.load(model_name)\n","    \n","    def encode(self, sentence):\n","        spt = sentencepiece_pb2.SentencePieceText()\n","        spt.ParseFromString(self.sp.encode_as_serialized_proto(sentence))\n","        offsets = []\n","        ids = []\n","        for piece in spt.pieces:\n","            ids.append(piece.id)\n","            offsets.append((piece.begin, piece.end))\n","        return {'ids' : ids,\n","                'offsets' : offsets}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiUv9iHz1wvu","trusted":true,"colab_type":"code","colab":{}},"source":["class config:\n","    MAX_LEN = 128\n","    TRAIN_BATCH_SIZE = 16 # 32 #64\n","    VALID_BATCH_SIZE =  16\n","    EPOCHS = 10\n","    \n","    MODEL_CONFIG = transformers.XLNetConfig\n","    MODEL = transformers.XLNetForQuestionAnswering\n","    if IN_COLAB:\n","        BASE_PATH = Path.cwd() / \"drive\" / \"My Drive\" / \"kaggle\" / \"tweet_sentiment_extraction\"     \n","        MODEL_PATH = BASE_PATH  / \"model_save\" / \"model_0613_6\"\n","        FOLDED_TRAINING_FILE = BASE_PATH / \"input\" / \"train-5fold\" / \"train_folds.csv\"\n","        TRAINING_FILE = BASE_PATH / \"input\" / \"train.csv\"\n","        TESTING_FILE = BASE_PATH  / \"input\" / \"test.csv\"\n","        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"sample_submission.csv\"\n","        SUBMISSION_FILE = BASE_PATH / \"input\" / \"submission.csv\"\n","        SLANG_FILE = BASE_PATH / \"input\" / \"slang_abbreviations.csv\"\n","        EMOJIS_FILE = BASE_PATH / \"input\" / \"emojis.csv\"\n","    else:\n","        BASE_PATH = Path('/kaggle')\n","        MODEL_PATH = BASE_PATH  / \"input\" / \"xlnetmodel06136\"\n","        FOLDED_TRAINING_FILE = BASE_PATH / \"working\" / \"train_folds.csv\"\n","        TRAINING_FILE = BASE_PATH  / \"input\" / \"tweet-sentiment-extraction\" / \"train.csv\"\n","        TESTING_FILE = BASE_PATH  / \"input\" / \"tweet-sentiment-extraction\" / \"test.csv\"\n","        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"tweet-sentiment-extraction\" / \"sample_submission.csv\"\n","        SUBMISSION_FILE = BASE_PATH / \"working\" / \"submission.csv\"\n","        SLANG_FILE = BASE_PATH / \"input\" / \"slang-abbreviations\" / \"slang_abbreviations.csv\"\n","        EMOJIS_FILE = BASE_PATH / \"input\" / \"slang-abbreviations\" / \"emojis.csv\"\n","      \n","    \n","    PRETRAINED_MODEL_DIR = BASE_PATH / \"input\" / \"xlnetbasecased\"\n","    TOKENIZER = SentencePieceTokenizer(str(PRETRAINED_MODEL_DIR / 'xlnet-base-cased-spiece.model'))\n","    SLANG_DICT = pd.read_csv(SLANG_FILE, header=None, names=['slang', 'normalised']).set_index('slang').to_dict()['normalised']\n","    EMOJI_DICT = pd.read_csv(EMOJIS_FILE, header=None, names=['emoji', 'normalised']).set_index('emoji').to_dict()['normalised']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AciDwLjC8G77","outputId":"04e7e908-0e4d-498c-c93e-3df27c48e287","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1592226753559,"user_tz":-120,"elapsed":121163,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["[config.TOKENIZER.sp.id_to_piece(x) for x in range(0,10)]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<unk>',\n"," '<s>',\n"," '</s>',\n"," '<cls>',\n"," '<sep>',\n"," '<pad>',\n"," '<mask>',\n"," '<eod>',\n"," '<eop>',\n"," '.']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"UMoIyYf39Zct","outputId":"e1870496-4244-435f-8bea-65678cd055eb","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1592226753561,"user_tz":-120,"elapsed":121043,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["[config.TOKENIZER.sp.piece_to_id(x) for x in ['positive', 'negative', 'neutral']]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[19036, 25976, 24734]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"2hXHgCXG1wv2","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"Z6o-Di3ObeEK","trusted":true,"colab_type":"code","colab":{}},"source":["def seed_everything(seed_value):\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    os.environ['PYTHONHASHSEED'] = str(seed_value)\n","    \n","    if torch.cuda.is_available(): \n","        torch.cuda.manual_seed(seed_value)\n","        torch.cuda.manual_seed_all(seed_value)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = True\n","\n","seed = 24\n","seed_everything(seed) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cg-Brrulbg8P","outputId":"faba53b0-35a7-4b86-8f51-d82b8ddae8f2","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1592226755129,"user_tz":-120,"elapsed":122510,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["def create_train_folds():\n","    df = pd.read_csv(config.TRAINING_FILE)\n","    df = df.dropna().reset_index(drop=True)\n","    df[\"kfold\"] = -1\n","\n","    df = df.sample(frac=1).reset_index(drop=True)\n","\n","    kf = model_selection.StratifiedKFold(n_splits=5, random_state=seed)\n","\n","    for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.sentiment.values)):\n","        print(len(trn_), len(val_))\n","        df.loc[val_, 'kfold'] = fold\n","\n","    df.to_csv(config.FOLDED_TRAINING_FILE, index=False)\n","\n","create_train_folds()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n","  FutureWarning\n"],"name":"stderr"},{"output_type":"stream","text":["21984 5496\n","21984 5496\n","21984 5496\n","21984 5496\n","21984 5496\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wLmNjNd41wv3","trusted":true,"colab_type":"code","colab":{}},"source":["class AverageMeter:\n","    \"\"\"\n","    Computes and stores the average and current value\n","    \"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def jaccard(str1, str2):\n","    a = set(str1.lower().split())\n","    b = set(str2.lower().split())\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","\n","class EarlyStopping:\n","    # https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n","    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n","    def __init__(self, patience=7, verbose=False, delta=0):\n","        \"\"\"\n","        Args:\n","            patience (int): How long to wait after last time validation loss improved.\n","                            Default: 7\n","            verbose (bool): If True, prints a message for each validation loss improvement. \n","                            Default: False\n","            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n","                            Default: 0\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","\n","    def __call__(self, val_loss, model, name):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model, name)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model, name)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model, name):\n","        '''Saves model when validation loss decrease.'''\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), name)\n","        self.val_loss_min = val_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EZ8fCbd41wv6","colab_type":"text"},"source":["## Data processing"]},{"cell_type":"code","metadata":{"id":"MUCcTAMr1wv7","trusted":true,"colab_type":"code","colab":{}},"source":["def update_char_map(char_map, char_pos_patterns, len_repl):\n","\n","  for ent in char_pos_patterns:\n","    ent_pos = ent.span()\n","    char_map[ent_pos[0]+1:] = [char_map[ent_pos[0]]+len_repl-1 if i<ent_pos[1] else char_map[i]-(ent_pos[1]-ent_pos[0])+len_repl for i in range(ent_pos[0]+1,len(char_map)) ]\n","\n","  return(char_map)\n","\n","\n","def normalise_tweet(tweet, slang_dict):#, max_len):\n","  \"\"\"\n","  Clean tweet by decoding html character references, \n","  replacing URLs, removing repeated characters and \n","  replacing slang and emojis.\n","  \"\"\"\n","  char_map = list(range(len(tweet)))\n","  \n","  # bugfix for one sample in train set. (a8048c2ff5)\n","  clean_tweet = re.sub('&not', ' not', tweet)\n","\n","  # replace html  references corresponding with unicode character (e.g. $amp: to &)\n","  clean_tweet = html.unescape(clean_tweet)\n","  # update character position mapping to reflect named character references \n","  # ignores html character references that don't end with ';' to avoid double matches\n","  char_pos_html_entities = []  \n","  [char_pos_html_entities.extend(list(re.finditer('&'+k, tweet))) for k in html.entities.html5.keys() if ((k[-1]==';') or (k=='')) and (re.search('&'+k, tweet))]\n","  char_map = update_char_map(char_map, char_pos_html_entities, 1)\n","\n","  # update character position mapping to reflect _numerical_ character references \n","  char_pos_html_numerical_entities = []  \n","  [char_pos_html_numerical_entities.extend(list(re.finditer('&#'+str(k), tweet))) for k in html.entities.codepoint2name.keys() if re.search('&#'+str(k), tweet)]\n","  char_map = update_char_map(char_map, char_pos_html_numerical_entities, 1)\n","  \n","  # bugfix for one sample in test set.\n","  clean_tweet = re.sub('Â¡', 'Ai', clean_tweet)\n","\n","  # replace URLs with \"URL\"\n","  clean_tweet = re.sub(r'(http://[^\\s-]*)', 'URL', clean_tweet)\n","  # update character position mapping to reflect this\n","  if re.search(r'(http://[^\\s-]*)', tweet):\n","    char_pos_urls = re.finditer(r'(http://[^\\s-]*)', tweet)\n","    char_map = update_char_map(char_map, char_pos_urls, 3)\n","\n","  # replace unicode replacement character with \"'\"\n","  clean_tweet = re.sub(r'(ï¿½)', \"'\", clean_tweet)\n","  # update character position mapping to reflect this\n","  if re.search(r'(ï¿½)', tweet):\n","    char_pos_urls = re.finditer(r'(ï¿½)', tweet)\n","    char_map = update_char_map(char_map, char_pos_urls, 1)\n"," \n","\n","  # replace letters or exclamation marks that are repeated >2 times consecutively (except \"www.\")\n","  # with a single character (e.g. sorryyyyyy -> sorry)\n","  # doesn't work perfectly, e.g. sleeeeeeep. -> slep.\n","  clean_tweet = re.sub(r'(?!www.)([a-zA-Z\\!])\\1{2,}', '\\\\1', clean_tweet, flags=re.I)\n","  # update character position mapping to reflect this\n","  # note: urls are replaced with #s so that any repetitions within URL are ignored\n","  char_pos_repeats = re.finditer(r'(?!www.)([a-zA-Z\\!])\\1{2,}', re.sub(r'(http://[^\\s-]*)|(ï¿½)', lambda x: '#'*len(x.group()), tweet), flags=re.I)\n","  char_map = update_char_map(char_map, char_pos_repeats, 1)\n","\n","  # # replace slang abbreviations with real words\n","  # # https://www.webopedia.com/quick_ref/textmessageabbreviations.asp\n","  new_clean_tweet = ''\n","  change = 0\n","  for i,e in enumerate(re.finditer('([0-9a-zA-Z]+|[^0-9a-zA-Z]+)', clean_tweet)):\n","    \n","    if e.group().upper() in config.SLANG_DICT.keys():\n","      new_clean_tweet = new_clean_tweet + config.SLANG_DICT[e.group().upper()]\n","\n","      # 1. Find position of chunk to be replaced in 'clean_tweet'\n","      # 2. Use that position to locate the corresponding chunk in the  char_map\n","      # 3. Update that chunk and everything after it in char_map using the position that that chunk will have (not its position in clean_tweet)\n","      len_repl = len(config.SLANG_DICT[e.group().upper()])\n","      ent_pos = e.span()\n","      \n","      ent_pos = tuple(x+change for x in ent_pos)\n","\n","      min_ix = min([j for j,x in enumerate(char_map) if x>=ent_pos[0]])\n","      max_ix = max([j for j,x in enumerate(char_map) if x<ent_pos[1]])\n","\n","      char_map[min_ix+1:] = [ent_pos[0]+len_repl-1 if k<max_ix else char_map[k]-(ent_pos[1]-ent_pos[0])+len_repl for k in range(min_ix+1, len(char_map))]\n","      change += len_repl - (ent_pos[1]-ent_pos[0])\n","    else:\n","      new_clean_tweet = new_clean_tweet + e.group()\n","\n","  clean_tweet = new_clean_tweet\n","\n","  # # replace emoticons with real words\n","  # # https://en.wikipedia.org/wiki/List_of_emoticons\n","  new_clean_tweet = ''\n","  change = 0\n","  for i,e in enumerate(re.finditer('(\\s+|\\S+)', clean_tweet)):\n","    \n","    if e.group().upper() in config.EMOJI_DICT.keys():\n","      new_clean_tweet = new_clean_tweet + config.EMOJI_DICT[e.group().upper()]\n","\n","      # 1. Find position of chunk to be replaced in 'clean_tweet'\n","      # 2. Use that position to locate the corresponding chunk in the  char_map\n","      # 3. Update that chunk and everything after it in char_map using the position that that chunk will have (not its position in clean_tweet)\n","      len_repl = len(config.EMOJI_DICT[e.group().upper()])\n","      ent_pos = e.span()\n","      \n","      ent_pos = tuple(x+change for x in ent_pos)\n","\n","      min_ix = min([j for j,x in enumerate(char_map) if x>=ent_pos[0]])\n","      max_ix = max([j for j,x in enumerate(char_map) if x<ent_pos[1]])\n","\n","      char_map[min_ix+1:] = [ent_pos[0]+len_repl-1 if k<max_ix else char_map[k]-(ent_pos[1]-ent_pos[0])+len_repl for k in range(min_ix+1, len(char_map))]\n","      change += len_repl - (ent_pos[1]-ent_pos[0])\n","    else:\n","      new_clean_tweet = new_clean_tweet + e.group()\n","\n","  clean_tweet = new_clean_tweet\n","  \n","  #char_map = [min(x, max_len-1) for x in char_map]\n","\n","  char_map_inverse = (pd.Series([max([j for j,k in enumerate(char_map) if k==i], default=None) for i in range(len(clean_tweet))])\n","  .fillna(method='backfill')\n","  .fillna(len(tweet)-1)\n","  .astype(int)\n","  .values\n","  .tolist())\n","\n","  return clean_tweet, char_map, char_map_inverse\n","  \n","  \n","def process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict):\n","    \"\"\"\n","    Preprocessing the data to the XLNet model formatting\n","    \"\"\"\n","#     tweet = \n","#     selected_text = \n","\n","    raw_tweet = \" \" + \" \".join(str(tweet).split()) #tweet\n","    raw_selected_text = \" \" + \" \".join(str(selected_text).split()) #selected_text\n","\n","    # find start and indices of selected_text in tweet\n","    len_st = len(raw_selected_text) - 1\n","    raw_idx0 = None\n","    raw_idx1 = None\n","\n","    for ind in (i for i, e in enumerate(raw_tweet) if e == raw_selected_text[1]):\n","      if \" \" + raw_tweet[ind: ind+len_st] == raw_selected_text:\n","            raw_idx0 = ind\n","            raw_idx1 = ind + len_st - 1\n","            break\n","\n","    tweet, char_map, char_map_inverse = normalise_tweet(raw_tweet, slang_dict)#, max_len)\n","    \n","    try:\n","        idx0 = char_map[raw_idx0]\n","        idx1 = char_map[raw_idx1]\n","    except:\n","        print('raw tweet: '+str(raw_tweet))\n","        print('cleaned tweet: '+str(tweet))\n","        print('rawidx0: '+str(raw_idx0))\n","        print('idx0: '+str(idx0))\n","        print('rawidx1: '+str(raw_idx1))\n","        print('char_map: '+str(char_map))\n","        print('len char_map: '+str(len(char_map)))\n","        raise\n","    \n","    selected_text = tweet[idx0:(idx1+1)]\n","\n","    try:\n","        # create character mask for selected_text in tweet\n","        char_targets = [0] * len(tweet)\n","        if idx0 != None and idx1 != None:\n","            for ct in range(idx0, idx1 + 1):\n","                char_targets[ct] = 1\n","    except:\n","        print('raw tweet: '+str(raw_tweet))\n","        print('cleaned tweet: '+str(tweet))\n","        print('char_targets: '+str(char_targets))\n","        print('char map: '+str(char_map))\n","        print(len(char_map))\n","        print(len(char_targets))\n","        print('idx0: '+str(idx0))\n","        print('idx1: '+str(idx1))\n","        raise\n","    \n","    tok_tweet = tokenizer.encode(tweet)\n","    \n","    input_ids_orig = tok_tweet['ids']\n","    tweet_offsets = tok_tweet['offsets']\n","    \n","    target_idx = []\n","    for j, (offset1, offset2) in enumerate(tweet_offsets):\n","        if sum(char_targets[offset1: offset2]) > 0:\n","            target_idx.append(j)\n","    \n","\n","    try:\n","      targets_start = target_idx[0]\n","      targets_end = target_idx[-1]\n","    except:\n","      print(idx0)\n","      print(idx1)\n","      print(char_targets)\n","      print(tweet)\n","      print(selected_text)\n","      print(target_idx)\n","      raise\n","\n","    #######\n","    sentiment_id = {\n","        'positive': 19036,\n","        'negative': 25976,\n","        'neutral': 24734\n","    }\n","    #######\n","    \n","    # https://huggingface.co/transformers/model_doc/xlnet.html#transformers.XLNetTokenizer.build_inputs_with_special_tokens\n","    input_ids = [sentiment_id[sentiment]] + [4] + input_ids_orig + [4] + [3]\n","    #input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n","    token_type_ids = [0]*2 + [1] * (len(input_ids_orig)+1) + [2]\n","    mask = [1] * len(token_type_ids)\n","    tweet_offsets = [(0, 0)] * 2 + tweet_offsets + [(0, 0)] * 2\n","    targets_start += 2\n","    targets_end += 2\n","\n","    padding_length = max_len - len(input_ids)\n","    if padding_length > 0:\n","        input_ids = input_ids + ([5] * padding_length)\n","        mask = mask + ([0] * padding_length)\n","        token_type_ids = token_type_ids + ([0] * padding_length)\n","        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n","    \n","    return {\n","        'ids': input_ids,\n","        'mask': mask,\n","        'token_type_ids': token_type_ids,\n","        'targets_start': targets_start,\n","        'targets_end': targets_end,\n","        'orig_tweet': tweet,\n","        'orig_selected': selected_text,\n","        'sentiment': sentiment,\n","        'offsets': tweet_offsets,\n","        'raw_tweet': raw_tweet,\n","        'raw_selected_text': raw_selected_text,\n","        'char_map_inverse': \"_\".join([str(x) for x in char_map_inverse]),\n","        'char_map': str(char_map)\n","    }"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HaLOn6TJao5y","colab_type":"code","colab":{}},"source":["# tweet = \"&not gonna lie. it`s 60 degrees in here. thanks for leavin me your sweater molly. brrrrr\"\n","# selected_text = 'thanks'\n","\n","# raw_tweet = \" \" + \" \".join(str(tweet).split()) #tweet\n","# raw_selected_text = \" \" + \" \".join(str(selected_text).split()) #selected_text\n","# print(raw_tweet)\n","# print(raw_selected_text)\n","# print(raw_selected_text[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6bEyOxMtq5z","colab_type":"code","colab":{}},"source":["# tweet, char_map, char_map_inverse = normalise_tweet(raw_tweet, config.SLANG_DICT)#, max_len)\n","# print(tweet)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LD3dWEz1uVf4","colab_type":"code","colab":{}},"source":["# tok_tweet = config.TOKENIZER.encode(tweet)\n","\n","# input_ids_orig = tok_tweet['ids']\n","# tweet_offsets = tok_tweet['offsets']\n","# print(tweet)\n","# print([config.TOKENIZER.sp.id_to_piece(x) for x in input_ids_orig])\n","# print(tweet_offsets)\n","# print([tweet[o[0]:o[1]] for o in tweet_offsets])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zFNgb6lLQAmq","colab_type":"code","colab":{}},"source":["\n","# # find start and indices of selected_text in tweet\n","# len_st = len(raw_selected_text) - 1\n","# raw_idx0 = None\n","# raw_idx1 = None\n","\n","# for ind in (i for i, e in enumerate(raw_tweet) if e == raw_selected_text[1]):\n","#   if \" \" + raw_tweet[ind: ind+len_st] == raw_selected_text:\n","#         raw_idx0 = ind\n","#         raw_idx1 = ind + len_st - 1\n","#         break\n","# print(raw_idx0)\n","# print(raw_idx1)\n","# print(raw_tweet[ind: ind+len_st])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nLsq4UJxQd1_","colab_type":"code","colab":{}},"source":["\n","# list(enumerate(raw_tweet))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"-Ga3_rDjD9uj","colab_type":"code","colab":{}},"source":["# tweet = \"&not gonna lie. it`s 60 degrees in here. thanks for leavin me your sweater molly. brrrrr\"\n","# selected_text = 'thanks'\n","# sentiment = 'positive'\n","# tokenizer = config.TOKENIZER\n","# max_len = 128\n","# slang_dict = config.SLANG_DICT\n","\n","# print(process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)['offsets'])\n","# print([config.TOKENIZER.sp.id_to_piece(x) for x in process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)['ids']])\n","# print(process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)['orig_tweet'])\n","# print(process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)['raw_tweet'])\n","\n","\n","\n","#    char_map_inverse = [int(x) for x in char_map_inverse.split('_')]\n","# raw_char_idx_start = char_map_inverse[int(offsets[idx_start][0])]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y-eRe4x11wv-","colab_type":"text"},"source":["## Data loader"]},{"cell_type":"code","metadata":{"id":"4s9Id3hW1wv-","trusted":true,"colab_type":"code","colab":{}},"source":["class TweetDataset:\n","    def __init__(self, tweet, sentiment, selected_text):\n","        self.tweet = tweet\n","        self.sentiment = sentiment\n","        self.selected_text = selected_text\n","        self.tokenizer = config.TOKENIZER\n","        self.max_len = config.MAX_LEN\n","        self.slang_dict = config.SLANG_DICT\n","    \n","    def __len__(self):\n","        return len(self.tweet)\n","\n","    def __getitem__(self, item):\n","        data = process_data(\n","            self.tweet[item], \n","            self.selected_text[item], \n","            self.sentiment[item],\n","            self.tokenizer,\n","            self.max_len,\n","            self.slang_dict\n","        )\n","\n","        return {\n","            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n","            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n","            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n","            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n","            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n","            'orig_tweet': data[\"orig_tweet\"],\n","            'orig_selected': data[\"orig_selected\"],\n","            'sentiment': data[\"sentiment\"],\n","            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\n","            'raw_tweet': data[\"raw_tweet\"],\n","            'raw_selected_text': data['raw_selected_text'],\n","            'char_map_inverse': data[\"char_map_inverse\"],\n","            'char_map': data[\"char_map\"]\n","        }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GXJ2VElJ1wwI","colab_type":"text"},"source":["## Loss function"]},{"cell_type":"code","metadata":{"id":"hFNXnE3u1wwJ","trusted":true,"colab_type":"code","colab":{}},"source":["# def loss_fn(start_logits, end_logits, start_positions, end_positions):\n","#     loss_fct = nn.CrossEntropyLoss()\n","#     start_loss = loss_fct(start_logits, start_positions)\n","#     end_loss = loss_fct(end_logits, end_positions)\n","#     total_loss = (start_loss + end_loss)\n","#     return total_loss\n","\n","def loss_fn(start_logprobs, end_logprobs, start_positions, end_positions):\n","    loss_fct = nn.NLLLoss()\n","    start_loss = loss_fct(start_logits, start_positions)\n","    end_loss = loss_fct(end_logits, end_positions)\n","    total_loss = (start_loss + end_loss)\n","    return total_loss\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCxxX5Jw1wwM","colab_type":"text"},"source":["## Training function"]},{"cell_type":"code","metadata":{"id":"NL1vcjOa1wwN","trusted":true,"colab_type":"code","colab":{}},"source":["def train_fn(data_loader, model, optimizer, device, scheduler=None):\n","    model.train()\n","    losses = AverageMeter()\n","    jaccards = AverageMeter()\n","\n","    tk0 = tqdm(data_loader, total=len(data_loader))\n","    \n","    for bi, d in enumerate(tk0):\n","\n","        ids = d[\"ids\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        mask = d[\"mask\"]\n","        targets_start = d[\"targets_start\"]\n","        targets_end = d[\"targets_end\"]\n","        sentiment = d[\"sentiment\"]\n","        orig_selected = d[\"orig_selected\"]\n","        orig_tweet = d[\"orig_tweet\"]\n","        targets_start = d[\"targets_start\"]\n","        targets_end = d[\"targets_end\"]\n","        offsets = d[\"offsets\"]\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        targets_start = targets_start.to(device, dtype=torch.long)\n","        targets_end = targets_end.to(device, dtype=torch.long)\n","\n","        model.zero_grad()\n","        \n","        # outputs_start, outputs_end = model(\n","        #     ids=ids,\n","        #     mask=mask,\n","        #     token_type_ids=token_type_ids,\n","        # )\n","        # loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n","        # loss.backward()\n","\n","        outputs = model(\n","            input_ids=ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids,\n","            start_positions=targets_start, \n","            end_positions=targets_end\n","        )\n","        \n","        loss = outputs[0]\n","        loss.backward()\n","\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n","        # outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n","        \n","        # jaccard_scores = []\n","        # for px, tweet in enumerate(orig_tweet):\n","        #     selected_tweet = orig_selected[px]\n","        #     tweet_sentiment = sentiment[px]\n","        #     jaccard_score, _ = calculate_jaccard_score(\n","        #         original_tweet=tweet,\n","        #         target_string=selected_tweet,\n","        #         sentiment_val=tweet_sentiment,\n","        #         idx_start=np.argmax(outputs_start[px, :]),\n","        #         idx_end=np.argmax(outputs_end[px, :]),\n","        #         offsets=offsets[px]\n","        #     )\n","        #     jaccard_scores.append(jaccard_score)\n","\n","        # jaccards.update(np.mean(jaccard_scores), ids.size(0))\n","        losses.update(loss.item(), ids.size(0))\n","        tk0.set_postfix(loss=losses.avg)#, jaccard=jaccards.avg)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jalt8e9B1wwQ","colab_type":"text"},"source":["## Evaluation function"]},{"cell_type":"code","metadata":{"id":"chIyiuLB1wwR","trusted":true,"colab_type":"code","colab":{}},"source":["def calculate_jaccard_score(\n","    raw_tweet, \n","    target_string, \n","    sentiment_val, \n","    idx_start, \n","    idx_end, \n","    offsets,\n","    char_map_inverse,\n","    cleaned_tweet=None,\n","    verbose=False):\n","    \n","    char_map_inverse = [int(x) for x in char_map_inverse.split('_')]\n","\n","    if idx_end < idx_start:\n","        idx_end = idx_start\n","        \n","    # raw_char_idx_start = int(max([i for i,e in enumerate(char_map) if e<=offsets[idx_start][0]]+[0]))\n","    # raw_char_idx_end = int(min([i for i,e in enumerate(char_map) if e>=offsets[idx_end][1]]+[len(char_map)]))\n","\n","    raw_char_idx_start = char_map_inverse[int(offsets[idx_start][0])]\n","    try:\n","      raw_char_idx_end = char_map_inverse[int(offsets[idx_end][1])-1]\n","    except:\n","      print('\\nraw tweet: '+str(raw_tweet))\n","      print('cleaned tweet: '+str(cleaned_tweet))\n","      print('char map: '+str(char_map_inverse))\n","      print('index start:'+str(idx_start))\n","      print('index end:'+str(idx_end))\n","      print('offsets: '+str(offsets))\n","      print(len(offsets))\n","      print(len(char_map_inverse))\n","      print(offsets[idx_end])\n","      print(offsets[idx_end][1])\n","      print(int(offsets[idx_end][1]))\n","      print(char_map_inverse[int(offsets[idx_end][1])])\n","      raise()\n","\n","    # filtered_output  = \"\"\n","    # for ix in range(idx_start, idx_end + 1):\n","    #     filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n","\n","    #     # add spacing to output\n","    #     if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n","    #         filtered_output += \" \"\n","\n","    filtered_output = raw_tweet[raw_char_idx_start:raw_char_idx_end+1]\n","\n","    if sentiment_val == \"neutral\" or len(raw_tweet.split()) < 2:\n","        filtered_output = raw_tweet\n","\n","    if sentiment_val != \"neutral\" and verbose == True:\n","        if filtered_output.strip().lower() != target_string.strip().lower():\n","            print(\"********************************\")\n","            print(f\"Output= {filtered_output.strip()}\")\n","            print(f\"Target= {target_string.strip()}\")\n","            print(f\"Tweet= {raw_tweet.strip()}\")\n","            print(\"********************************\")\n","\n","    jac = jaccard(target_string.strip(), filtered_output.strip())\n","    return jac, filtered_output\n","\n","\n","def eval_fn(data_loader, model, device):\n","    model.eval()\n","    losses = AverageMeter()\n","    jaccards = AverageMeter()\n","    \n","    with torch.no_grad():\n","        tk0 = tqdm(data_loader, total=len(data_loader))\n","        for bi, d in enumerate(tk0):\n","            ids = d[\"ids\"]\n","            token_type_ids = d[\"token_type_ids\"]\n","            mask = d[\"mask\"]\n","            sentiment = d[\"sentiment\"]\n","            orig_selected = d[\"orig_selected\"]\n","            orig_tweet = d[\"orig_tweet\"]\n","            targets_start = d[\"targets_start\"]\n","            targets_end = d[\"targets_end\"]\n","            offsets = d[\"offsets\"]\n","            raw_tweet = d[\"raw_tweet\"]\n","            raw_selected = d[\"raw_selected_text\"]\n","            char_map_inverse = d[\"char_map_inverse\"]\n","\n","            ids = ids.to(device, dtype=torch.long)\n","            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","            mask = mask.to(device, dtype=torch.long)\n","            targets_start = targets_start.to(device, dtype=torch.long)\n","            targets_end = targets_end.to(device, dtype=torch.long)\n","            #char_map_inverse = char_map_inverse.to(device, dtype=torch.long)\n","\n","            outputs = model(\n","                input_ids=ids,\n","                attention_mask=mask,\n","                token_type_ids=token_type_ids,\n","                start_positions=targets_start,\n","                end_positions=targets_end\n","            )\n","            loss = outputs[0]\n","            \n","            # run it again to get the probabilities\n","            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n","            outputs = model(\n","                input_ids=ids,\n","                attention_mask=mask,\n","                token_type_ids=token_type_ids\n","            )\n","            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n","            start_top_probs = outputs[0]  \n","          \n","            # start_top_probs contain those positions' probabilities\n","            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n","            start_top_index = outputs[1] \n","\n","            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n","            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n","            # this can be verified by checking summation to unity\n","            end_top_probs = outputs[2] \n","            end_top_index = outputs[3] \n","            \n","            # calculate joint probability of start, end position tuples\n","            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n","\n","            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n","            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n","            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n","\n","            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n","                      indices]\n","\n","            # get (flat) position in sequence of highest probability tuple\n","            top_start_end_probs_sorted = start_end_probs_sorted.argmax(dim=1)\n","\n","            # convert flat position to separate start and end positions\n","            start_top_positions = (top_start_end_probs_sorted % torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n","            end_top_positions = (top_start_end_probs_sorted // torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n","            \n","            jaccard_scores = []\n","            for px, tweet in enumerate(raw_tweet):\n","                tweet_raw_selected_text = raw_selected[px]\n","                tweet_sentiment = sentiment[px]\n","                tweet_offsets = offsets[px]\n","                tweet_char_map_inverse = char_map_inverse[px]\n","\n","                start_top_position = start_top_positions[px]\n","                end_top_position = end_top_positions[px]\n","                \n","                cleaned_tweet = orig_tweet[px]\n","                \n","                jaccard_score, _ = calculate_jaccard_score(\n","                    raw_tweet=tweet,\n","                    target_string=tweet_raw_selected_text,\n","                    sentiment_val=tweet_sentiment,\n","                    idx_start=start_top_position,\n","                    idx_end=end_top_position,\n","                    offsets=tweet_offsets,\n","                    char_map_inverse=tweet_char_map_inverse,\n","                    cleaned_tweet=cleaned_tweet\n","                )\n","                jaccard_scores.append(jaccard_score)\n","\n","            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n","            losses.update(loss.item(), ids.size(0))\n","            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n","    \n","    print(f\"Jaccard = {jaccards.avg}\")\n","    print(f\"Loss = {losses.avg}\")\n","    return jaccards.avg, losses.avg"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sWiozMAq1wwW","colab_type":"text"},"source":["## Training "]},{"cell_type":"code","metadata":{"id":"el1CDathhxr8","trusted":true,"colab_type":"code","colab":{}},"source":["def init_model(config):\n","    model_config = config.MODEL_CONFIG.from_pretrained(config.PRETRAINED_MODEL_DIR )#/ \"config.json\")\n","    model_config.output_hidden_states = True\n","    model_config.start_n_top = config.MAX_LEN\n","    model_config.end_n_top = config.MAX_LEN\n","    #'/kaggle/input/xlnet-base-tf/xlnet-base-cased'\n","    model = config.MODEL.from_pretrained(config.PRETRAINED_MODEL_DIR, config=model_config)#, state_dict='/kaggle/input/xlnetmodel05081/model_3.bin')\n","    \n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0tHJ24G1wwW","trusted":true,"colab_type":"code","colab":{}},"source":["def run_fold(fold):\n","\n","    dfx = pd.read_csv(config.FOLDED_TRAINING_FILE)\n","\n","    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n","    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n","\n","    train_dataset = TweetDataset(\n","        tweet=df_train.text.values,\n","        sentiment=df_train.sentiment.values,\n","        selected_text=df_train.selected_text.values\n","    )\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.TRAIN_BATCH_SIZE,\n","        num_workers=4\n","    )\n","\n","    valid_dataset = TweetDataset(\n","        tweet=df_valid.text.values,\n","        sentiment=df_valid.sentiment.values,\n","        selected_text=df_valid.selected_text.values\n","    )\n","\n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=config.VALID_BATCH_SIZE,\n","        num_workers=2\n","    )\n","    \n","    device = torch.device(\"cuda\")\n","\n","    # initialise model\n","    model = init_model(config)\n","    \n","    model.to(device)\n","\n","    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, \n","        num_warmup_steps=0, \n","        num_training_steps=num_train_steps\n","    )\n","\n","    es = EarlyStopping(patience=2, verbose=True)\n","    print(f\"Training is Starting for fold={fold}\")\n","    \n","    for epoch in range(config.EPOCHS):\n","        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n","        jaccard, loss = eval_fn(valid_data_loader, model, device)\n","        print(f\"Jaccard Score = {jaccard}\")\n","        print(f\"Loss score = {loss}\")\n","        es(loss, model, name=config.MODEL_PATH / f\"model_{fold}.bin\")\n","        \n","        if es.early_stop:\n","            print(\"Early stopping\")\n","            break\n","  \n","    return es.val_loss_min"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NYeJuSDoaqXJ","colab_type":"code","colab":{}},"source":["def run_val_fold(fold):\n","\n","    dfx = pd.read_csv(config.FOLDED_TRAINING_FILE)\n","\n","    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n","    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n","\n","    train_dataset = TweetDataset(\n","        tweet=df_train.text.values,\n","        sentiment=df_train.sentiment.values,\n","        selected_text=df_train.selected_text.values\n","    )\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=config.TRAIN_BATCH_SIZE,\n","        num_workers=4\n","    )\n","\n","    valid_dataset = TweetDataset(\n","        tweet=df_valid.text.values,\n","        sentiment=df_valid.sentiment.values,\n","        selected_text=df_valid.selected_text.values\n","    )\n","\n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        batch_size=config.VALID_BATCH_SIZE,\n","        num_workers=2\n","    )\n","    \n","    device = torch.device(\"cuda\")\n","\n","    print(f\"Evaluating fold={fold}\")\n","\n","    # initialise model\n","    model = init_model(config)\n","    model_filename = 'model_'+str(fold)+'.bin'\n","    model.load_state_dict(torch.load(config.MODEL_PATH / model_filename, map_location=device))\n","    model.to(device)\n","    \n","    jaccard, loss = eval_fn(valid_data_loader, model, device)\n","\n","    return jaccard"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2uAP683Z1wwZ","colab_type":"text"},"source":["## Run training"]},{"cell_type":"code","metadata":{"id":"mUwMNkb1Hq3w","trusted":true,"colab_type":"code","colab":{}},"source":["def run_training():\n","  if not os.path.exists(config.MODEL_PATH):\n","    os.mkdir(config.MODEL_PATH)\n","  val_loss = []\n","  for ifold in [0]:#range(5):\n","      q = run_fold(ifold)\n","      val_loss.append(q)\n","  print(f'Mean val loss: {np.mean(val_loss)}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JA7aV6JmajL4","colab_type":"code","colab":{}},"source":["def get_cv_loss():\n","  val_jaccard = []\n","  for ifold in range(5):\n","      q = run_val_fold(ifold)\n","      val_jaccard.append(q)\n","  print(f'Mean val loss: {np.mean(val_jaccard)}')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iyui81xMHLqQ","colab_type":"text"},"source":["## Predict test set"]},{"cell_type":"code","metadata":{"id":"pW0jJHfUkzTs","trusted":true,"colab_type":"code","colab":{}},"source":["def predict_test():\n","  df_test = pd.read_csv(config.TESTING_FILE)\n","  df_test.loc[:, \"selected_text\"] = df_test.text.values\n","\n","  models = []\n","\n","  for mf in os.listdir(config.MODEL_PATH):\n","    m = init_model(config)\n","    \n","    m.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n","    print(config.MODEL_PATH / mf)\n","    m.eval()\n","    # ensure we get output probabilities for all combinations of start and end position\n","    m.start_n_top = config.MAX_LEN\n","    m.end_n_top = config.MAX_LEN\n","    m.to(device)\n","\n","    models.append(m)\n","\n","  test_dataset = TweetDataset(\n","          tweet=df_test.text.values,\n","          sentiment=df_test.sentiment.values,\n","          selected_text=df_test.selected_text.values\n","      )\n","\n","  test_data_loader = torch.utils.data.DataLoader(\n","      test_dataset,\n","      shuffle=False,\n","      batch_size=config.VALID_BATCH_SIZE,\n","      num_workers=1\n","  )\n","\n","  final_output = []\n","\n","  with torch.no_grad():\n","      tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n","      for bi, d in enumerate(tk0):\n","          ids = d[\"ids\"]\n","          token_type_ids = d[\"token_type_ids\"]\n","          mask = d[\"mask\"]\n","          sentiment = d[\"sentiment\"]\n","          orig_selected = d[\"orig_selected\"]\n","          orig_tweet = d[\"orig_tweet\"]\n","          targets_start = d[\"targets_start\"]\n","          targets_end = d[\"targets_end\"]\n","          offsets = d[\"offsets\"].numpy()\n","          raw_tweet = d[\"raw_tweet\"]\n","          raw_selected = d[\"raw_selected_text\"]\n","          char_map_inverse = d[\"char_map_inverse\"]\n","\n","          ids = ids.to(device, dtype=torch.long)\n","          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","          mask = mask.to(device, dtype=torch.long)\n","          targets_start = targets_start.to(device, dtype=torch.long)\n","          targets_end = targets_end.to(device, dtype=torch.long)\n","          \n","          summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n","\n","          for model in models: \n","            # run it again to get the probabilities\n","            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n","            outputs = model(\n","                input_ids=ids,\n","                attention_mask=mask,\n","                token_type_ids=token_type_ids\n","            )\n","\n","            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n","            start_top_probs = outputs[0]  \n","          \n","            # start_top_probs contain those positions' probabilities\n","            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n","            start_top_index = outputs[1] \n","\n","            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n","            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n","            # this can be verified by checking summation to unity\n","            end_top_probs = outputs[2] \n","            end_top_index = outputs[3] \n","            \n","            # calculate joint probability of start, end position tuples\n","            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n","\n","            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n","            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n","            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n","\n","            #start_end_probs_sorted = start_end_probs[torch.arange(start_end_probs.shape[0]), indices]\n","            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n","                      indices]\n","\n","            summed_start_end_probs_sorted += start_end_probs_sorted\n","\n","          avg_start_end_probs_sorted = summed_start_end_probs_sorted/torch.tensor(len(models))\n","\n","          # get (flat) position in sequence of highest probability tuple\n","          top_avg_start_end_probs_sorted = avg_start_end_probs_sorted.argmax(dim=1)\n","\n","          # convert flat position to separate start and end positions\n","          start_top_positions = (top_avg_start_end_probs_sorted % torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n","          end_top_positions = (top_avg_start_end_probs_sorted // torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n","          \n","          jaccard_scores = []\n","          for px, tweet in enumerate(raw_tweet):\n","              raw_selected_text = raw_selected[px]\n","              tweet_sentiment = sentiment[px]\n","              tweet_offsets = offsets[px]\n","              tweet_char_map_inverse = char_map_inverse[px]\n","\n","              start_top_position = start_top_positions[px]\n","              end_top_position = end_top_positions[px]\n","                \n","              cleaned_tweet = orig_tweet[px]\n","\n","              _, output_sentence = calculate_jaccard_score(\n","                  raw_tweet=tweet,\n","                  target_string=raw_selected_text,\n","                  sentiment_val=tweet_sentiment,\n","                  idx_start=start_top_position,\n","                  idx_end=end_top_position,\n","                  offsets=tweet_offsets,\n","                  char_map_inverse=tweet_char_map_inverse,\n","                  cleaned_tweet=cleaned_tweet,\n","                  verbose=True\n","              )\n","              final_output.append(output_sentence)\n","\n","\n","  sample = pd.read_csv(config.SAMPLE_SUBMISSION_FILE)\n","  sample.loc[:, 'selected_text'] = final_output\n","  sample.to_csv(\"submission.csv\", index=False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"jgxuK3BSD9vL","colab_type":"code","colab":{}},"source":["def predict_test():\n","  df_test = pd.read_csv(config.TESTING_FILE)\n","  df_test.loc[:, \"selected_text\"] = df_test.text.values\n","\n","  models = []\n","\n","  for mf in os.listdir(config.MODEL_PATH):\n","    m = init_model(config)\n","    \n","    m.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n","    print(config.MODEL_PATH / mf)\n","    m.eval()\n","    # ensure we get output probabilities for all combinations of start and end position\n","    m.start_n_top = config.MAX_LEN\n","    m.end_n_top = config.MAX_LEN\n","    m.to(device)\n","\n","    models.append(m)\n","\n","  test_dataset = TweetDataset(\n","          tweet=df_test.text.values,\n","          sentiment=df_test.sentiment.values,\n","          selected_text=df_test.selected_text.values\n","      )\n","\n","  test_data_loader = torch.utils.data.DataLoader(\n","      test_dataset,\n","      shuffle=False,\n","      batch_size=config.VALID_BATCH_SIZE,\n","      num_workers=1\n","  )\n","\n","  final_output = []\n","\n","  with torch.no_grad():\n","      tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n","      for bi, d in enumerate(tk0):\n","          ids = d[\"ids\"]\n","          token_type_ids = d[\"token_type_ids\"]\n","          mask = d[\"mask\"]\n","          sentiment = d[\"sentiment\"]\n","          orig_selected = d[\"orig_selected\"]\n","          orig_tweet = d[\"orig_tweet\"]\n","          targets_start = d[\"targets_start\"]\n","          targets_end = d[\"targets_end\"]\n","          offsets = d[\"offsets\"].numpy()\n","          raw_tweet = d[\"raw_tweet\"]\n","          raw_selected = d[\"raw_selected_text\"]\n","          char_map_inverse = d[\"char_map_inverse\"]\n","\n","          ids = ids.to(device, dtype=torch.long)\n","          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","          mask = mask.to(device, dtype=torch.long)\n","          targets_start = targets_start.to(device, dtype=torch.long)\n","          targets_end = targets_end.to(device, dtype=torch.long)\n","          \n","          summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n","\n","          for model in models: \n","            # run it again to get the probabilities\n","            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n","            outputs = model(\n","                input_ids=ids,\n","                attention_mask=mask,\n","                token_type_ids=token_type_ids\n","            )\n","\n","            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n","            start_top_probs = outputs[0]  \n","          \n","            # start_top_probs contain those positions' probabilities\n","            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n","            start_top_index = outputs[1] \n","\n","            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n","            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n","            # this can be verified by checking summation to unity\n","            end_top_probs = outputs[2] \n","            end_top_index = outputs[3] \n","            \n","            # calculate joint probability of start, end position tuples\n","            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n","\n","            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n","            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n","            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n","\n","            #start_end_probs_sorted = start_end_probs[torch.arange(start_end_probs.shape[0]), indices]\n","            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n","                      indices]\n","\n","            summed_start_end_probs_sorted += start_end_probs_sorted\n","\n","          avg_start_end_probs_sorted = summed_start_end_probs_sorted/torch.tensor(len(models))\n","\n","          # get (flat) position in sequence of highest probability tuple\n","          top_avg_start_end_probs_sorted = avg_start_end_probs_sorted.argmax(dim=1)\n","\n","          # convert flat position to separate start and end positions\n","          start_top_positions = (top_avg_start_end_probs_sorted % torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n","          end_top_positions = (top_avg_start_end_probs_sorted // torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n","          \n","          jaccard_scores = []\n","          for px, tweet in enumerate(raw_tweet):\n","              raw_selected_text = raw_selected[px]\n","              tweet_sentiment = sentiment[px]\n","              tweet_offsets = offsets[px]\n","              tweet_char_map_inverse = char_map_inverse[px]\n","\n","              start_top_position = start_top_positions[px]\n","              end_top_position = end_top_positions[px]\n","                \n","              cleaned_tweet = orig_tweet[px]\n","\n","              _, output_sentence = calculate_jaccard_score(\n","                  raw_tweet=tweet,\n","                  target_string=raw_selected_text,\n","                  sentiment_val=tweet_sentiment,\n","                  idx_start=start_top_position,\n","                  idx_end=end_top_position,\n","                  offsets=tweet_offsets,\n","                  char_map_inverse=tweet_char_map_inverse,\n","                  cleaned_tweet=cleaned_tweet,\n","                  verbose=True\n","              )\n","              final_output.append(output_sentence)\n","\n","\n","  sample = pd.read_csv(config.SAMPLE_SUBMISSION_FILE)\n","  sample.loc[:, 'selected_text'] = final_output\n","  sample.to_csv(\"predictions_voting.csv\", index=False)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"G_mf6eF6D9vO","colab_type":"code","colab":{}},"source":["def gen_probs_test():\n","  df_test = pd.read_csv(config.TESTING_FILE)\n","\n","  models = []\n","\n","  for mf in os.listdir(config.MODEL_PATH):#[0:1]:\n","    if not mf.endswith('.bin'):\n","        continue\n","    m = init_model(config)\n","    \n","    m.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n","    print(config.MODEL_PATH / mf)\n","    m.eval()\n","    # ensure we get output probabilities for all combinations of start and end position\n","    m.start_n_top = config.MAX_LEN#2#config.MAX_LEN\n","    m.end_n_top = config.MAX_LEN#3#\n","    m.to(device)\n","\n","    models.append(m)\n","\n","  test_dataset = TweetDataset(\n","          tweet=df_test.text.values,\n","          sentiment=df_test.sentiment.values,\n","          selected_text=df_test.text.values\n","      )\n","\n","  test_data_loader = torch.utils.data.DataLoader(\n","      test_dataset,\n","      shuffle=False,\n","      batch_size=config.VALID_BATCH_SIZE,\n","      num_workers=1\n","  )\n","\n","  final_output_start = []\n","  final_output_end = []\n","  final_tweets = []\n","\n","  with torch.no_grad():\n","      tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n","      for bi, d in enumerate(tk0):\n","          ids = d[\"ids\"]\n","          token_type_ids = d[\"token_type_ids\"]\n","          mask = d[\"mask\"]\n","          sentiment = d[\"sentiment\"]\n","          orig_selected = d[\"orig_selected\"]\n","          orig_tweet = d[\"orig_tweet\"]\n","          targets_start = d[\"targets_start\"]\n","          targets_end = d[\"targets_end\"]\n","          char_map_inverse = d[\"char_map_inverse\"]\n","          offsets = d[\"offsets\"].numpy().tolist()\n","          raw_tweet = d[\"raw_tweet\"]\n","\n","#           # convert char_maps from strings back to lists\n","#           char_map = torch.tensor([eval(x) for x in char_map]).to(device, dtype=torch.long)\n","        \n","          ids = ids.to(device, dtype=torch.long)\n","          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","          mask = mask.to(device, dtype=torch.long)\n","          targets_start = targets_start.to(device, dtype=torch.long)\n","          targets_end = targets_end.to(device, dtype=torch.long)\n","          \n","          summed_start_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN).to(device) # config.MAX_LEN\n","          summed_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN).to(device)  # config.MAX_LEN*config.MAX_LEN\n","\n","          for model in models: \n","            # run it again to get the probabilities\n","            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n","            outputs = model(\n","                input_ids=ids,\n","                attention_mask=mask,\n","                token_type_ids=token_type_ids\n","            )\n","\n","            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability (for each sample)\n","            start_top_probs = outputs[0]  \n","            \n","            # start_top_probs contain those positions' probabilities (for each sample)\n","            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1] \n","            start_top_index = outputs[1] \n","            \n","            # sort start_top_probs so that element (i,j) represents the probability for tweet i of character j being the start position\n","            _, indices = torch.sort(start_top_index, dim=1)\n","            start_top_probs_sorted = start_top_probs[torch.repeat_interleave(torch.arange(start_top_probs.shape[0]), start_top_probs.shape[1]).view(start_top_probs.shape),\n","                      indices]\n","\n","            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n","            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n","            # this can be verified by checking summation to unity\n","            end_top_probs = outputs[2] \n","            end_top_index = outputs[3] \n","\n","            # sort end_top_probs by position of element (rather than by its probability)\n","            # resulting dimensions: n_sample, end_n_top, start_n_top\n","            _, indices = torch.sort(end_top_index, dim=1)\n","            end_top_probs_sorted = end_top_probs[torch.repeat_interleave(torch.arange(end_top_probs.shape[0]), end_top_probs.shape[1]).view(end_top_probs.shape),\n","                      indices]\n","            \n","            # average the end position probabilities across start positions\n","            end_top_probs_sorted = end_top_probs_sorted.view([end_top_probs_sorted.shape[0], model.end_n_top, model.start_n_top]).mean(dim=2)\n","   \n","            summed_start_probs_sorted += start_top_probs_sorted\n","            summed_end_probs_sorted += end_top_probs_sorted\n","\n","          avg_start_probs_sorted = (summed_start_probs_sorted/torch.tensor(len(models))).cpu().detach().numpy()\n","          avg_end_probs_sorted = (summed_end_probs_sorted/torch.tensor(len(models))).cpu().detach().numpy()\n","                  \n","          # convert starting and ending token probabilities to starting and ending character probabilities\n","          for i, t in enumerate(raw_tweet):\n","            start_char_probs = [0]*len(t)\n","            end_char_probs = [0]*len(t)\n","            inverse_map = [int(x) for x in char_map_inverse[i].split('_')]\n","            for j,o in enumerate(offsets[i]):\n","                if o==[0,0]: continue\n","                try:\n","                    start_char_probs[inverse_map[o[0]]] = avg_start_probs_sorted[i][j]\n","                    end_char_probs[inverse_map[o[1]-1]] = avg_end_probs_sorted[i][j]\n","                except:\n","                    print('offsets: '+str(o))\n","                    print('len(tweet):'+str(len(t)))\n","                    print('len(start_char_probs): '+str(len(start_char_probs)))\n","                    print('tweet: '+str(t))\n","                    print('len(inverse_map): '+str(len(inverse_map)))\n","                    print('segment: '+str(orig_tweet[o[0]:o[1]]))\n","                    print(inverse_map[o[1]-1])\n","                    print(avg_end_probs_sorted[i][j])\n","                    raise()\n","            \n","            final_output_start.append(start_char_probs)\n","            final_output_end.append(end_char_probs)\n","          final_tweets.extend(raw_tweet)\n","                 \n","  df_test.loc[:, 'start_position_probs'] = final_output_start\n","  df_test.loc[:, 'end_position_probs'] = final_output_end\n","  df_test.loc[:, 'orig_tweet'] = final_tweets\n","  df_test.to_csv(\"start_end_predictions.csv\", index=False)\n","\n","  return df_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"A2FdTWwID9vT","colab_type":"code","colab":{}},"source":["def predict_test_for_voting():\n","    \n","    df_test = pd.read_csv(config.TESTING_FILE)\n","    df_test.loc[:, \"selected_text\"] = df_test.text.values\n","\n","    test_dataset = TweetDataset(\n","          tweet=df_test.text.values,\n","          sentiment=df_test.sentiment.values,\n","          selected_text=df_test.selected_text.values\n","      )\n","\n","    test_data_loader = torch.utils.data.DataLoader(\n","      test_dataset,\n","      shuffle=False,\n","      batch_size=config.VALID_BATCH_SIZE,\n","      num_workers=1\n","    )\n","    \n","    preds_df = df_test.loc[:, ['textID']]\n","    \n","    for mf in os.listdir(config.MODEL_PATH):\n","        if not mf.endswith('.bin'):\n","            continue\n","            \n","        model = init_model(config)\n","        model.load_state_dict(torch.load(config.MODEL_PATH / mf))\n","        print(config.MODEL_PATH / mf)\n","\n","        model.eval()\n","        # ensure we get output probabilities for all combinations of start and end position\n","        model.start_n_top = config.MAX_LEN\n","        model.end_n_top = config.MAX_LEN\n","        model.to(device)\n","        \n","        final_output = []\n","        \n","        with torch.no_grad():\n","\n","            tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n","\n","            for bi, d in enumerate(tk0):\n","                ids = d[\"ids\"]\n","                token_type_ids = d[\"token_type_ids\"]\n","                mask = d[\"mask\"]\n","                sentiment = d[\"sentiment\"]\n","                orig_selected = d[\"orig_selected\"]\n","                orig_tweet = d[\"orig_tweet\"]\n","                targets_start = d[\"targets_start\"]\n","                targets_end = d[\"targets_end\"]\n","                offsets = d[\"offsets\"].numpy()\n","                raw_tweet = d[\"raw_tweet\"]\n","                raw_selected = d[\"raw_selected_text\"]\n","                char_map_inverse = d[\"char_map_inverse\"]\n","        \n","                ids = ids.to(device, dtype=torch.long)\n","                token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","                mask = mask.to(device, dtype=torch.long)\n","                targets_start = targets_start.to(device, dtype=torch.long)\n","                targets_end = targets_end.to(device, dtype=torch.long)\n","\n","                summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n","\n","                # run it again to get the probabilities\n","                # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n","                outputs = model(\n","                    input_ids=ids,\n","                    attention_mask=mask,\n","                    token_type_ids=token_type_ids\n","                )\n","\n","                # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n","                sorted_start_probs = outputs[0]  \n","\n","                # start_top_probs contain those positions' probabilities\n","                # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n","                sorted_start_index = outputs[1] \n","\n","                # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n","                # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n","                # this can be verified by checking summation to unity\n","                sorted_end_probs = outputs[2] \n","                sorted_end_index = outputs[3] \n","\n","                # calculate joint probability of start, end position tuples\n","                sorted_joint_probs = (sorted_start_probs.repeat(1, model.end_n_top)*sorted_end_probs)\n","                top_joint_index = sorted_joint_probs.argmax(dim=1)\n","                \n","                # convert flat position to separate start and end positions\n","                top_end_index = sorted_end_index[torch.arange(sorted_end_index.shape[0]), top_joint_index]\n","                top_start_index = sorted_start_index[torch.arange(sorted_start_index.shape[0]), top_joint_index % torch.tensor(config.MAX_LEN).to(device)]\n","                \n","                for px, tweet in enumerate(raw_tweet):  \n","                    _, output_sentence = calculate_jaccard_score(\n","                        raw_tweet=tweet,\n","                        target_string=raw_selected[px],\n","                        sentiment_val=sentiment[px],\n","                        idx_start=top_start_index[px],\n","                        idx_end=top_end_index[px],\n","                        offsets=offsets[px],\n","                        char_map_inverse=char_map_inverse[px],\n","                        cleaned_tweet=orig_tweet[px]\n","                    )\n","        \n","                    final_output.append(output_sentence)\n","\n","        preds_df.loc[:, mf] = final_output\n","\n","    # reshape output\n","    preds_df = preds_df.melt(id_vars = 'textID', var_name='model', value_name='selected_text')\n","    \n","    preds_df.to_csv('predictions_voting.csv', index=False)\n","    \n","    return preds_df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcRLGTjxV5Xi","trusted":true,"colab_type":"code","colab":{}},"source":["def predict_train(n_sample=None):\n","  df_train = pd.read_csv(config.TRAINING_FILE)\n","  \n","  if n_sample:\n","    df_train = df_train.sample(n_sample)\n","  \n","  final_output = []\n","\n","  for mf in os.listdir(config.MODEL_PATH):\n","\n","    model = init_model(config)\n","    \n","    model.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n","    print(config.MODEL_PATH / mf)\n","    model.eval()\n","    # ensure we get output probabilities for all combinations of start and end position\n","    model.start_n_top = config.MAX_LEN\n","    model.end_n_top = config.MAX_LEN\n","    model.to(device)\n","\n","    fold = int(re.findall('model_(\\d).bin', mf)[0])\n","    \n","    if df_train.pipe(lambda x:x[x.kfold==fold]).shape[0]==0:\n","      continue\n","    \n","    train_dataset = TweetDataset(\n","            tweet=df_train.pipe(lambda x:x[x.kfold==fold]).text.values,\n","            sentiment=df_train.pipe(lambda x:x[x.kfold==fold]).sentiment.values,\n","            selected_text=df_train.pipe(lambda x:x[x.kfold==fold]).selected_text.values\n","        )\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        shuffle=False,\n","        batch_size=16, #config.VALID_BATCH_SIZE,\n","        num_workers=1\n","    )\n","\n","    tk0 = tqdm(train_data_loader, total=len(train_data_loader))\n","    for bi, d in enumerate(tk0):\n","        ids = d[\"ids\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        mask = d[\"mask\"]\n","        sentiment = d[\"sentiment\"]\n","        orig_selected = d[\"orig_selected\"]\n","        orig_tweet = d[\"orig_tweet\"]\n","        targets_start = d[\"targets_start\"]\n","        targets_end = d[\"targets_end\"]\n","        offsets = d[\"offsets\"]\n","        raw_tweet = d[\"raw_tweet\"]\n","        raw_selected = d[\"raw_selected_text\"]\n","        char_map_inverse = d[\"char_map_inverse\"]\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        targets_start = targets_start.to(device, dtype=torch.long)\n","        targets_end = targets_end.to(device, dtype=torch.long)\n","\n","        outputs = model(\n","            input_ids=ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids,\n","            start_positions=targets_start,\n","            end_positions=targets_end\n","        )\n","        loss = outputs[0]\n","        \n","        # run it again to get the probabilities\n","        # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n","        outputs = model(\n","            input_ids=ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids\n","        )\n","        # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n","        start_top_probs = outputs[0]  \n","      \n","        # start_top_probs contain those positions' probabilities\n","        # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n","        start_top_index = outputs[1] \n","\n","        # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n","        # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n","        # this can be verified by checking summation to unity\n","        end_top_probs = outputs[2] \n","        end_top_index = outputs[3] \n","        \n","        # calculate joint probability of start, end position tuples\n","        start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n","\n","        # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n","        mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n","        _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n","\n","        start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n","                  indices]\n","\n","        # get (flat) position in sequence of highest probability tuple\n","        top_start_end_probs_sorted = start_end_probs_sorted.argmax(dim=1)\n","\n","        # convert flat position to separate start and end positions\n","        start_top_positions = (top_start_end_probs_sorted % torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n","        end_top_positions = (top_start_end_probs_sorted // torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n","        \n","        jaccard_scores = []\n","        for px, tweet in enumerate(raw_tweet):\n","            tweet_raw_selected_text = raw_selected[px]\n","            tweet_sentiment = sentiment[px]\n","            tweet_offsets = offsets[px]\n","            tweet_char_map_inverse = char_map_inverse[px]\n","\n","            start_top_position = start_top_positions[px]\n","            end_top_position = end_top_positions[px]\n","            \n","            cleaned_tweet = orig_tweet[px]\n","            \n","            _, output_sentence = calculate_jaccard_score(\n","                raw_tweet=tweet,\n","                target_string=tweet_raw_selected_text,\n","                sentiment_val=tweet_sentiment,\n","                idx_start=start_top_position,\n","                idx_end=end_top_position,\n","                offsets=tweet_offsets,\n","                char_map_inverse=tweet_char_map_inverse,\n","                cleaned_tweet=cleaned_tweet\n","            )\n","            final_output.append({'text':tweet, 'prediction':output_sentence})\n","    \n","    del model, train_dataset, train_data_loader\n","    gc.collect()\n","\n","  df_train = df_train.merge(pd.DataFrame(final_output), on='text', how='left')\n","\n","  return df_train\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sy0b2gnexnQB","outputId":"07a6cd3e-4003-4671-99c7-b91cdb16b051","trusted":true,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1592226756104,"user_tz":-120,"elapsed":122963,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["IN_KAGGLE_COMMIT = False\n","if (not IN_COLAB) and ('runtime' not in get_ipython().config.IPKernelApp.connection_file):\n","   IN_KAGGLE_COMMIT = True\n","\n","\n","print(IN_KAGGLE_COMMIT)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ub22h-3qxOIJ","colab_type":"text"},"source":["# Run"]},{"cell_type":"code","metadata":{"id":"WSqxmPyXYbhZ","outputId":"f6736ccf-98e7-48a7-9086-68d0155452ca","trusted":false,"colab_type":"code","executionInfo":{"status":"ok","timestamp":1592234308305,"user_tz":-120,"elapsed":7548873,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":578}},"source":[" %%time\n"," \n","if IN_COLAB:\n","    run_training()\n","\n","if IN_KAGGLE_COMMIT:\n","    #predict_test()\n","    #gen_probs_test()\n","    predict_test_for_voting()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/1374 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Training is Starting for fold=0\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1374/1374 [25:13<00:00,  1.10s/it, loss=1.09]\n","100%|██████████| 344/344 [06:25<00:00,  1.12s/it, jaccard=0.699, loss=0.832]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.6991815998470465\n","Loss = 0.832125565934563\n","Jaccard Score = 0.6991815998470465\n","Loss score = 0.832125565934563\n","Validation loss decreased (inf --> 0.832126).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1374/1374 [24:48<00:00,  1.08s/it, loss=0.771]\n","100%|██████████| 344/344 [06:23<00:00,  1.12s/it, jaccard=0.7, loss=0.806]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7000042308098022\n","Loss = 0.8063622435020741\n","Jaccard Score = 0.7000042308098022\n","Loss score = 0.8063622435020741\n","Validation loss decreased (0.832126 --> 0.806362).  Saving model ...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1374/1374 [24:48<00:00,  1.08s/it, loss=0.654]\n","100%|██████████| 344/344 [06:24<00:00,  1.12s/it, jaccard=0.703, loss=0.877]\n","  0%|          | 0/1374 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7033174365021408\n","Loss = 0.8773689320340774\n","Jaccard Score = 0.7033174365021408\n","Loss score = 0.8773689320340774\n","EarlyStopping counter: 1 out of 2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1374/1374 [24:47<00:00,  1.08s/it, loss=0.552]\n","100%|██████████| 344/344 [06:22<00:00,  1.11s/it, jaccard=0.702, loss=0.96]"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7022890720447954\n","Loss = 0.9597559294716239\n","Jaccard Score = 0.7022890720447954\n","Loss score = 0.9597559294716239\n","EarlyStopping counter: 2 out of 2\n","Early stopping\n","Mean val loss: 0.8063622435020741\n","CPU times: user 16min 59s, sys: 3min 52s, total: 20min 51s\n","Wall time: 2h 5min 48s\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"SYN-zRF5v6FY","colab_type":"code","outputId":"66fa562b-1608-4045-ce64-e83c18a74b52","executionInfo":{"status":"ok","timestamp":1592207299689,"user_tz":-120,"elapsed":3228224,"user":{"displayName":"Simon Dunne","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhB7c9rNrGf3_trJZgqV8dKBNzXjR_Dfu4JWPdd=s64","userId":"01632599044590218026"}},"colab":{"base_uri":"https://localhost:8080/","height":374}},"source":["get_cv_loss()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Evaluating fold=0\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 344/344 [10:26<00:00,  1.82s/it, jaccard=0.696, loss=0.817]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.6963852003172368\n","Loss = 0.8167823214284544\n","Evaluating fold=1\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 344/344 [10:29<00:00,  1.83s/it, jaccard=0.704, loss=0.823]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7036242974222892\n","Loss = 0.8227394022795831\n","Evaluating fold=2\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 344/344 [10:29<00:00,  1.83s/it, jaccard=0.703, loss=0.814]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7031660023647034\n","Loss = 0.8143632163387199\n","Evaluating fold=3\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 344/344 [10:30<00:00,  1.83s/it, jaccard=0.704, loss=0.811]\n"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.7039190932883281\n","Loss = 0.8110352138591472\n","Evaluating fold=4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 344/344 [10:29<00:00,  1.83s/it, jaccard=0.698, loss=0.839]"],"name":"stderr"},{"output_type":"stream","text":["Jaccard = 0.6977646500076556\n","Loss = 0.8390276559575677\n","Mean val loss: 0.7009718486800426\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FJg0fq5jlMZw","trusted":false,"colab_type":"code","colab":{}},"source":["# test_df = pd.read_csv(config.TESTING_FILE).set_index(\"textID\")\n","\n","# sub_df = pd.read_csv(config.SUBMISSION_FILE).set_index(\"textID\")\n","\n","# # Everything not presented in the public set \n","# # will take a value of the original text\n","# test_df[\"selected_text\"] = test_df.text\n","\n","# # Get the public ids and assign them\n","# public_idxs = sub_df.index.values\n","# test_df.loc[public_idxs, \"selected_text\"] = sub_df.selected_text.values\n","# test_df[[\"selected_text\"]].to_csv(\"submission.csv\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ufcbBmS1QyJ_","trusted":false,"colab_type":"code","colab":{}},"source":["# train_df = pd.read_csv(config.TRAINING_FILE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jA5rt_0GNan","trusted":false,"colab_type":"code","colab":{}},"source":["# output = predict_train()#1000)\n","# output.to_csv(config.MODEL_PATH / 'train_predictions.csv', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7M9-QbC5F87F","trusted":false,"colab_type":"code","colab":{}},"source":["# output = pd.read_csv(config.MODEL_PATH / 'train_predictions.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fdXN4b5PBgaV","colab_type":"text"},"source":["# Scratch"]},{"cell_type":"code","metadata":{"id":"NhIOFKnzQr-M","trusted":false,"colab_type":"code","colab":{}},"source":["# dfx = pd.read_csv(config.TRAINING_FILE)\n","\n","# train_dataset = TweetDataset(\n","#     tweet=dfx.text.values,\n","#     sentiment=dfx.sentiment.values,\n","#     selected_text=dfx.selected_text.values\n","# )\n","\n","# train_data_loader = torch.utils.data.DataLoader(\n","#     train_dataset,\n","#     batch_size=config.TRAIN_BATCH_SIZE,\n","#     num_workers=4\n","# )\n","\n","# words = []\n","\n","# for t in train_dataset:\n","#   t_words = t['orig_tweet'].split()\n","#   words += t_words\n","\n","# from collections import Counter\n","# words_counter = Counter(words)\n","# words_counter_df = pd.DataFrame.from_dict(words_counter, orient='index', columns=['count']).reset_index()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DP80xqhJRjLS","trusted":false,"colab_type":"code","colab":{}},"source":["# train_dataset[241]['orig_tweet']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzPAOyavYp66","trusted":false,"colab_type":"code","colab":{}},"source":["# fold = 0\n","\n","# dfx = pd.read_csv(config.TRAINING_FILE)\n","\n","# df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n","# df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n","\n","# train_dataset = TweetDataset(\n","#     tweet=df_train.text.values,\n","#     sentiment=df_train.sentiment.values,\n","#     selected_text=df_train.selected_text.values\n","# )\n","\n","# train_data_loader = torch.utils.data.DataLoader(\n","#     train_dataset,\n","#     batch_size=config.TRAIN_BATCH_SIZE,\n","#     num_workers=4\n","# )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSopugv2e2ey","trusted":false,"colab_type":"code","colab":{}},"source":["# train_data_loader.dataset.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VV0Wweg0fDQF","trusted":false,"colab_type":"code","colab":{}},"source":["# len(df_train.iloc[1,:].text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxYgcR0qVNQu","trusted":false,"colab_type":"code","colab":{}},"source":["# tweet = \" We've just 16ï¿½C today&amp;cold wind..  Want it 2b like 25ï¿½ to 30ï¿½! I love hot weather! But I reaped the 1st strawberry yday!\"\n","# clean_tweet = \" We've just 16ï¿½C today&cold wind..  Want it To be like 25ï¿½ to 30ï¿½! I love hot weather! But I reaped the 1st strawberry yday!\"\n","# selected_text = \"We've just 16ï¿½C today&amp;cold wind..  Want it 2b like 25ï¿½ to 30ï¿½! I love hot weather! But I reaped the 1st strawberry yd\"\n","# #char_map = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]\n","# len(clean_tweet)\n","# #char_map_inverse = pd.Series([max([j for j,k in enumerate(char_map) if k==i], default=None) for i in range(len(clean_tweet))]).fillna(method='backfill').fillna(len(tweet)-1).astype(int).values.tolist()\n","# #print([tweet[e] for e in char_map_inverse])\n","# #print(len(tweet))\n","# print(len(tweet))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"phz0AqNEphWf","trusted":false,"colab_type":"code","colab":{}},"source":["# char_targets = list(range(len(tweet)))\n","# char_targets = [0, 1, 2]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7UgYYG99piES","trusted":false,"colab_type":"code","colab":{}},"source":["# char_targets[200:210]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlYWaCx0Zn-v","trusted":false,"colab_type":"code","colab":{}},"source":["# char_map_inverse = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 51, 51, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]\n","# len(char_map_inverse)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aEGLJGHFaliQ","trusted":false,"colab_type":"code","colab":{}},"source":["# print(df_valid.pipe(lambda x:x[x.text==\" We've just 16ï¿½C today&amp;cold wind..  Want it 2b like 25ï¿½ to 30ï¿½! I love hot weather! But I reaped the 1st strawberry yday!\"]).iloc[0].selected_text[-1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkgJmnXJt0e6","trusted":false,"colab_type":"code","colab":{}},"source":["# tweet, selected_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFU0go8KadR9","trusted":false,"colab_type":"code","colab":{}},"source":["# #selected_text = \n","# tweet = data['raw_tweet']\n","# selected_text = data['raw_selected_text']\n","# sentiment = 'neutral'\n","# tokenizer = config.TOKENIZER\n","# max_len = config.MAX_LEN\n","# slang_dict = config.SLANG_DICT\n","# data = process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)\n","# print(data)"],"execution_count":0,"outputs":[]}]}