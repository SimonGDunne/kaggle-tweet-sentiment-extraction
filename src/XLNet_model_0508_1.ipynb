{"cells":[{"metadata":{"id":"7jaLISF2magZ"},"cell_type":"markdown","source":"Code taken/adapted from:\n* https://www.youtube.com/watch?v=U51ranzJBpY [ TOKENISER ]\n* https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch [ TRAINING ]\n* https://www.kaggle.com/abhishek/roberta-inference-5-folds [ INFERENCE ] \n* https://www.kaggle.com/masterscrat/detect-if-notebook-is-running-interactively [ CHECK WHERE NOTEBOOK IS RUNNING ]\n* https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/141502 [ SUBMISSION ]"},{"metadata":{"id":"VjKIw2dP1wuj","outputId":"1ed2b720-cdd7-44c9-b796-ada19eb30a7f","trusted":true},"cell_type":"code","source":"!pip install transformers\n!pip install tokenizers\n!pip install protobuf","execution_count":null,"outputs":[]},{"metadata":{"id":"1RjWp8i3_R4y","outputId":"ce45e6a9-e0fe-4945-f230-a65e22bb2ae5","trusted":true},"cell_type":"code","source":"try:\n    from google.colab import drive\n    IN_COLAB = True\n    drive.mount('/content/drive')\n    !wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n    !wget https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model\nexcept:\n    IN_COLAB = False\n    \n    import sys\n    sys.path.append('/kaggle/input/sentencepiece-pb2/')","execution_count":null,"outputs":[]},{"metadata":{"id":"NBiazpTm1wvB"},"cell_type":"markdown","source":"## Import library"},{"metadata":{"id":"vEBAM8Yn1wvC","trusted":true},"cell_type":"code","source":"import os\n\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport os\nimport tokenizers\nimport string\nimport torch\nimport transformers\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport re\nimport sentencepiece as spm\nimport sentencepiece_pb2","execution_count":null,"outputs":[]},{"metadata":{"id":"76RJVzb_1wvF","outputId":"70e0af84-b4b6-46ae-c238-7e0acd31b026","trusted":true},"cell_type":"code","source":"# If there's a GPU available...\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":null,"outputs":[]},{"metadata":{"id":"P0NlI3nJ6Kw4","trusted":true},"cell_type":"code","source":"class SentencePieceTokenizer:\n    def __init__(self, model_name):\n        self.sp = spm.SentencePieceProcessor()\n        self.sp.load(model_name)\n    \n    def encode(self, sentence):\n        spt = sentencepiece_pb2.SentencePieceText()\n        spt.ParseFromString(self.sp.encode_as_serialized_proto(sentence))\n        offsets = []\n        ids = []\n        for piece in spt.pieces:\n            ids.append(piece.id)\n            offsets.append((piece.begin, piece.end))\n        return {'ids' : ids,\n                'offsets' : offsets}","execution_count":null,"outputs":[]},{"metadata":{"id":"UiUv9iHz1wvu","trusted":true},"cell_type":"code","source":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 32 #64\n    VALID_BATCH_SIZE =  16\n    EPOCHS = 10\n    \n    MODEL_CONFIG = transformers.XLNetConfig\n    MODEL = transformers.XLNetForQuestionAnswering\n    if IN_COLAB:\n        \n        BASE_PATH = Path.cwd() / \"drive\" / \"My Drive\" / \"kaggle\" / \"tweet_sentiment_extraction\"\n        PRETRAINED_MODEL_DIR = BASE_PATH / \"input\" / \"xlnetbasecased\"\n        TOKENIZER = SentencePieceTokenizer(str(PRETRAINED_MODEL_DIR / 'xlnet-base-cased-spiece.model'))\n        MODEL_PATH = BASE_PATH  / \"model_save\" / \"model_0508_1\"\n        TRAINING_FILE = BASE_PATH / \"input\" / \"train-5fold\" / \"train_folds.csv\"\n        TESTING_FILE = BASE_PATH  / \"input\" / \"test.csv\"\n        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"sample_submission.csv\"\n        SUBMISSION_FILE = BASE_PATH / \"input\" / \"submission.csv\"\n    else:\n        BASE_PATH = Path('/kaggle')\n        PRETRAINED_MODEL_DIR = BASE_PATH / \"input\" / \"xlnetbasecased\"\n        TOKENIZER = SentencePieceTokenizer( str(PRETRAINED_MODEL_DIR / \"xlnet-base-cased-spiece.model\"))\n        MODEL_PATH = BASE_PATH  / \"input\" / \"xlnetmodel05081\"\n        TRAINING_FILE = BASE_PATH / \"input\" / \"trainfolds\" / \"train_folds.csv\"\n        TESTING_FILE = BASE_PATH  / \"input\" / \"tweet-sentiment-extraction\" / \"test.csv\"\n        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"tweet-sentiment-extraction\" / \"sample_submission.csv\"\n        SUBMISSION_FILE = BASE_PATH / \"working\" / \"submission.csv\"","execution_count":null,"outputs":[]},{"metadata":{"id":"AciDwLjC8G77","outputId":"2cfd3267-2b58-4717-b845-3674ece3d19e","trusted":true},"cell_type":"code","source":"[config.TOKENIZER.sp.id_to_piece(x) for x in range(0,10)]","execution_count":null,"outputs":[]},{"metadata":{"id":"UMoIyYf39Zct","outputId":"d415f3c4-5894-4d76-c69a-53e29eb74d13","trusted":true},"cell_type":"code","source":"[config.TOKENIZER.sp.piece_to_id(x) for x in ['positive', 'negative', 'neutral']]","execution_count":null,"outputs":[]},{"metadata":{"id":"2hXHgCXG1wv2"},"cell_type":"markdown","source":"## Utils"},{"metadata":{"id":"wLmNjNd41wv3","trusted":true},"cell_type":"code","source":"class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) / (len(a) + len(b) - len(c))\n\n\nclass EarlyStopping:\n    # https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model, name):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, name)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, name)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, name):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), name)\n        self.val_loss_min = val_loss","execution_count":null,"outputs":[]},{"metadata":{"id":"EZ8fCbd41wv6"},"cell_type":"markdown","source":"## Data processing"},{"metadata":{"id":"MUCcTAMr1wv7","trusted":true},"cell_type":"code","source":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    \"\"\"\n    Preprocessing the data to the XLNet model formatting\n    \"\"\"\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n    # find start and indices of selected_text in tweet\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    # create character mask for selected_text in tweet\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n\n    tok_tweet = tokenizer.encode(tweet)\n    \n    input_ids_orig = tok_tweet['ids']\n    tweet_offsets = tok_tweet['offsets']\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    #######\n    sentiment_id = {\n        'positive': 19036,\n        'negative': 25976,\n        'neutral': 24734\n    }\n    #######\n    \n    # https://huggingface.co/transformers/model_doc/xlnet.html#transformers.XLNetTokenizer.build_inputs_with_special_tokens\n    input_ids = [sentiment_id[sentiment]] + [4] + input_ids_orig + [4] + [3]\n    #input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0]*2 + [1] * (len(input_ids_orig)+1) + [2]\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 2 + tweet_offsets + [(0, 0)] * 2\n    targets_start += 2\n    targets_end += 2\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([5] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","execution_count":null,"outputs":[]},{"metadata":{"id":"y-eRe4x11wv-"},"cell_type":"markdown","source":"## Data loader"},{"metadata":{"id":"4s9Id3hW1wv-","trusted":true},"cell_type":"code","source":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = config.TOKENIZER\n        self.max_len = config.MAX_LEN\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len\n        )\n\n        return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","execution_count":null,"outputs":[]},{"metadata":{"id":"GXJ2VElJ1wwI"},"cell_type":"markdown","source":"## Loss function"},{"metadata":{"id":"hFNXnE3u1wwJ","trusted":true},"cell_type":"code","source":"# def loss_fn(start_logits, end_logits, start_positions, end_positions):\n#     loss_fct = nn.CrossEntropyLoss()\n#     start_loss = loss_fct(start_logits, start_positions)\n#     end_loss = loss_fct(end_logits, end_positions)\n#     total_loss = (start_loss + end_loss)\n#     return total_loss\n\ndef loss_fn(start_logprobs, end_logprobs, start_positions, end_positions):\n    loss_fct = nn.NLLLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss\n    ","execution_count":null,"outputs":[]},{"metadata":{"id":"xCxxX5Jw1wwM"},"cell_type":"markdown","source":"## Training function"},{"metadata":{"id":"NL1vcjOa1wwN","trusted":true},"cell_type":"code","source":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        \n        # outputs_start, outputs_end = model(\n        #     ids=ids,\n        #     mask=mask,\n        #     token_type_ids=token_type_ids,\n        # )\n        # loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        # loss.backward()\n\n        outputs = model(\n            input_ids=ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids,\n            start_positions=targets_start, \n            end_positions=targets_end\n        )\n        \n        loss = outputs[0]\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()\n\n        # outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        # outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        \n        # jaccard_scores = []\n        # for px, tweet in enumerate(orig_tweet):\n        #     selected_tweet = orig_selected[px]\n        #     tweet_sentiment = sentiment[px]\n        #     jaccard_score, _ = calculate_jaccard_score(\n        #         original_tweet=tweet,\n        #         target_string=selected_tweet,\n        #         sentiment_val=tweet_sentiment,\n        #         idx_start=np.argmax(outputs_start[px, :]),\n        #         idx_end=np.argmax(outputs_end[px, :]),\n        #         offsets=offsets[px]\n        #     )\n        #     jaccard_scores.append(jaccard_score)\n\n        # jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg)#, jaccard=jaccards.avg)","execution_count":null,"outputs":[]},{"metadata":{"id":"Jalt8e9B1wwQ"},"cell_type":"markdown","source":"## Evaluation function"},{"metadata":{"id":"chIyiuLB1wwR","trusted":true},"cell_type":"code","source":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    \n    filtered_output  = \"\"\n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n\n        # add spacing to output\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    if sentiment_val != \"neutral\" and verbose == True:\n        if filtered_output.strip().lower() != target_string.strip().lower():\n            print(\"********************************\")\n            print(f\"Output= {filtered_output.strip()}\")\n            print(f\"Target= {target_string.strip()}\")\n            print(f\"Tweet= {original_tweet.strip()}\")\n            print(\"********************************\")\n\n    jac = jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    losses = AverageMeter()\n    jaccards = AverageMeter()\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids,\n                start_positions=targets_start,\n                end_positions=targets_end\n            )\n            loss = outputs[0]\n            \n            # run it again to get the probabilities\n            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n            start_top_probs = outputs[0]  \n          \n            # start_top_probs contain those positions' probabilities\n            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n            start_top_index = outputs[1] \n\n            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n            # this can be verified by checking summation to unity\n            end_top_probs = outputs[2] \n            end_top_index = outputs[3] \n            \n            # calculate joint probability of start, end position tuples\n            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n\n            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n\n            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n                      indices]\n\n            # get (flat) position in sequence of highest probability tuple\n            top_start_end_probs_sorted = start_end_probs_sorted.argmax(dim=1)\n\n            # convert flat position to separate start and end positions\n            start_top_positions = (top_start_end_probs_sorted % torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n            end_top_positions = (top_start_end_probs_sorted // torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n            \n            jaccard_scores = []\n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                start_top_position = start_top_positions[px]\n                end_top_position = end_top_positions[px]\n            \n                jaccard_score, _ = calculate_jaccard_score(\n                    original_tweet=tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=start_top_position,\n                    idx_end=end_top_position,\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n    \n    print(f\"Jaccard = {jaccards.avg}\")\n    print(f\"Loss = {losses.avg}\")\n    return jaccards.avg, losses.avg","execution_count":null,"outputs":[]},{"metadata":{"id":"sWiozMAq1wwW"},"cell_type":"markdown","source":"## Training "},{"metadata":{"id":"el1CDathhxr8","trusted":true},"cell_type":"code","source":"def init_model(config):\n    model_config = config.MODEL_CONFIG.from_pretrained(config.PRETRAINED_MODEL_DIR )#/ \"config.json\")\n    model_config.output_hidden_states = True\n    model_config.start_n_top = config.MAX_LEN\n    model_config.end_n_top = config.MAX_LEN\n    #'/kaggle/input/xlnet-base-tf/xlnet-base-cased'\n    model = config.MODEL.from_pretrained(config.PRETRAINED_MODEL_DIR, config=model_config)#, state_dict='/kaggle/input/xlnetmodel05081/model_3.bin')\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"id":"v0tHJ24G1wwW","trusted":true},"cell_type":"code","source":"def run_fold(fold):\n\n    dfx = pd.read_csv(config.TRAINING_FILE)\n\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n\n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        num_workers=2\n    )\n    \n    device = torch.device(\"cuda\")\n\n    # initialise model\n    model = init_model(config)\n    \n    model.to(device)\n\n    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n    es = EarlyStopping(patience=2, verbose=True)\n    print(f\"Training is Starting for fold={fold}\")\n    \n    for epoch in range(config.EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        jaccard, loss = eval_fn(valid_data_loader, model, device)\n        print(f\"Jaccard Score = {jaccard}\")\n        print(f\"Loss score = {loss}\")\n        es(loss, model, name=config.MODEL_PATH / f\"model_{fold}.bin\")\n        \n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n  \n    return es.val_loss_min","execution_count":null,"outputs":[]},{"metadata":{"id":"2uAP683Z1wwZ"},"cell_type":"markdown","source":"## Run training"},{"metadata":{"id":"mUwMNkb1Hq3w","trusted":true},"cell_type":"code","source":"def run_training():\n  val_loss = []\n  for ifold in range(5):\n      q = run_fold(ifold)\n      val_loss.append(q)\n  print(f'Mean val loss: {np.mean(val_loss)}')","execution_count":null,"outputs":[]},{"metadata":{"id":"Iyui81xMHLqQ"},"cell_type":"markdown","source":"## Predict test set"},{"metadata":{"id":"pW0jJHfUkzTs","trusted":true},"cell_type":"code","source":"def predict_test():\n  df_test = pd.read_csv(config.TESTING_FILE)\n  df_test.loc[:, \"selected_text\"] = df_test.text.values\n\n  models = []\n\n  for mf in os.listdir(config.MODEL_PATH):\n    m = init_model(config)\n    \n    m.load_state_dict(torch.load(config.MODEL_PATH / mf))\n    print(config.MODEL_PATH / mf)\n    m.eval()\n    # ensure we get output probabilities for all combinations of start and end position\n    m.start_n_top = config.MAX_LEN\n    m.end_n_top = config.MAX_LEN\n    m.to(device)\n\n    models.append(m)\n\n  test_dataset = TweetDataset(\n          tweet=df_test.text.values,\n          sentiment=df_test.sentiment.values,\n          selected_text=df_test.selected_text.values\n      )\n\n  test_data_loader = torch.utils.data.DataLoader(\n      test_dataset,\n      shuffle=False,\n      batch_size=config.VALID_BATCH_SIZE,\n      num_workers=1\n  )\n\n  final_output = []\n\n  with torch.no_grad():\n      tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n      for bi, d in enumerate(tk0):\n          ids = d[\"ids\"]\n          token_type_ids = d[\"token_type_ids\"]\n          mask = d[\"mask\"]\n          sentiment = d[\"sentiment\"]\n          orig_selected = d[\"orig_selected\"]\n          orig_tweet = d[\"orig_tweet\"]\n          targets_start = d[\"targets_start\"]\n          targets_end = d[\"targets_end\"]\n          offsets = d[\"offsets\"].numpy()\n\n          ids = ids.to(device, dtype=torch.long)\n          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n          mask = mask.to(device, dtype=torch.long)\n          targets_start = targets_start.to(device, dtype=torch.long)\n          targets_end = targets_end.to(device, dtype=torch.long)\n          \n          summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n\n          for model in models: \n            # run it again to get the probabilities\n            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n            start_top_probs = outputs[0]  \n          \n            # start_top_probs contain those positions' probabilities\n            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n            start_top_index = outputs[1] \n\n            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n            # this can be verified by checking summation to unity\n            end_top_probs = outputs[2] \n            end_top_index = outputs[3] \n            \n            # calculate joint probability of start, end position tuples\n            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n\n            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n\n            #start_end_probs_sorted = start_end_probs[torch.arange(start_end_probs.shape[0]), indices]\n            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n                      indices]\n\n            summed_start_end_probs_sorted += start_end_probs_sorted\n\n          avg_start_end_probs_sorted = summed_start_end_probs_sorted/torch.tensor(len(models))\n\n          # get (flat) position in sequence of highest probability tuple\n          top_avg_start_end_probs_sorted = avg_start_end_probs_sorted.argmax(dim=1)\n\n          # convert flat position to separate start and end positions\n          start_top_positions = (top_avg_start_end_probs_sorted % torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n          end_top_positions = (top_avg_start_end_probs_sorted // torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n          \n          jaccard_scores = []\n          for px, tweet in enumerate(orig_tweet):\n              selected_tweet = orig_selected[px]\n              tweet_sentiment = sentiment[px]\n              _, output_sentence = calculate_jaccard_score(\n                  original_tweet=tweet,\n                  target_string=selected_tweet,\n                  sentiment_val=tweet_sentiment,\n                  idx_start=start_top_positions[px],\n                  idx_end=end_top_positions[px],\n                  offsets=offsets[px],\n                  verbose=True\n              )\n              final_output.append(output_sentence)\n\n\n  sample = pd.read_csv(config.SAMPLE_SUBMISSION_FILE)\n  sample.loc[:, 'selected_text'] = final_output\n  sample.to_csv(\"submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"sy0b2gnexnQB","outputId":"add343a0-3db7-4bb8-f064-82eaccf8ca55","trusted":true},"cell_type":"code","source":"IN_KAGGLE_COMMIT = False\nif (not IN_COLAB) and ('runtime' not in get_ipython().config.IPKernelApp.connection_file):\n   IN_KAGGLE_COMMIT = True\n\n\nprint(IN_KAGGLE_COMMIT)","execution_count":null,"outputs":[]},{"metadata":{"id":"WSqxmPyXYbhZ","trusted":true},"cell_type":"code","source":" %%time\n \nif IN_COLAB:\n    run_training()\n\nif IN_KAGGLE_COMMIT:\n    predict_test()","execution_count":null,"outputs":[]},{"metadata":{"id":"FJg0fq5jlMZw","trusted":false},"cell_type":"code","source":"# test_df = pd.read_csv(config.TESTING_FILE).set_index(\"textID\")\n\n# sub_df = pd.read_csv(config.SUBMISSION_FILE).set_index(\"textID\")\n\n# # Everything not presented in the public set \n# # will take a value of the original text\n# test_df[\"selected_text\"] = test_df.text\n\n# # Get the public ids and assign them\n# public_idxs = sub_df.index.values\n# test_df.loc[public_idxs, \"selected_text\"] = sub_df.selected_text.values\n# test_df[[\"selected_text\"]].to_csv(\"submission.csv\")","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"XLNet_model_0508_1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":4}