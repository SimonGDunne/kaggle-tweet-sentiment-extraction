{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNet for Tweet Sentiment Extraction\n",
    "\n",
    "## Overview\n",
    "### Context\n",
    "This notebook illustrates a model used in my team's entry in Kaggle's 2020 [Tweet Sentiment Extraction](https://www.kaggle.com/c/tweet-sentiment-extraction) competition. The dataset provided to competition entrants contained more than 27,000 tweets that had each been labelled by their *sentiment*. For example, a tweet like \"I just spent the day with a labradoodle called Sporky. He is amazing.\" would likely be labelled with the sentiment positive.\n",
    "\n",
    "The objective of the competition was not to reproduce this sentiment labelling. Instead we were tasked with determining the word or phrase within the tweet that had led it to be labelled as having a positive, negative or neutral sentiment. For example, in our tweet \"I just spent the day with a labradoodle called Sporky. He is amazing.\", the phrase \"He is amazing\" could arguably be the phrase that led it to be labelled positive.\n",
    "\n",
    "This competition provided me with an opportunity to test out new state-of-the-art machine learning models whose performance on some natural language processing (NLP) tasks have exceeded that of humans. These models combine several new ideas in NLP, including:\n",
    "* Learning **word embeddings**, or representations of words that exist in a semantic space in which distance between words indicates the relationship between them (see Word2Vec and GloVe). For example, the relative positions of the word embeddings of \"son\" and \"mother\" in semantic space, would be similar to the relative positions of \"Prince\" and \"Queen\".\n",
    "* When learning these word embedddings, paying **attention** to the context of a word can help a model understand its meaning (see ELMo). For example, in the sentences \"The stick fell from the tree.\" and \"I should stick to my day job\", the surrounding words help us understand that the word \"stick\" has different meanings in each.\n",
    "* Neural network architectures known as Transformers that allow for **parallelisation** thereby speeding training and inference, and attention mechanisms that **long-range dependencies** between words than RNN architectures .\n",
    "* Training the models using **transfer learning** methods (see ULMFit) where the model is first pretrained on a general language task with a massive unlabelled dataset. For example, the model might be trained to predict the missing word in a sentence taken from the corpus of all English Wikipedia entries. Once the model has been trained on this task, it is then fine-tuned on a smaller task-specific dataset. This allows pre-trained models that have been gained a deep understanding of language by being trained on enormous amounts of text on the computational resources to be shared users who can then fine tune them to the their own domain-specific task.\n",
    "\n",
    "For this competition, I focused on a model called *XLNet*. Two variations on this XLNet model were ensembled with a RoBERTa model fine-tuned by my Kaggle teammate to win us a silver medal (top 5%). This notebook preprocesses and trains the model and uses it to predict answers to a test sets.\n",
    "\n",
    "### What is XLNet?\n",
    "Similar to other recently developed NLP models, XLNet is based on a variation on the Transformer architecture and was first pretrained on a large-scale unlabeled text corpus, before being finetuned on downstream tasks.\n",
    "However, XLNet has the advantage that it was pretrained for longer on a larger dataset than previous models. XLNet also differs from its predecessors in the that the language task it uses during pretraining uses permutation sampling rather than masking of the input sequences and takes into account potential dependencies between the words that it is trying to predict. For example, given the task to fill in the blanks in the sentence \"_ _ is the greatest city in the world.\", previous models like BERT might make odd predictions like \"New Francisco\" or \"San York\", while XLNet, because it is not taking into account the semantic dependencies between the missing words, would be more likely to predict \"New York\" or \"San Francisco\".\n",
    "\n",
    "\n",
    "#### What did I do?\n",
    "\n",
    "XLNet comes pre-trained on an enormous corpus with . \n",
    "Task specific training. Supervised learning. Question answering fine-tuning\n",
    "How is this question answering?/\n",
    "* Pytorch for GPU training, Transformers packages provide NLP model \n",
    "* Preprocessing?\n",
    "* Emoji stuff\n",
    "* Question-answering??\n",
    "* Fine-tuned on Google Colab using free GPU resources they provide.\n",
    "Hard-coded output: For tweets labelled Neutral the selected text is the whole tweet.\n",
    "\n",
    "Demonstrate the relationships between word embeddings model.transformer.wte('this')\n",
    "\n",
    "\n",
    "### Sources\n",
    "The following were useful resources for understanding aspects of the model or for provided code snippets and hints.\n",
    "* http://jalammar.github.io/illustrated-bert/ (BERT)\n",
    "* [XLNet paper](https://arxiv.org/abs/1906.08237), [slide deck](https://www.cs.princeton.edu/courses/archive/spring20/cos598C/lectures/lec5-pretraining2.pdf)\n",
    "* [Visualisation of attention mechanism in RNNs for translation](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) \n",
    "* [Illustration of the Transformer architecture](https://jalammar.github.io/illustrated-transformer/) and [an annotated version of the original paper on using attention with transformers](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "* https://www.youtube.com/watch?v=U51ranzJBpY [ TOKENISER ]\n",
    "* https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch [ TRAINING ]\n",
    "* https://www.kaggle.com/abhishek/roberta-inference-5-folds [ INFERENCE ] \n",
    "* https://www.kaggle.com/masterscrat/detect-if-notebook-is-running-interactively [ CHECK WHERE NOTEBOOK IS RUNNING ]\n",
    "* https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/141502 [ SUBMISSION ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "1RjWp8i3_R4y",
    "outputId": "b38b43ff-2b0d-4385-d690-df67af525dd6"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install tokenizers\n",
    "!pip install protobuf\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    drive.mount('/content/drive')\n",
    "    !wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append('/kaggle/input/sentencepiece-pb2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBiazpTm1wvB"
   },
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEBAM8Yn1wvC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import re\n",
    "import html\n",
    "import random\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import model_selection\n",
    "import tokenizers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import sentencepiece as spm\n",
    "import sentencepiece_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "76RJVzb_1wvF",
    "outputId": "ebb61580-3791-4b10-89af-33469d0716ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiUv9iHz1wvu"
   },
   "outputs": [],
   "source": [
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, model_name):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(model_name)\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        spt = sentencepiece_pb2.SentencePieceText()\n",
    "        spt.ParseFromString(self.sp.encode_as_serialized_proto(sentence))\n",
    "        offsets = []\n",
    "        ids = []\n",
    "        for piece in spt.pieces:\n",
    "            ids.append(piece.id)\n",
    "            offsets.append((piece.begin, piece.end))\n",
    "        return {'ids' : ids,\n",
    "                'offsets' : offsets}\n",
    "\n",
    "\n",
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 16 # 32 #64\n",
    "    VALID_BATCH_SIZE =  16\n",
    "    EPOCHS = 10\n",
    "    \n",
    "    MODEL_CONFIG = transformers.XLNetConfig\n",
    "    MODEL = transformers.XLNetForQuestionAnswering\n",
    "    if IN_COLAB:\n",
    "        BASE_PATH = Path.cwd() / \"drive\" / \"My Drive\" / \"kaggle\" / \"tweet_sentiment_extraction\"     \n",
    "        MODEL_PATH = BASE_PATH  / \"model_save\" / \"model_0613_6\"\n",
    "        FOLDED_TRAINING_FILE = BASE_PATH / \"input\" / \"train-5fold\" / \"train_folds.csv\"\n",
    "        TRAINING_FILE = BASE_PATH / \"input\" / \"train.csv\"\n",
    "        TESTING_FILE = BASE_PATH  / \"input\" / \"test.csv\"\n",
    "        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"sample_submission.csv\"\n",
    "        SUBMISSION_FILE = BASE_PATH / \"input\" / \"submission.csv\"\n",
    "        SLANG_FILE = BASE_PATH / \"input\" / \"slang_abbreviations.csv\"\n",
    "        EMOJIS_FILE = BASE_PATH / \"input\" / \"emojis.csv\"\n",
    "        PRIVATE_TEST = BASE_PATH / \"input\" / \"test_private_df.csv\"\n",
    "    else:\n",
    "        BASE_PATH = Path('/kaggle')\n",
    "        MODEL_PATH = BASE_PATH  / \"input\" / \"xlnetmodel06136\"\n",
    "        FOLDED_TRAINING_FILE = BASE_PATH / \"working\" / \"train_folds.csv\"\n",
    "        TRAINING_FILE = BASE_PATH  / \"input\" / \"tweet-sentiment-extraction\" / \"train.csv\"\n",
    "        TESTING_FILE = BASE_PATH  / \"input\" / \"tweet-sentiment-extraction\" / \"test.csv\"\n",
    "        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"tweet-sentiment-extraction\" / \"sample_submission.csv\"\n",
    "        SUBMISSION_FILE = BASE_PATH / \"working\" / \"submission.csv\"\n",
    "        SLANG_FILE = BASE_PATH / \"input\" / \"slang-abbreviations\" / \"slang_abbreviations.csv\"\n",
    "        EMOJIS_FILE = BASE_PATH / \"input\" / \"slang-abbreviations\" / \"emojis.csv\"\n",
    "        \n",
    "    PRETRAINED_MODEL_DIR = BASE_PATH / \"input\" / \"xlnetbasecased\"\n",
    "    TOKENIZER = SentencePieceTokenizer(str(PRETRAINED_MODEL_DIR / 'xlnet-base-cased-spiece.model'))\n",
    "    SLANG_DICT = pd.read_csv(SLANG_FILE, header=None, names=['slang', 'normalised']).set_index('slang').to_dict()['normalised']\n",
    "    EMOJI_DICT = pd.read_csv(EMOJIS_FILE, header=None, names=['emoji', 'normalised']).set_index('emoji').to_dict()['normalised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "AciDwLjC8G77",
    "outputId": "03e6dba6-5740-4bd2-8552-dc6d85fd4c04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<cls>',\n",
       " '<sep>',\n",
       " '<pad>',\n",
       " '<mask>',\n",
       " '<eod>',\n",
       " '<eop>',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[config.TOKENIZER.sp.id_to_piece(x) for x in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "UMoIyYf39Zct",
    "outputId": "2c406e2c-415d-4662-844e-8449309ba52b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19036, 25976, 24734]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[config.TOKENIZER.sp.piece_to_id(x) for x in ['positive', 'negative', 'neutral']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hXHgCXG1wv2"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z6o-Di3ObeEK"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 24\n",
    "seed_everything(seed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "cg-Brrulbg8P",
    "outputId": "49bf153e-1f5e-437f-e8f1-a05ed79c976c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21984 5496\n",
      "21984 5496\n",
      "21984 5496\n",
      "21984 5496\n",
      "21984 5496\n"
     ]
    }
   ],
   "source": [
    "def create_train_folds():\n",
    "    df = pd.read_csv(config.TRAINING_FILE)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    df[\"kfold\"] = -1\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, random_state=seed)\n",
    "\n",
    "    for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.sentiment.values)):\n",
    "        print(len(trn_), len(val_))\n",
    "        df.loc[val_, 'kfold'] = fold\n",
    "\n",
    "    df.to_csv(config.FOLDED_TRAINING_FILE, index=False)\n",
    "\n",
    "create_train_folds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLmNjNd41wv3"
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    # https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, name):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, name)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, name)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, name):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), name)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ8fCbd41wv6"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUCcTAMr1wv7"
   },
   "outputs": [],
   "source": [
    "def update_char_map(char_map, char_pos_patterns, len_repl):\n",
    "    for ent in char_pos_patterns:\n",
    "        ent_pos = ent.span()\n",
    "        char_map[ent_pos[0]+1:] = [char_map[ent_pos[0]]+len_repl-1 if i<ent_pos[1] else char_map[i]-(ent_pos[1]-ent_pos[0])+len_repl for i in range(ent_pos[0]+1,len(char_map)) ]\n",
    "\n",
    "    return(char_map)\n",
    "\n",
    "\n",
    "def normalise_tweet(tweet, slang_dict):#, max_len):\n",
    "    \"\"\"\n",
    "    Clean tweet by decoding html character references, \n",
    "    replacing URLs, removing repeated characters and \n",
    "    replacing slang and emojis.\n",
    "    \"\"\"\n",
    "    char_map = list(range(len(tweet)))\n",
    "\n",
    "    # bugfix for one sample in train set. (a8048c2ff5)\n",
    "    clean_tweet = re.sub('&not', ' not', tweet)\n",
    "\n",
    "    # replace html references with corresponding unicode character (e.g. $amp: to &)\n",
    "    clean_tweet = html.unescape(clean_tweet)\n",
    "\n",
    "    # update character position mapping to reflect named character references \n",
    "    # ignores html character references that don't end with ';' to avoid double matches\n",
    "    char_pos_html_entities = []  \n",
    "    [char_pos_html_entities.extend(list(re.finditer('&'+k, tweet))) for k in html.entities.html5.keys() if ((k[-1]==';') or (k=='')) and (re.search('&'+k, tweet))]\n",
    "    char_map = update_char_map(char_map, char_pos_html_entities, 1)\n",
    "\n",
    "    # update character position mapping to reflect _numerical_ character references \n",
    "    char_pos_html_numerical_entities = []  \n",
    "    [char_pos_html_numerical_entities.extend(list(re.finditer('&#'+str(k), tweet))) for k in html.entities.codepoint2name.keys() if re.search('&#'+str(k), tweet)]\n",
    "    char_map = update_char_map(char_map, char_pos_html_numerical_entities, 1)\n",
    "\n",
    "    # bugfix for one sample in test set.\n",
    "    clean_tweet = re.sub('Â¡', 'Ai', clean_tweet)\n",
    "\n",
    "    # replace URLs with \"URL\"\n",
    "    clean_tweet = re.sub(r'(http://[^\\s-]*)', 'URL', clean_tweet)\n",
    "    \n",
    "    # update character position mapping to reflect this\n",
    "    if re.search(r'(http://[^\\s-]*)', tweet):\n",
    "    char_pos_urls = re.finditer(r'(http://[^\\s-]*)', tweet)\n",
    "    char_map = update_char_map(char_map, char_pos_urls, 3)\n",
    "\n",
    "    # replace unicode replacement character with \"'\"\n",
    "    clean_tweet = re.sub(r'(ï¿½)', \"'\", clean_tweet)\n",
    "    # update character position mapping to reflect this\n",
    "    if re.search(r'(ï¿½)', tweet):\n",
    "    char_pos_urls = re.finditer(r'(ï¿½)', tweet)\n",
    "    char_map = update_char_map(char_map, char_pos_urls, 1)\n",
    "\n",
    "    # replace letters or exclamation marks that are repeated >2 times consecutively (except \"www.\")\n",
    "    # with a single character (e.g. sorryyyyyy -> sorry)\n",
    "    # doesn't work perfectly, e.g. sleeeeeeep. -> slep.\n",
    "    clean_tweet = re.sub(r'(?!www.)([a-zA-Z\\!])\\1{2,}', '\\\\1', clean_tweet, flags=re.I)\n",
    "\n",
    "    # update character position mapping to reflect this\n",
    "    # note: urls are replaced with #s so that any repetitions within URL are ignored\n",
    "    char_pos_repeats = re.finditer(r'(?!www.)([a-zA-Z\\!])\\1{2,}', re.sub(r'(http://[^\\s-]*)|(ï¿½)', lambda x: '#'*len(x.group()), tweet), flags=re.I)\n",
    "    char_map = update_char_map(char_map, char_pos_repeats, 1)\n",
    "\n",
    "    # # replace slang abbreviations with real words\n",
    "    # # https://www.webopedia.com/quick_ref/textmessageabbreviations.asp\n",
    "    new_clean_tweet = ''\n",
    "    change = 0\n",
    "    for i,e in enumerate(re.finditer('([0-9a-zA-Z]+|[^0-9a-zA-Z]+)', clean_tweet)):\n",
    "\n",
    "    if e.group().upper() in config.SLANG_DICT.keys():\n",
    "        new_clean_tweet = new_clean_tweet + config.SLANG_DICT[e.group().upper()]\n",
    "\n",
    "        # 1. Find position of chunk to be replaced in 'clean_tweet'\n",
    "        # 2. Use that position to locate the corresponding chunk in the  char_map\n",
    "        # 3. Update that chunk and everything after it in char_map using the position that that chunk will have (not its position in clean_tweet)\n",
    "        len_repl = len(config.SLANG_DICT[e.group().upper()])\n",
    "        ent_pos = e.span()\n",
    "\n",
    "        ent_pos = tuple(x+change for x in ent_pos)\n",
    "\n",
    "        min_ix = min([j for j,x in enumerate(char_map) if x>=ent_pos[0]])\n",
    "        max_ix = max([j for j,x in enumerate(char_map) if x<ent_pos[1]])\n",
    "\n",
    "        char_map[min_ix+1:] = [ent_pos[0]+len_repl-1 if k<max_ix else char_map[k]-(ent_pos[1]-ent_pos[0])+len_repl for k in range(min_ix+1, len(char_map))]\n",
    "        change += len_repl - (ent_pos[1]-ent_pos[0])\n",
    "    else:\n",
    "        new_clean_tweet = new_clean_tweet + e.group()\n",
    "\n",
    "    clean_tweet = new_clean_tweet\n",
    "\n",
    "    # # replace emoticons with real words\n",
    "    # # https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "    new_clean_tweet = ''\n",
    "    change = 0\n",
    "    for i,e in enumerate(re.finditer('(\\s+|\\S+)', clean_tweet)):\n",
    "\n",
    "    if e.group().upper() in config.EMOJI_DICT.keys():\n",
    "        new_clean_tweet = new_clean_tweet + config.EMOJI_DICT[e.group().upper()]\n",
    "\n",
    "        # 1. Find position of chunk to be replaced in 'clean_tweet'\n",
    "        # 2. Use that position to locate the corresponding chunk in the  char_map\n",
    "        # 3. Update that chunk and everything after it in char_map using the position that that chunk will have (not its position in clean_tweet)\n",
    "        len_repl = len(config.EMOJI_DICT[e.group().upper()])\n",
    "        ent_pos = e.span()\n",
    "\n",
    "        ent_pos = tuple(x+change for x in ent_pos)\n",
    "\n",
    "        min_ix = min([j for j,x in enumerate(char_map) if x>=ent_pos[0]])\n",
    "        max_ix = max([j for j,x in enumerate(char_map) if x<ent_pos[1]])\n",
    "\n",
    "        char_map[min_ix+1:] = [ent_pos[0]+len_repl-1 if k<max_ix else char_map[k]-(ent_pos[1]-ent_pos[0])+len_repl for k in range(min_ix+1, len(char_map))]\n",
    "        change += len_repl - (ent_pos[1]-ent_pos[0])\n",
    "    else:\n",
    "        new_clean_tweet = new_clean_tweet + e.group()\n",
    "\n",
    "    clean_tweet = new_clean_tweet\n",
    "\n",
    "    #char_map = [min(x, max_len-1) for x in char_map]\n",
    "\n",
    "    char_map_inverse = (pd.Series([max([j for j,k in enumerate(char_map) if k==i], default=None) for i in range(len(clean_tweet))])\n",
    "    .fillna(method='backfill')\n",
    "    .fillna(len(tweet)-1)\n",
    "    .astype(int)\n",
    "    .values\n",
    "    .tolist())\n",
    "\n",
    "    return clean_tweet, char_map, char_map_inverse\n",
    "\n",
    "\n",
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict):\n",
    "    \"\"\"\n",
    "    Preprocessing the data to the XLNet model formatting\n",
    "    \"\"\"\n",
    "\n",
    "    raw_tweet = \" \" + \" \".join(str(tweet).split()) #tweet\n",
    "    raw_selected_text = \" \" + \" \".join(str(selected_text).split()) #selected_text\n",
    "\n",
    "    # find start and indices of selected_text in tweet\n",
    "    len_st = len(raw_selected_text) - 1\n",
    "    raw_idx0 = None\n",
    "    raw_idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(raw_tweet) if e == raw_selected_text[1]):\n",
    "        if \" \" + raw_tweet[ind: ind+len_st] == raw_selected_text:\n",
    "            raw_idx0 = ind\n",
    "            raw_idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    tweet, char_map, char_map_inverse = normalise_tweet(raw_tweet, slang_dict)#, max_len)\n",
    "\n",
    "    try:\n",
    "        idx0 = char_map[raw_idx0]\n",
    "        idx1 = char_map[raw_idx1]\n",
    "    except:\n",
    "        print('raw tweet: '+str(raw_tweet))\n",
    "        print('cleaned tweet: '+str(tweet))\n",
    "        print('rawidx0: '+str(raw_idx0))\n",
    "        print('idx0: '+str(idx0))\n",
    "        print('rawidx1: '+str(raw_idx1))\n",
    "        print('char_map: '+str(char_map))\n",
    "        print('len char_map: '+str(len(char_map)))\n",
    "        raise\n",
    "\n",
    "    selected_text = tweet[idx0:(idx1+1)]\n",
    "\n",
    "    try:\n",
    "        # create character mask for selected_text in tweet\n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for ct in range(idx0, idx1 + 1):\n",
    "                char_targets[ct] = 1\n",
    "    except:\n",
    "        print('raw tweet: '+str(raw_tweet))\n",
    "        print('cleaned tweet: '+str(tweet))\n",
    "        print('char_targets: '+str(char_targets))\n",
    "        print('char map: '+str(char_map))\n",
    "        print(len(char_map))\n",
    "        print(len(char_targets))\n",
    "        print('idx0: '+str(idx0))\n",
    "        print('idx1: '+str(idx1))\n",
    "        raise\n",
    "\n",
    "    tok_tweet = tokenizer.encode(tweet)\n",
    "\n",
    "    input_ids_orig = tok_tweet['ids']\n",
    "    tweet_offsets = tok_tweet['offsets']\n",
    "\n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "\n",
    "\n",
    "    try:\n",
    "        targets_start = target_idx[0]\n",
    "        targets_end = target_idx[-1]\n",
    "    except:\n",
    "        print(idx0)\n",
    "        print(idx1)\n",
    "        print(char_targets)\n",
    "        print(tweet)\n",
    "        print(selected_text)\n",
    "        print(target_idx)\n",
    "        raise\n",
    "\n",
    "    #######\n",
    "    sentiment_id = {\n",
    "        'positive': 19036,\n",
    "        'negative': 25976,\n",
    "        'neutral': 24734\n",
    "    }\n",
    "    #######\n",
    "\n",
    "    # https://huggingface.co/transformers/model_doc/xlnet.html#transformers.XLNetTokenizer.build_inputs_with_special_tokens\n",
    "    input_ids = [sentiment_id[sentiment]] + [4] + input_ids_orig + [4] + [3]\n",
    "    #input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0]*2 + [1] * (len(input_ids_orig)+1) + [2]\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 2 + tweet_offsets + [(0, 0)] * 2\n",
    "    targets_start += 2\n",
    "    targets_end += 2\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([5] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "\n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets,\n",
    "        'raw_tweet': raw_tweet,\n",
    "        'raw_selected_text': raw_selected_text,\n",
    "        'char_map_inverse': \"_\".join([str(x) for x in char_map_inverse]),\n",
    "        'char_map': str(char_map)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaLOn6TJao5y"
   },
   "outputs": [],
   "source": [
    "# tweet = \"&not gonna lie. it`s 60 degrees in here. thanks for leavin me your sweater molly. brrrrr\"\n",
    "# selected_text = 'thanks'\n",
    "\n",
    "# raw_tweet = \" \" + \" \".join(str(tweet).split()) #tweet\n",
    "# raw_selected_text = \" \" + \" \".join(str(selected_text).split()) #selected_text\n",
    "# print(raw_tweet)\n",
    "# print(raw_selected_text)\n",
    "# print(raw_selected_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6bEyOxMtq5z"
   },
   "outputs": [],
   "source": [
    "# tweet, char_map, char_map_inverse = normalise_tweet(raw_tweet, config.SLANG_DICT)#, max_len)\n",
    "# print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LD3dWEz1uVf4"
   },
   "outputs": [],
   "source": [
    "# tok_tweet = config.TOKENIZER.encode(tweet)\n",
    "\n",
    "# input_ids_orig = tok_tweet['ids']\n",
    "# tweet_offsets = tok_tweet['offsets']\n",
    "# print(tweet)\n",
    "# print([config.TOKENIZER.sp.id_to_piece(x) for x in input_ids_orig])\n",
    "# print(tweet_offsets)\n",
    "# print([tweet[o[0]:o[1]] for o in tweet_offsets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFNgb6lLQAmq"
   },
   "outputs": [],
   "source": [
    "\n",
    "# # find start and indices of selected_text in tweet\n",
    "# len_st = len(raw_selected_text) - 1\n",
    "# raw_idx0 = None\n",
    "# raw_idx1 = None\n",
    "\n",
    "# for ind in (i for i, e in enumerate(raw_tweet) if e == raw_selected_text[1]):\n",
    "#   if \" \" + raw_tweet[ind: ind+len_st] == raw_selected_text:\n",
    "#         raw_idx0 = ind\n",
    "#         raw_idx1 = ind + len_st - 1\n",
    "#         break\n",
    "# print(raw_idx0)\n",
    "# print(raw_idx1)\n",
    "# print(raw_tweet[ind: ind+len_st])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLsq4UJxQd1_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# list(enumerate(raw_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ga3_rDjD9uj"
   },
   "outputs": [],
   "source": [
    "# tweet = \"&not gonna lie. it`s 60 degrees in here. thanks for leavin me your sweater molly. brrrrr\"\n",
    "# selected_text = 'thanks'\n",
    "# sentiment = 'positive'\n",
    "# tokenizer = config.TOKENIZER\n",
    "# max_len = 128\n",
    "# slang_dict = config.SLANG_DICT\n",
    "\n",
    "# print(process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)['offsets'])\n",
    "# print([config.TOKENIZER.sp.id_to_piece(x) for x in process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)['ids']])\n",
    "# print(process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)['orig_tweet'])\n",
    "# print(process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)['raw_tweet'])\n",
    "\n",
    "\n",
    "\n",
    "#    char_map_inverse = [int(x) for x in char_map_inverse.split('_')]\n",
    "# raw_char_idx_start = char_map_inverse[int(offsets[idx_start][0])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-eRe4x11wv-"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s9Id3hW1wv-"
   },
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweets, sentiments, selected_texts):\n",
    "        self.tweets = tweets\n",
    "        self.sentiments = sentiments\n",
    "        self.selected_texts = selected_texts\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "        self.slang_dict = config.SLANG_DICT\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.tweets[item], \n",
    "            self.selected_texts[item], \n",
    "            self.sentiments[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len,\n",
    "            self.slang_dict\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_tweet\"],\n",
    "            'orig_selected': data[\"orig_selected\"],\n",
    "            'sentiment': data[\"sentiment\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long),\n",
    "            'raw_tweet': data[\"raw_tweet\"],\n",
    "            'raw_selected_text': data['raw_selected_text'],\n",
    "            'char_map_inverse': data[\"char_map_inverse\"],\n",
    "            'char_map': data[\"char_map\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXJ2VElJ1wwI"
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFNXnE3u1wwJ"
   },
   "outputs": [],
   "source": [
    "# def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "#     loss_fct = nn.CrossEntropyLoss()\n",
    "#     start_loss = loss_fct(start_logits, start_positions)\n",
    "#     end_loss = loss_fct(end_logits, end_positions)\n",
    "#     total_loss = (start_loss + end_loss)\n",
    "#     return total_loss\n",
    "\n",
    "def loss_fn(start_logprobs, end_logprobs, start_positions, end_positions):\n",
    "    loss_fct = nn.NLLLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCxxX5Jw1wwM"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NL1vcjOa1wwN"
   },
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # outputs_start, outputs_end = model(\n",
    "        #     ids=ids,\n",
    "        #     mask=mask,\n",
    "        #     token_type_ids=token_type_ids,\n",
    "        # )\n",
    "        # loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "        # loss.backward()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            start_positions=targets_start, \n",
    "            end_positions=targets_end\n",
    "        )\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        # outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        \n",
    "        # jaccard_scores = []\n",
    "        # for px, tweet in enumerate(orig_tweet):\n",
    "        #     selected_tweet = orig_selected[px]\n",
    "        #     tweet_sentiment = sentiment[px]\n",
    "        #     jaccard_score, _ = calculate_jaccard_score(\n",
    "        #         original_tweet=tweet,\n",
    "        #         target_string=selected_tweet,\n",
    "        #         sentiment_val=tweet_sentiment,\n",
    "        #         idx_start=np.argmax(outputs_start[px, :]),\n",
    "        #         idx_end=np.argmax(outputs_end[px, :]),\n",
    "        #         offsets=offsets[px]\n",
    "        #     )\n",
    "        #     jaccard_scores.append(jaccard_score)\n",
    "\n",
    "        # jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        tk0.set_postfix(loss=losses.avg)#, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jalt8e9B1wwQ"
   },
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chIyiuLB1wwR"
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    raw_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    char_map_inverse,\n",
    "    cleaned_tweet=None,\n",
    "    verbose=False):\n",
    "    \n",
    "    char_map_inverse = [int(x) for x in char_map_inverse.split('_')]\n",
    "\n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "        \n",
    "    # raw_char_idx_start = int(max([i for i,e in enumerate(char_map) if e<=offsets[idx_start][0]]+[0]))\n",
    "    # raw_char_idx_end = int(min([i for i,e in enumerate(char_map) if e>=offsets[idx_end][1]]+[len(char_map)]))\n",
    "\n",
    "    raw_char_idx_start = char_map_inverse[int(offsets[idx_start][0])]\n",
    "    try:\n",
    "        raw_char_idx_end = char_map_inverse[int(offsets[idx_end][1])-1]\n",
    "    except:\n",
    "        print('\\nraw tweet: '+str(raw_tweet))\n",
    "        print('cleaned tweet: '+str(cleaned_tweet))\n",
    "        print('char map: '+str(char_map_inverse))\n",
    "        print('index start:'+str(idx_start))\n",
    "        print('index end:'+str(idx_end))\n",
    "        print('offsets: '+str(offsets))\n",
    "        print(len(offsets))\n",
    "        print(len(char_map_inverse))\n",
    "        print(offsets[idx_end])\n",
    "        print(offsets[idx_end][1])\n",
    "        print(int(offsets[idx_end][1]))\n",
    "        print(char_map_inverse[int(offsets[idx_end][1])])\n",
    "        raise()\n",
    "\n",
    "    # filtered_output  = \"\"\n",
    "    # for ix in range(idx_start, idx_end + 1):\n",
    "    #     filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "\n",
    "    #     # add spacing to output\n",
    "    #     if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "    #         filtered_output += \" \"\n",
    "\n",
    "    filtered_output = raw_tweet[raw_char_idx_start:raw_char_idx_end+1]\n",
    "\n",
    "    if sentiment_val == \"neutral\" or len(raw_tweet.split()) < 2:\n",
    "        filtered_output = raw_tweet\n",
    "\n",
    "    if sentiment_val != \"neutral\" and verbose == True:\n",
    "        if filtered_output.strip().lower() != target_string.strip().lower():\n",
    "            print(\"********************************\")\n",
    "            print(f\"Output= {filtered_output.strip()}\")\n",
    "            print(f\"Target= {target_string.strip()}\")\n",
    "            print(f\"Tweet= {raw_tweet.strip()}\")\n",
    "            print(\"********************************\")\n",
    "\n",
    "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
    "    return jac, filtered_output\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"]\n",
    "            raw_tweet = d[\"raw_tweet\"]\n",
    "            raw_selected = d[\"raw_selected_text\"]\n",
    "            char_map_inverse = d[\"char_map_inverse\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "            #char_map_inverse = char_map_inverse.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                start_positions=targets_start,\n",
    "                end_positions=targets_end\n",
    "            )\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            # run it again to get the probabilities\n",
    "            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "            outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n",
    "            start_top_probs = outputs[0]  \n",
    "          \n",
    "            # start_top_probs contain those positions' probabilities\n",
    "            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n",
    "            start_top_index = outputs[1] \n",
    "\n",
    "            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "            # this can be verified by checking summation to unity\n",
    "            end_top_probs = outputs[2] \n",
    "            end_top_index = outputs[3] \n",
    "            \n",
    "            # calculate joint probability of start, end position tuples\n",
    "            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n",
    "\n",
    "            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n",
    "            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n",
    "            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n",
    "\n",
    "            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n",
    "                      indices]\n",
    "\n",
    "            # get (flat) position in sequence of highest probability tuple\n",
    "            top_start_end_probs_sorted = start_end_probs_sorted.argmax(dim=1)\n",
    "\n",
    "            # convert flat position to separate start and end positions\n",
    "            start_top_positions = (top_start_end_probs_sorted % torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n",
    "            end_top_positions = (top_start_end_probs_sorted // torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n",
    "            \n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(raw_tweet):\n",
    "                tweet_raw_selected_text = raw_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                tweet_offsets = offsets[px]\n",
    "                tweet_char_map_inverse = char_map_inverse[px]\n",
    "\n",
    "                start_top_position = start_top_positions[px]\n",
    "                end_top_position = end_top_positions[px]\n",
    "                \n",
    "                cleaned_tweet = orig_tweet[px]\n",
    "                \n",
    "                jaccard_score, _ = calculate_jaccard_score(\n",
    "                    raw_tweet=tweet,\n",
    "                    target_string=tweet_raw_selected_text,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=start_top_position,\n",
    "                    idx_end=end_top_position,\n",
    "                    offsets=tweet_offsets,\n",
    "                    char_map_inverse=tweet_char_map_inverse,\n",
    "                    cleaned_tweet=cleaned_tweet\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "    print(f\"Jaccard = {jaccards.avg}\")\n",
    "    print(f\"Loss = {losses.avg}\")\n",
    "    return jaccards.avg, losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWiozMAq1wwW"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el1CDathhxr8"
   },
   "outputs": [],
   "source": [
    "def init_model(config):\n",
    "    model_config = config.MODEL_CONFIG.from_pretrained(config.PRETRAINED_MODEL_DIR )#/ \"config.json\")\n",
    "    model_config.output_hidden_states = True\n",
    "    model_config.start_n_top = config.MAX_LEN\n",
    "    model_config.end_n_top = config.MAX_LEN\n",
    "    #'/kaggle/input/xlnet-base-tf/xlnet-base-cased'\n",
    "    model = config.MODEL.from_pretrained(config.PRETRAINED_MODEL_DIR, config=model_config)#, state_dict='/kaggle/input/xlnetmodel05081/model_3.bin')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0tHJ24G1wwW"
   },
   "outputs": [],
   "source": [
    "def run_fold(fold):\n",
    "\n",
    "    dfx = pd.read_csv(config.FOLDED_TRAINING_FILE)\n",
    "\n",
    "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TweetDataset(\n",
    "        tweet=df_train.text.values,\n",
    "        sentiment=df_train.sentiment.values,\n",
    "        selected_text=df_train.selected_text.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = TweetDataset(\n",
    "        tweet=df_valid.text.values,\n",
    "        sentiment=df_valid.sentiment.values,\n",
    "        selected_text=df_valid.selected_text.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # initialise model\n",
    "    model = init_model(config)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(patience=2, verbose=True)\n",
    "    print(f\"Training is Starting for fold={fold}\")\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "        jaccard, loss = eval_fn(valid_data_loader, model, device)\n",
    "        print(f\"Jaccard Score = {jaccard}\")\n",
    "        print(f\"Loss score = {loss}\")\n",
    "        es(loss, model, name=config.MODEL_PATH / f\"model_{fold}.bin\")\n",
    "        \n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "  \n",
    "    return es.val_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYeJuSDoaqXJ"
   },
   "outputs": [],
   "source": [
    "def run_val_fold(fold):\n",
    "\n",
    "    dfx = pd.read_csv(config.FOLDED_TRAINING_FILE)\n",
    "\n",
    "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TweetDataset(\n",
    "        tweet=df_train.text.values,\n",
    "        sentiment=df_train.sentiment.values,\n",
    "        selected_text=df_train.selected_text.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = TweetDataset(\n",
    "        tweet=df_valid.text.values,\n",
    "        sentiment=df_valid.sentiment.values,\n",
    "        selected_text=df_valid.selected_text.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print(f\"Evaluating fold={fold}\")\n",
    "\n",
    "    # initialise model\n",
    "    model = init_model(config)\n",
    "    model_filename = 'model_'+str(fold)+'.bin'\n",
    "    model.load_state_dict(torch.load(config.MODEL_PATH / model_filename, map_location=device))\n",
    "    model.to(device)\n",
    "    \n",
    "    jaccard, loss = eval_fn(valid_data_loader, model, device)\n",
    "\n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uAP683Z1wwZ"
   },
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUwMNkb1Hq3w"
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    if not os.path.exists(config.MODEL_PATH):\n",
    "    os.mkdir(config.MODEL_PATH)\n",
    "    val_loss = []\n",
    "    for ifold in [0]:#range(5):\n",
    "        q = run_fold(ifold)\n",
    "        val_loss.append(q)\n",
    "    print(f'Mean val loss: {np.mean(val_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JA7aV6JmajL4"
   },
   "outputs": [],
   "source": [
    "def get_cv_loss():\n",
    "    val_jaccard = []\n",
    "    for ifold in range(5):\n",
    "        q = run_val_fold(ifold)\n",
    "        val_jaccard.append(q)\n",
    "    print(f'Mean val loss: {np.mean(val_jaccard)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iyui81xMHLqQ"
   },
   "source": [
    "## Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ib6IORs6isgZ"
   },
   "outputs": [],
   "source": [
    "def predict_private_test():\n",
    "    # df_all = pd.read_csv(config.FULL_DATASET_FILE)#.head(32)\n",
    "    # df_train = pd.read_csv(config.TRAINING_FILE)\n",
    "    # df_test = pd.read_csv(config.TESTING_FILE)\n",
    "    # print(df_all.shape)\n",
    "    # print(df_train.shape)\n",
    "    # print(df_test.shape)\n",
    "\n",
    "    # df_private_test = (df_all\n",
    "    #                    .rename(columns={'content':'text'})\n",
    "    #                    .merge(df_train, on='text',how='left', suffixes=['', '_train'])\n",
    "    #                    .merge(df_test, on='text', how='left', suffixes=['', '_test'])\n",
    "    #                    .pipe(lambda x:x[x.sentiment_train.isnull() & x.sentiment_test.isnull()])\n",
    "    #                    .loc[:, ['sentiment', 'text']])\n",
    "    # print(df_private_test.shape)\n",
    "    df_private_test = pd.read_csv(config.PRIVATE_TEST).rename(columns={'content':'text'})\n",
    "    df_private_test.loc[:, \"sentiment\"] = 'positive'\n",
    "    df_private_test.loc[:, \"selected_text\"] = df_private_test.text.values\n",
    "\n",
    "    model = init_model(config)\n",
    "    model.load_state_dict(torch.load(config.MODEL_PATH / 'model_0.bin', map_location=device))\n",
    "    model.eval()\n",
    "    # ensure we get output probabilities for all combinations of start and end position\n",
    "    model.start_n_top = config.MAX_LEN\n",
    "    model.end_n_top = config.MAX_LEN\n",
    "    model.to(device) \n",
    "\n",
    "    test_dataset = TweetDataset(\n",
    "          tweet=df_private_test.text.values,\n",
    "          sentiment=df_private_test.sentiment.values,\n",
    "          selected_text=df_private_test.selected_text.values\n",
    "      )\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      shuffle=False,\n",
    "      batch_size=config.VALID_BATCH_SIZE,\n",
    "      num_workers=1\n",
    "    )\n",
    "\n",
    "    final_output = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "            raw_tweet = d[\"raw_tweet\"]\n",
    "            raw_selected = d[\"raw_selected_text\"]\n",
    "            char_map_inverse = d[\"char_map_inverse\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "            summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n",
    "\n",
    "            # run it again to get the probabilities\n",
    "            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "            outputs = model(\n",
    "              input_ids=ids,\n",
    "              attention_mask=mask,\n",
    "              token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n",
    "            sorted_start_probs = outputs[0]  \n",
    "\n",
    "            # start_top_probs contain those positions' probabilities\n",
    "            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n",
    "            sorted_start_index = outputs[1] \n",
    "\n",
    "            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "            # this can be verified by checking summation to unity\n",
    "            sorted_end_probs = outputs[2] \n",
    "            sorted_end_index = outputs[3] \n",
    "\n",
    "            # calculate joint probability of start, end position tuples\n",
    "            sorted_joint_probs = (sorted_start_probs.repeat(1, model.end_n_top)*sorted_end_probs)\n",
    "            top_joint_index = sorted_joint_probs.argmax(dim=1)\n",
    "\n",
    "            # convert flat position to separate start and end positions\n",
    "            top_end_index = sorted_end_index[torch.arange(sorted_end_index.shape[0]), top_joint_index]\n",
    "            top_start_index = sorted_start_index[torch.arange(sorted_start_index.shape[0]), top_joint_index % torch.tensor(config.MAX_LEN).to(device)]\n",
    "\n",
    "            for px, tweet in enumerate(raw_tweet):  \n",
    "              _, output_sentence = calculate_jaccard_score(\n",
    "                  raw_tweet=tweet,\n",
    "                  target_string=raw_selected[px],\n",
    "                  sentiment_val=sentiment[px],\n",
    "                  idx_start=top_start_index[px],\n",
    "                  idx_end=top_end_index[px],\n",
    "                  offsets=offsets[px],\n",
    "                  char_map_inverse=char_map_inverse[px],\n",
    "                  cleaned_tweet=orig_tweet[px]\n",
    "              )\n",
    "\n",
    "              final_output.append(output_sentence)\n",
    "\n",
    "\n",
    "    df_private_test['selected_text'] = final_output\n",
    "\n",
    "    return df_private_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzDhoB6Qy4zN"
   },
   "outputs": [],
   "source": [
    "# df_private_test = predict_private_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pW0jJHfUkzTs"
   },
   "outputs": [],
   "source": [
    " def predict_test():\n",
    "    df_test = pd.read_csv(config.TESTING_FILE)\n",
    "    df_test.loc[:, \"selected_text\"] = df_test.text.values\n",
    "\n",
    "    models = []\n",
    "\n",
    "    for mf in os.listdir(config.MODEL_PATH):\n",
    "    if not mf.endswith('.bin'):\n",
    "        continue\n",
    "    m = init_model(config)\n",
    "\n",
    "    m.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n",
    "    print(config.MODEL_PATH / mf)\n",
    "    m.eval()\n",
    "    # ensure we get output probabilities for all combinations of start and end position\n",
    "    m.start_n_top = config.MAX_LEN\n",
    "    m.end_n_top = config.MAX_LEN\n",
    "    m.to(device)\n",
    "\n",
    "    models.append(m)\n",
    "\n",
    "    test_dataset = TweetDataset(\n",
    "          tweet=df_test.text.values,\n",
    "          sentiment=df_test.sentiment.values,\n",
    "          selected_text=df_test.selected_text.values\n",
    "      )\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      shuffle=False,\n",
    "      batch_size=config.VALID_BATCH_SIZE,\n",
    "      num_workers=1\n",
    "    )\n",
    "\n",
    "    final_output = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"].numpy()\n",
    "            raw_tweet = d[\"raw_tweet\"]\n",
    "            raw_selected = d[\"raw_selected_text\"]\n",
    "            char_map_inverse = d[\"char_map_inverse\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "            summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n",
    "\n",
    "            for model in models: \n",
    "            # run it again to get the probabilities\n",
    "            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "            outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n",
    "            start_top_probs = outputs[0]  \n",
    "\n",
    "            # start_top_probs contain those positions' probabilities\n",
    "            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n",
    "            start_top_index = outputs[1] \n",
    "\n",
    "            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "            # this can be verified by checking summation to unity\n",
    "            end_top_probs = outputs[2] \n",
    "            end_top_index = outputs[3] \n",
    "\n",
    "            # calculate joint probability of start, end position tuples\n",
    "            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n",
    "\n",
    "            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n",
    "            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n",
    "            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n",
    "\n",
    "            #start_end_probs_sorted = start_end_probs[torch.arange(start_end_probs.shape[0]), indices]\n",
    "            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n",
    "                      indices]\n",
    "\n",
    "            summed_start_end_probs_sorted += start_end_probs_sorted\n",
    "\n",
    "        avg_start_end_probs_sorted = summed_start_end_probs_sorted/torch.tensor(len(models))\n",
    "\n",
    "        # get (flat) position in sequence of highest probability tuple\n",
    "        top_avg_start_end_probs_sorted = avg_start_end_probs_sorted.argmax(dim=1)\n",
    "\n",
    "        # convert flat position to separate start and end positions\n",
    "        start_top_positions = (top_avg_start_end_probs_sorted % torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n",
    "        end_top_positions = (top_avg_start_end_probs_sorted // torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n",
    "\n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(raw_tweet):\n",
    "            raw_selected_text = raw_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            tweet_offsets = offsets[px]\n",
    "            tweet_char_map_inverse = char_map_inverse[px]\n",
    "\n",
    "            start_top_position = start_top_positions[px]\n",
    "            end_top_position = end_top_positions[px]\n",
    "\n",
    "            cleaned_tweet = orig_tweet[px]\n",
    "\n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "              raw_tweet=tweet,\n",
    "              target_string=raw_selected_text,\n",
    "              sentiment_val=tweet_sentiment,\n",
    "              idx_start=start_top_position,\n",
    "              idx_end=end_top_position,\n",
    "              offsets=tweet_offsets,\n",
    "              char_map_inverse=tweet_char_map_inverse,\n",
    "              cleaned_tweet=cleaned_tweet,\n",
    "              verbose=True\n",
    "            )\n",
    "            final_output.append(output_sentence)\n",
    "\n",
    "\n",
    "    sample = pd.read_csv(config.SAMPLE_SUBMISSION_FILE)\n",
    "    sample.loc[:, 'selected_text'] = final_output\n",
    "    sample.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_mf6eF6D9vO"
   },
   "outputs": [],
   "source": [
    "def gen_probs_test():\n",
    "  df_test = pd.read_csv(config.TESTING_FILE)\n",
    "\n",
    "  models = []\n",
    "\n",
    "  for mf in os.listdir(config.MODEL_PATH):#[0:1]:\n",
    "    if not mf.endswith('.bin'):\n",
    "        continue\n",
    "    m = init_model(config)\n",
    "    \n",
    "    m.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n",
    "    print(config.MODEL_PATH / mf)\n",
    "    m.eval()\n",
    "    # ensure we get output probabilities for all combinations of start and end position\n",
    "    m.start_n_top = config.MAX_LEN#2#config.MAX_LEN\n",
    "    m.end_n_top = config.MAX_LEN#3#\n",
    "    m.to(device)\n",
    "\n",
    "    models.append(m)\n",
    "\n",
    "  test_dataset = TweetDataset(\n",
    "          tweet=df_test.text.values,\n",
    "          sentiment=df_test.sentiment.values,\n",
    "          selected_text=df_test.text.values\n",
    "      )\n",
    "\n",
    "  test_data_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      shuffle=False,\n",
    "      batch_size=config.VALID_BATCH_SIZE,\n",
    "      num_workers=1\n",
    "  )\n",
    "\n",
    "  final_output_start = []\n",
    "  final_output_end = []\n",
    "  final_tweets = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "      tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
    "      for bi, d in enumerate(tk0):\n",
    "          ids = d[\"ids\"]\n",
    "          token_type_ids = d[\"token_type_ids\"]\n",
    "          mask = d[\"mask\"]\n",
    "          sentiment = d[\"sentiment\"]\n",
    "          orig_selected = d[\"orig_selected\"]\n",
    "          orig_tweet = d[\"orig_tweet\"]\n",
    "          targets_start = d[\"targets_start\"]\n",
    "          targets_end = d[\"targets_end\"]\n",
    "          char_map_inverse = d[\"char_map_inverse\"]\n",
    "          offsets = d[\"offsets\"].numpy().tolist()\n",
    "          raw_tweet = d[\"raw_tweet\"]\n",
    "\n",
    "#           # convert char_maps from strings back to lists\n",
    "#           char_map = torch.tensor([eval(x) for x in char_map]).to(device, dtype=torch.long)\n",
    "        \n",
    "          ids = ids.to(device, dtype=torch.long)\n",
    "          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "          mask = mask.to(device, dtype=torch.long)\n",
    "          targets_start = targets_start.to(device, dtype=torch.long)\n",
    "          targets_end = targets_end.to(device, dtype=torch.long)\n",
    "          \n",
    "          summed_start_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN).to(device) # config.MAX_LEN\n",
    "          summed_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN).to(device)  # config.MAX_LEN*config.MAX_LEN\n",
    "\n",
    "          for model in models: \n",
    "            # run it again to get the probabilities\n",
    "            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "            outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability (for each sample)\n",
    "            start_top_probs = outputs[0]  \n",
    "            \n",
    "            # start_top_probs contain those positions' probabilities (for each sample)\n",
    "            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1] \n",
    "            start_top_index = outputs[1] \n",
    "            \n",
    "            # sort start_top_probs so that element (i,j) represents the probability for tweet i of character j being the start position\n",
    "            _, indices = torch.sort(start_top_index, dim=1)\n",
    "            start_top_probs_sorted = start_top_probs[torch.repeat_interleave(torch.arange(start_top_probs.shape[0]), start_top_probs.shape[1]).view(start_top_probs.shape),\n",
    "                      indices]\n",
    "\n",
    "            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "            # this can be verified by checking summation to unity\n",
    "            end_top_probs = outputs[2] \n",
    "            end_top_index = outputs[3] \n",
    "\n",
    "            # sort end_top_probs by position of element (rather than by its probability)\n",
    "            # resulting dimensions: n_sample, end_n_top, start_n_top\n",
    "            _, indices = torch.sort(end_top_index, dim=1)\n",
    "            end_top_probs_sorted = end_top_probs[torch.repeat_interleave(torch.arange(end_top_probs.shape[0]), end_top_probs.shape[1]).view(end_top_probs.shape),\n",
    "                      indices]\n",
    "            \n",
    "            # average the end position probabilities across start positions\n",
    "            end_top_probs_sorted = end_top_probs_sorted.view([end_top_probs_sorted.shape[0], model.end_n_top, model.start_n_top]).mean(dim=2)\n",
    "   \n",
    "            summed_start_probs_sorted += start_top_probs_sorted\n",
    "            summed_end_probs_sorted += end_top_probs_sorted\n",
    "\n",
    "          avg_start_probs_sorted = (summed_start_probs_sorted/torch.tensor(len(models))).cpu().detach().numpy()\n",
    "          avg_end_probs_sorted = (summed_end_probs_sorted/torch.tensor(len(models))).cpu().detach().numpy()\n",
    "                  \n",
    "          # convert starting and ending token probabilities to starting and ending character probabilities\n",
    "          for i, t in enumerate(raw_tweet):\n",
    "            start_char_probs = [0]*len(t)\n",
    "            end_char_probs = [0]*len(t)\n",
    "            inverse_map = [int(x) for x in char_map_inverse[i].split('_')]\n",
    "            for j,o in enumerate(offsets[i]):\n",
    "                if o==[0,0]: continue\n",
    "                try:\n",
    "                    start_char_probs[inverse_map[o[0]]] = avg_start_probs_sorted[i][j]\n",
    "                    end_char_probs[inverse_map[o[1]-1]] = avg_end_probs_sorted[i][j]\n",
    "                except:\n",
    "                    print('offsets: '+str(o))\n",
    "                    print('len(tweet):'+str(len(t)))\n",
    "                    print('len(start_char_probs): '+str(len(start_char_probs)))\n",
    "                    print('tweet: '+str(t))\n",
    "                    print('len(inverse_map): '+str(len(inverse_map)))\n",
    "                    print('segment: '+str(orig_tweet[o[0]:o[1]]))\n",
    "                    print(inverse_map[o[1]-1])\n",
    "                    print(avg_end_probs_sorted[i][j])\n",
    "                    raise()\n",
    "            \n",
    "            final_output_start.append(start_char_probs)\n",
    "            final_output_end.append(end_char_probs)\n",
    "          final_tweets.extend(raw_tweet)\n",
    "                 \n",
    "  df_test.loc[:, 'start_position_probs'] = final_output_start\n",
    "  df_test.loc[:, 'end_position_probs'] = final_output_end\n",
    "  df_test.loc[:, 'orig_tweet'] = final_tweets\n",
    "  df_test.to_csv(\"start_end_predictions.csv\", index=False)\n",
    "\n",
    "  return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2FdTWwID9vT"
   },
   "outputs": [],
   "source": [
    "def predict_test_for_voting():\n",
    "    \n",
    "    df_test = pd.read_csv(config.TESTING_FILE)\n",
    "    df_test.loc[:, \"selected_text\"] = df_test.text.values\n",
    "\n",
    "    test_dataset = TweetDataset(\n",
    "          tweet=df_test.text.values,\n",
    "          sentiment=df_test.sentiment.values,\n",
    "          selected_text=df_test.selected_text.values\n",
    "      )\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      shuffle=False,\n",
    "      batch_size=config.VALID_BATCH_SIZE,\n",
    "      num_workers=1\n",
    "    )\n",
    "    \n",
    "    preds_df = df_test.loc[:, ['textID']]\n",
    "    \n",
    "    for mf in os.listdir(config.MODEL_PATH):\n",
    "        if not mf.endswith('.bin'):\n",
    "            continue\n",
    "            \n",
    "        model = init_model(config)\n",
    "        model.load_state_dict(torch.load(config.MODEL_PATH / mf))\n",
    "        print(config.MODEL_PATH / mf)\n",
    "\n",
    "        model.eval()\n",
    "        # ensure we get output probabilities for all combinations of start and end position\n",
    "        model.start_n_top = config.MAX_LEN\n",
    "        model.end_n_top = config.MAX_LEN\n",
    "        model.to(device)\n",
    "        \n",
    "        final_output = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
    "\n",
    "            for bi, d in enumerate(tk0):\n",
    "                ids = d[\"ids\"]\n",
    "                token_type_ids = d[\"token_type_ids\"]\n",
    "                mask = d[\"mask\"]\n",
    "                sentiment = d[\"sentiment\"]\n",
    "                orig_selected = d[\"orig_selected\"]\n",
    "                orig_tweet = d[\"orig_tweet\"]\n",
    "                targets_start = d[\"targets_start\"]\n",
    "                targets_end = d[\"targets_end\"]\n",
    "                offsets = d[\"offsets\"].numpy()\n",
    "                raw_tweet = d[\"raw_tweet\"]\n",
    "                raw_selected = d[\"raw_selected_text\"]\n",
    "                char_map_inverse = d[\"char_map_inverse\"]\n",
    "        \n",
    "                ids = ids.to(device, dtype=torch.long)\n",
    "                token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "                mask = mask.to(device, dtype=torch.long)\n",
    "                targets_start = targets_start.to(device, dtype=torch.long)\n",
    "                targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "                summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n",
    "\n",
    "                # run it again to get the probabilities\n",
    "                # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "                outputs = model(\n",
    "                    input_ids=ids,\n",
    "                    attention_mask=mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "\n",
    "                # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n",
    "                sorted_start_probs = outputs[0]  \n",
    "\n",
    "                # start_top_probs contain those positions' probabilities\n",
    "                # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n",
    "                sorted_start_index = outputs[1] \n",
    "\n",
    "                # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "                # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "                # this can be verified by checking summation to unity\n",
    "                sorted_end_probs = outputs[2] \n",
    "                sorted_end_index = outputs[3] \n",
    "\n",
    "                # calculate joint probability of start, end position tuples\n",
    "                sorted_joint_probs = (sorted_start_probs.repeat(1, model.end_n_top)*sorted_end_probs)\n",
    "                top_joint_index = sorted_joint_probs.argmax(dim=1)\n",
    "                \n",
    "                # convert flat position to separate start and end positions\n",
    "                top_end_index = sorted_end_index[torch.arange(sorted_end_index.shape[0]), top_joint_index]\n",
    "                top_start_index = sorted_start_index[torch.arange(sorted_start_index.shape[0]), top_joint_index % torch.tensor(config.MAX_LEN).to(device)]\n",
    "                \n",
    "                for px, tweet in enumerate(raw_tweet):  \n",
    "                    _, output_sentence = calculate_jaccard_score(\n",
    "                        raw_tweet=tweet,\n",
    "                        target_string=raw_selected[px],\n",
    "                        sentiment_val=sentiment[px],\n",
    "                        idx_start=top_start_index[px],\n",
    "                        idx_end=top_end_index[px],\n",
    "                        offsets=offsets[px],\n",
    "                        char_map_inverse=char_map_inverse[px],\n",
    "                        cleaned_tweet=orig_tweet[px]\n",
    "                    )\n",
    "        \n",
    "                    final_output.append(output_sentence)\n",
    "\n",
    "        preds_df.loc[:, mf] = final_output\n",
    "\n",
    "    # reshape output\n",
    "    preds_df = preds_df.melt(id_vars = 'textID', var_name='model', value_name='selected_text')\n",
    "    \n",
    "    #preds_df.to_csv('predictions_voting.csv', index=False)\n",
    "    \n",
    "    return preds_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcRLGTjxV5Xi"
   },
   "outputs": [],
   "source": [
    "def predict_train(n_sample=None):\n",
    "  df_train = pd.read_csv(config.FOLDED_TRAINING_FILE)\n",
    "  \n",
    "  if n_sample:\n",
    "    df_train = df_train.sample(n_sample)\n",
    "  \n",
    "  df_train['prediction'] = df_train['text']\n",
    "  df_train['jac'] = 0\n",
    "\n",
    "  for mf in os.listdir(config.MODEL_PATH):\n",
    "    if not mf.endswith('.bin'):\n",
    "        continue\n",
    "    model = init_model(config)\n",
    "    \n",
    "    model.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n",
    "    print(config.MODEL_PATH / mf)\n",
    "    model.eval()\n",
    "    # ensure we get output probabilities for all combinations of start and end position\n",
    "    model.start_n_top = config.MAX_LEN\n",
    "    model.end_n_top = config.MAX_LEN\n",
    "    model.to(device)\n",
    "\n",
    "    fold = int(re.findall('model_(\\d).bin', mf)[0])\n",
    "    \n",
    "    if df_train.pipe(lambda x:x[x.kfold==fold]).shape[0]==0:\n",
    "      continue\n",
    "        \n",
    "    train_dataset = TweetDataset(\n",
    "            tweet=df_train.pipe(lambda x:x[x.kfold==fold]).text.values,\n",
    "            sentiment=df_train.pipe(lambda x:x[x.kfold==fold]).sentiment.values,\n",
    "            selected_text=df_train.pipe(lambda x:x[x.kfold==fold]).selected_text.values\n",
    "        )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=8, #config.VALID_BATCH_SIZE,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    final_output = []\n",
    "    final_jac = []\n",
    "\n",
    "    tk0 = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        offsets = d[\"offsets\"]\n",
    "        raw_tweet = d[\"raw_tweet\"]\n",
    "        raw_selected = d[\"raw_selected_text\"]\n",
    "        char_map_inverse = d[\"char_map_inverse\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        \n",
    "        # run it again to get the probabilities\n",
    "        # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n",
    "        sorted_start_probs = outputs[0]  \n",
    "\n",
    "        # start_top_probs contain those positions' probabilities\n",
    "        # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n",
    "        sorted_start_index = outputs[1] \n",
    "\n",
    "        # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "        # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "        # this can be verified by checking summation to unity\n",
    "        sorted_end_probs = outputs[2] \n",
    "        sorted_end_index = outputs[3] \n",
    "\n",
    "        # calculate joint probability of start, end position tuples\n",
    "        sorted_joint_probs = (sorted_start_probs.repeat(1, model.end_n_top)*sorted_end_probs)\n",
    "        top_joint_index = sorted_joint_probs.argmax(dim=1)\n",
    "        \n",
    "        # convert flat position to separate start and end positions\n",
    "        top_end_index = sorted_end_index[torch.arange(sorted_end_index.shape[0]), top_joint_index]\n",
    "        top_start_index = sorted_start_index[torch.arange(sorted_start_index.shape[0]), top_joint_index % torch.tensor(config.MAX_LEN).to(device)]\n",
    "        \n",
    "        for px, tweet in enumerate(raw_tweet):  \n",
    "            jac, output_sentence = calculate_jaccard_score(\n",
    "                raw_tweet=tweet,\n",
    "                target_string=raw_selected[px],\n",
    "                sentiment_val=sentiment[px],\n",
    "                idx_start=top_start_index[px],\n",
    "                idx_end=top_end_index[px],\n",
    "                offsets=offsets[px],\n",
    "                char_map_inverse=char_map_inverse[px],\n",
    "                cleaned_tweet=orig_tweet[px]\n",
    "            )\n",
    "\n",
    "            final_output.append(output_sentence)\n",
    "            final_jac.append(jac)\n",
    "    \n",
    "    del model, train_dataset, train_data_loader\n",
    "    gc.collect()\n",
    "\n",
    "    df_train.loc[df_train['kfold']==fold, 'prediction'] = final_output\n",
    "    df_train.loc[df_train['kfold']==fold, 'jac'] = final_jac\n",
    "\n",
    "  return df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "sy0b2gnexnQB",
    "outputId": "aefc3123-cae9-476e-dd64-518379c18586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "IN_KAGGLE_COMMIT = False\n",
    "if (not IN_COLAB) and ('runtime' not in get_ipython().config.IPKernelApp.connection_file):\n",
    "   IN_KAGGLE_COMMIT = True\n",
    "\n",
    "print(IN_KAGGLE_COMMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ub22h-3qxOIJ"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "53PezVmhPY7-",
    "outputId": "48e85594-9c08-46b4-b020-420a3a9f543a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0613_6/model_1.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 1/26 [00:10<04:31, 10.88s/it]\u001b[A\n",
      "  8%|▊         | 2/26 [00:22<04:29, 11.22s/it]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:33<04:16, 11.15s/it]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:45<04:08, 11.29s/it]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:56<03:55, 11.23s/it]\u001b[A\n",
      " 23%|██▎       | 6/26 [01:07<03:41, 11.09s/it]\u001b[A\n",
      " 27%|██▋       | 7/26 [01:18<03:29, 11.00s/it]\u001b[A\n",
      " 31%|███       | 8/26 [01:28<03:16, 10.91s/it]\u001b[A\n",
      " 35%|███▍      | 9/26 [01:39<03:04, 10.84s/it]\u001b[A\n",
      " 38%|███▊      | 10/26 [01:50<02:53, 10.83s/it]\u001b[A\n",
      " 42%|████▏     | 11/26 [02:01<02:42, 10.83s/it]\u001b[A\n",
      " 46%|████▌     | 12/26 [02:11<02:30, 10.77s/it]\u001b[A\n",
      " 50%|█████     | 13/26 [02:22<02:19, 10.76s/it]\u001b[A\n",
      " 54%|█████▍    | 14/26 [02:33<02:09, 10.77s/it]\u001b[A\n",
      " 58%|█████▊    | 15/26 [02:43<01:58, 10.73s/it]\u001b[A\n",
      " 62%|██████▏   | 16/26 [02:54<01:47, 10.72s/it]\u001b[A\n",
      " 65%|██████▌   | 17/26 [03:05<01:36, 10.69s/it]\u001b[A\n",
      " 69%|██████▉   | 18/26 [03:15<01:25, 10.69s/it]\u001b[A\n",
      " 73%|███████▎  | 19/26 [03:26<01:14, 10.68s/it]\u001b[A\n",
      " 77%|███████▋  | 20/26 [03:37<01:04, 10.68s/it]\u001b[A\n",
      " 81%|████████  | 21/26 [03:47<00:53, 10.67s/it]\u001b[A\n",
      " 85%|████████▍ | 22/26 [03:58<00:42, 10.65s/it]\u001b[A\n",
      " 88%|████████▊ | 23/26 [04:09<00:31, 10.65s/it]\u001b[A\n",
      " 92%|█████████▏| 24/26 [04:19<00:21, 10.66s/it]\u001b[A\n",
      " 96%|█████████▌| 25/26 [04:30<00:10, 10.65s/it]\u001b[A\n",
      "100%|██████████| 26/26 [04:41<00:00, 10.81s/it]\n",
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0613_6/model_2.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 1/26 [00:11<04:40, 11.24s/it]\u001b[A\n",
      "  8%|▊         | 2/26 [00:22<04:27, 11.17s/it]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:32<04:13, 11.03s/it]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:43<04:00, 10.93s/it]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:54<03:47, 10.83s/it]\u001b[A\n",
      " 23%|██▎       | 6/26 [01:04<03:35, 10.76s/it]\u001b[A\n",
      " 27%|██▋       | 7/26 [01:15<03:23, 10.73s/it]\u001b[A\n",
      " 31%|███       | 8/26 [01:26<03:12, 10.70s/it]\u001b[A\n",
      " 35%|███▍      | 9/26 [01:36<03:01, 10.69s/it]\u001b[A\n",
      " 38%|███▊      | 10/26 [01:47<02:50, 10.67s/it]\u001b[A\n",
      " 42%|████▏     | 11/26 [01:57<02:39, 10.64s/it]\u001b[A\n",
      " 46%|████▌     | 12/26 [02:08<02:29, 10.65s/it]\u001b[A\n",
      " 50%|█████     | 13/26 [02:19<02:18, 10.68s/it]\u001b[A\n",
      " 54%|█████▍    | 14/26 [02:30<02:08, 10.70s/it]\u001b[A\n",
      " 58%|█████▊    | 15/26 [02:40<01:57, 10.71s/it]\u001b[A\n",
      " 62%|██████▏   | 16/26 [02:52<01:49, 10.91s/it]\u001b[A\n",
      " 65%|██████▌   | 17/26 [03:03<01:38, 10.93s/it]\u001b[A\n",
      " 69%|██████▉   | 18/26 [03:14<01:28, 11.00s/it]\u001b[A\n",
      " 73%|███████▎  | 19/26 [03:25<01:17, 11.00s/it]\u001b[A\n",
      " 77%|███████▋  | 20/26 [03:36<01:05, 10.94s/it]\u001b[A\n",
      " 81%|████████  | 21/26 [03:47<00:54, 10.93s/it]\u001b[A\n",
      " 85%|████████▍ | 22/26 [03:57<00:43, 10.90s/it]\u001b[A\n",
      " 88%|████████▊ | 23/26 [04:08<00:32, 10.92s/it]\u001b[A\n",
      " 92%|█████████▏| 24/26 [04:19<00:21, 10.92s/it]\u001b[A\n",
      " 96%|█████████▌| 25/26 [04:30<00:10, 10.93s/it]\u001b[A\n",
      "100%|██████████| 26/26 [04:33<00:00, 10.52s/it]\n",
      "\n",
      "  0%|          | 0/28 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0613_6/model_3.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▎         | 1/28 [00:10<04:48, 10.69s/it]\u001b[A\n",
      "  7%|▋         | 2/28 [00:22<04:46, 11.01s/it]\u001b[A\n",
      " 11%|█         | 3/28 [00:33<04:36, 11.05s/it]\u001b[A\n",
      " 14%|█▍        | 4/28 [00:44<04:25, 11.08s/it]\u001b[A\n",
      " 18%|█▊        | 5/28 [00:55<04:13, 11.02s/it]\u001b[A\n",
      " 21%|██▏       | 6/28 [01:06<04:00, 10.95s/it]\u001b[A\n",
      " 25%|██▌       | 7/28 [01:17<03:49, 10.94s/it]\u001b[A\n",
      " 29%|██▊       | 8/28 [01:28<03:39, 10.96s/it]\u001b[A\n",
      " 32%|███▏      | 9/28 [01:39<03:27, 10.91s/it]\u001b[A\n",
      " 36%|███▌      | 10/28 [01:50<03:17, 10.95s/it]\u001b[A\n",
      " 39%|███▉      | 11/28 [02:01<03:05, 10.94s/it]\u001b[A\n",
      " 43%|████▎     | 12/28 [02:12<02:55, 10.96s/it]\u001b[A\n",
      " 46%|████▋     | 13/28 [02:22<02:43, 10.90s/it]\u001b[A\n",
      " 50%|█████     | 14/28 [02:33<02:32, 10.87s/it]\u001b[A\n",
      " 54%|█████▎    | 15/28 [02:44<02:20, 10.84s/it]\u001b[A\n",
      " 57%|█████▋    | 16/28 [02:55<02:10, 10.90s/it]\u001b[A\n",
      " 61%|██████    | 17/28 [03:06<02:00, 10.93s/it]\u001b[A\n",
      " 64%|██████▍   | 18/28 [03:17<01:49, 10.93s/it]\u001b[A\n",
      " 68%|██████▊   | 19/28 [03:28<01:38, 10.90s/it]\u001b[A\n",
      " 71%|███████▏  | 20/28 [03:39<01:27, 10.91s/it]\u001b[A\n",
      " 75%|███████▌  | 21/28 [03:50<01:16, 10.91s/it]\u001b[A\n",
      " 79%|███████▊  | 22/28 [04:00<01:05, 10.88s/it]\u001b[A\n",
      " 82%|████████▏ | 23/28 [04:11<00:54, 10.85s/it]\u001b[A\n",
      " 86%|████████▌ | 24/28 [04:22<00:43, 10.87s/it]\u001b[A\n",
      " 89%|████████▉ | 25/28 [04:33<00:32, 10.91s/it]\u001b[A\n",
      " 93%|█████████▎| 26/28 [04:44<00:21, 10.85s/it]\u001b[A\n",
      " 96%|█████████▋| 27/28 [04:55<00:10, 10.88s/it]\u001b[A\n",
      "100%|██████████| 28/28 [04:56<00:00, 10.60s/it]\n",
      "\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0613_6/model_4.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 1/24 [00:11<04:14, 11.06s/it]\u001b[A\n",
      "  8%|▊         | 2/24 [00:22<04:06, 11.19s/it]\u001b[A\n",
      " 12%|█▎        | 3/24 [00:33<03:54, 11.18s/it]\u001b[A\n",
      " 17%|█▋        | 4/24 [00:44<03:42, 11.11s/it]\u001b[A\n",
      " 21%|██        | 5/24 [00:55<03:29, 11.05s/it]\u001b[A\n",
      " 25%|██▌       | 6/24 [01:06<03:18, 11.00s/it]\u001b[A\n",
      " 29%|██▉       | 7/24 [01:17<03:05, 10.94s/it]\u001b[A\n",
      " 33%|███▎      | 8/24 [01:28<02:58, 11.13s/it]\u001b[A\n",
      " 38%|███▊      | 9/24 [01:39<02:46, 11.09s/it]\u001b[A\n",
      " 42%|████▏     | 10/24 [01:50<02:34, 11.06s/it]\u001b[A\n",
      " 46%|████▌     | 11/24 [02:01<02:22, 10.99s/it]\u001b[A\n",
      " 50%|█████     | 12/24 [02:12<02:11, 10.95s/it]\u001b[A\n",
      " 54%|█████▍    | 13/24 [02:23<02:01, 11.03s/it]\u001b[A\n",
      " 58%|█████▊    | 14/24 [02:34<01:49, 11.00s/it]\u001b[A\n",
      " 62%|██████▎   | 15/24 [02:45<01:38, 10.91s/it]\u001b[A\n",
      " 67%|██████▋   | 16/24 [02:56<01:26, 10.84s/it]\u001b[A\n",
      " 71%|███████   | 17/24 [03:06<01:15, 10.77s/it]\u001b[A\n",
      " 75%|███████▌  | 18/24 [03:17<01:04, 10.75s/it]\u001b[A\n",
      " 79%|███████▉  | 19/24 [03:28<00:53, 10.73s/it]\u001b[A\n",
      " 83%|████████▎ | 20/24 [03:38<00:42, 10.74s/it]\u001b[A\n",
      " 88%|████████▊ | 21/24 [03:49<00:32, 10.72s/it]\u001b[A\n",
      " 92%|█████████▏| 22/24 [04:00<00:21, 10.70s/it]\u001b[A\n",
      " 96%|█████████▌| 23/24 [04:10<00:10, 10.68s/it]\u001b[A\n",
      "100%|██████████| 24/24 [04:21<00:00, 10.89s/it]\n",
      "\n",
      "  0%|          | 0/26 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/kaggle/tweet_sentiment_extraction/model_save/model_0613_6/model_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  4%|▍         | 1/26 [00:11<04:38, 11.16s/it]\u001b[A\n",
      "  8%|▊         | 2/26 [00:22<04:26, 11.10s/it]\u001b[A\n",
      " 12%|█▏        | 3/26 [00:32<04:12, 10.96s/it]\u001b[A\n",
      " 15%|█▌        | 4/26 [00:43<03:59, 10.89s/it]\u001b[A\n",
      " 19%|█▉        | 5/26 [00:54<03:47, 10.81s/it]\u001b[A\n",
      " 23%|██▎       | 6/26 [01:04<03:35, 10.77s/it]\u001b[A\n",
      " 27%|██▋       | 7/26 [01:15<03:23, 10.74s/it]\u001b[A\n",
      " 31%|███       | 8/26 [01:26<03:13, 10.73s/it]\u001b[A\n",
      " 35%|███▍      | 9/26 [01:38<03:12, 11.32s/it]\u001b[A\n",
      " 38%|███▊      | 10/26 [01:52<03:14, 12.14s/it]\u001b[A\n",
      " 42%|████▏     | 11/26 [02:05<03:06, 12.41s/it]\u001b[A\n",
      " 46%|████▌     | 12/26 [02:17<02:49, 12.14s/it]\u001b[A\n",
      " 50%|█████     | 13/26 [02:28<02:32, 11.71s/it]\u001b[A\n",
      " 54%|█████▍    | 14/26 [02:38<02:16, 11.39s/it]\u001b[A\n",
      " 58%|█████▊    | 15/26 [02:49<02:03, 11.19s/it]\u001b[A\n",
      " 62%|██████▏   | 16/26 [03:00<01:50, 11.06s/it]\u001b[A\n",
      " 65%|██████▌   | 17/26 [03:11<01:38, 10.96s/it]\u001b[A\n",
      " 69%|██████▉   | 18/26 [03:21<01:26, 10.87s/it]\u001b[A\n",
      " 73%|███████▎  | 19/26 [03:32<01:15, 10.82s/it]\u001b[A\n",
      " 77%|███████▋  | 20/26 [03:43<01:04, 10.80s/it]\u001b[A\n",
      " 81%|████████  | 21/26 [03:53<00:53, 10.77s/it]\u001b[A\n",
      " 85%|████████▍ | 22/26 [04:04<00:43, 10.79s/it]\u001b[A\n",
      " 88%|████████▊ | 23/26 [04:15<00:32, 10.81s/it]\u001b[A\n",
      " 92%|█████████▏| 24/26 [04:26<00:21, 10.76s/it]\u001b[A\n",
      " 96%|█████████▌| 25/26 [04:36<00:10, 10.73s/it]\u001b[A\n",
      "100%|██████████| 26/26 [04:43<00:00, 10.92s/it]\n"
     ]
    }
   ],
   "source": [
    "preds_df = predict_train(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "3rwoxZhm6mz6",
    "outputId": "0275e523-fdea-4377-e866-5379c16239aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>kfold</th>\n",
       "      <th>prediction</th>\n",
       "      <th>jac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13709</th>\n",
       "      <td>6588cecae2</td>\n",
       "      <td>www.youtube.com/watch?v=6UrRxta8doM this was ...</td>\n",
       "      <td>www.youtube.com/watch?v=6UrRxta8doM this was t...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>www.youtube.com/watch?v=6UrRxta8doM this was ...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16294</th>\n",
       "      <td>0f4354b370</td>\n",
       "      <td>Entering twitter `lurk` mode, time to lock the...</td>\n",
       "      <td>Entering twitter `lurk` mode, time to lock the...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>Entering twitter `lurk` mode, time to lock th...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19527</th>\n",
       "      <td>7960586d30</td>\n",
       "      <td>But now it`s gone</td>\n",
       "      <td>But now it`s gone</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>But now it`s gone</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14424</th>\n",
       "      <td>41d28b37d5</td>\n",
       "      <td>Make sure ur plantin next to the water in th...</td>\n",
       "      <td>Make sure ur plantin next to the water in the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>Make sure ur plantin next to the water in the...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15725</th>\n",
       "      <td>bc63da3379</td>\n",
       "      <td>My apologies for the very impersonal #FF. Swam...</td>\n",
       "      <td>My apologies for the very impersonal #FF. Swam...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>My apologies for the very impersonal #FF. Swa...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14773</th>\n",
       "      <td>408c475b5a</td>\n",
       "      <td>Good morning!! Gonna clean the house a bit, th...</td>\n",
       "      <td>Good morning!! Gonna clean the house a bit, th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>Good morning!! Gonna clean the house a bit, t...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20206</th>\n",
       "      <td>39af08ad27</td>\n",
       "      <td>Gotta get up early tomorrow...gotta be at work...</td>\n",
       "      <td>Gotta get up early tomorrow...gotta be at work...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3</td>\n",
       "      <td>Gotta get up early tomorrow...gotta be at wor...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15244</th>\n",
       "      <td>ac87ee4101</td>\n",
       "      <td>So much for having a fun day off at the Brewer...</td>\n",
       "      <td>So much for having a fun day off at the Brewer...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>So much for having a fun day off at the Brewe...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23705</th>\n",
       "      <td>fa042d9ad5</td>\n",
       "      <td>Can you get me a sub from subway when ur on yo...</td>\n",
       "      <td>Can you get me a sub from subway when ur on yo...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>4</td>\n",
       "      <td>Can you get me a sub from subway when ur on y...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7521</th>\n",
       "      <td>05629e13d6</td>\n",
       "      <td>love you loads hun ) hot weather gutted i ain...</td>\n",
       "      <td>hot weather gutted i aint with you now chillax...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>love you loads hun ) hot weather gutted i ain...</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID  ...       jac\n",
       "13709  6588cecae2  ...  1.000000\n",
       "16294  0f4354b370  ...  1.000000\n",
       "19527  7960586d30  ...  1.000000\n",
       "14424  41d28b37d5  ...  1.000000\n",
       "15725  bc63da3379  ...  1.000000\n",
       "...           ...  ...       ...\n",
       "14773  408c475b5a  ...  1.000000\n",
       "20206  39af08ad27  ...  1.000000\n",
       "15244  ac87ee4101  ...  1.000000\n",
       "23705  fa042d9ad5  ...  1.000000\n",
       "7521   05629e13d6  ...  0.722222\n",
       "\n",
       "[408 rows x 7 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.pipe(lambda x:x[x.sentiment=='neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjNyUtzUxqOM"
   },
   "outputs": [],
   "source": [
    "preds_df.to_csv(config.BASE_PATH / \"sample_train_preds_0615_7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "HNXwfpI8wavE",
    "outputId": "88ac4a7c-2516-4df1-b396-7a28e2be3e59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 7)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df.shape#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "WSqxmPyXYbhZ",
    "outputId": "f6736ccf-98e7-48a7-9086-68d0155452ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1374 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is Starting for fold=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1374/1374 [25:13<00:00,  1.10s/it, loss=1.09]\n",
      " 17%|█▋        | 58/344 [01:05<04:46,  1.00s/it, jaccard=0.71, loss=0.814]"
     ]
    }
   ],
   "source": [
    " %%time\n",
    " \n",
    "if IN_COLAB:\n",
    "    #run_training()\n",
    "\n",
    "if IN_KAGGLE_COMMIT:\n",
    "    #predict_test()\n",
    "    #gen_probs_test()\n",
    "    predict_test_for_voting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "SYN-zRF5v6FY",
    "outputId": "66fa562b-1608-4045-ce64-e83c18a74b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fold=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [10:26<00:00,  1.82s/it, jaccard=0.696, loss=0.817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard = 0.6963852003172368\n",
      "Loss = 0.8167823214284544\n",
      "Evaluating fold=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [10:29<00:00,  1.83s/it, jaccard=0.704, loss=0.823]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard = 0.7036242974222892\n",
      "Loss = 0.8227394022795831\n",
      "Evaluating fold=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [10:29<00:00,  1.83s/it, jaccard=0.703, loss=0.814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard = 0.7031660023647034\n",
      "Loss = 0.8143632163387199\n",
      "Evaluating fold=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [10:30<00:00,  1.83s/it, jaccard=0.704, loss=0.811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard = 0.7039190932883281\n",
      "Loss = 0.8110352138591472\n",
      "Evaluating fold=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 344/344 [10:29<00:00,  1.83s/it, jaccard=0.698, loss=0.839]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard = 0.6977646500076556\n",
      "Loss = 0.8390276559575677\n",
      "Mean val loss: 0.7009718486800426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_cv_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJg0fq5jlMZw"
   },
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv(config.TESTING_FILE).set_index(\"textID\")\n",
    "\n",
    "# sub_df = pd.read_csv(config.SUBMISSION_FILE).set_index(\"textID\")\n",
    "\n",
    "# # Everything not presented in the public set \n",
    "# # will take a value of the original text\n",
    "# test_df[\"selected_text\"] = test_df.text\n",
    "\n",
    "# # Get the public ids and assign them\n",
    "# public_idxs = sub_df.index.values\n",
    "# test_df.loc[public_idxs, \"selected_text\"] = sub_df.selected_text.values\n",
    "# test_df[[\"selected_text\"]].to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufcbBmS1QyJ_"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv(config.TRAINING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jA5rt_0GNan"
   },
   "outputs": [],
   "source": [
    "# output = predict_train()#1000)\n",
    "# output.to_csv(config.MODEL_PATH / 'train_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7M9-QbC5F87F"
   },
   "outputs": [],
   "source": [
    "# output = pd.read_csv(config.MODEL_PATH / 'train_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdXN4b5PBgaV"
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhIOFKnzQr-M"
   },
   "outputs": [],
   "source": [
    "# dfx = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "# train_dataset = TweetDataset(\n",
    "#     tweet=dfx.text.values,\n",
    "#     sentiment=dfx.sentiment.values,\n",
    "#     selected_text=dfx.selected_text.values\n",
    "# )\n",
    "\n",
    "# train_data_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=config.TRAIN_BATCH_SIZE,\n",
    "#     num_workers=4\n",
    "# )\n",
    "\n",
    "# words = []\n",
    "\n",
    "# for t in train_dataset:\n",
    "#   t_words = t['orig_tweet'].split()\n",
    "#   words += t_words\n",
    "\n",
    "# from collections import Counter\n",
    "# words_counter = Counter(words)\n",
    "# words_counter_df = pd.DataFrame.from_dict(words_counter, orient='index', columns=['count']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DP80xqhJRjLS"
   },
   "outputs": [],
   "source": [
    "# train_dataset[241]['orig_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzPAOyavYp66"
   },
   "outputs": [],
   "source": [
    "# fold = 0\n",
    "\n",
    "# dfx = pd.read_csv(config.TRAINING_FILE)\n",
    "\n",
    "# df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "# df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "# train_dataset = TweetDataset(\n",
    "#     tweet=df_train.text.values,\n",
    "#     sentiment=df_train.sentiment.values,\n",
    "#     selected_text=df_train.selected_text.values\n",
    "# )\n",
    "\n",
    "# train_data_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     batch_size=config.TRAIN_BATCH_SIZE,\n",
    "#     num_workers=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSopugv2e2ey"
   },
   "outputs": [],
   "source": [
    "# train_data_loader.dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VV0Wweg0fDQF"
   },
   "outputs": [],
   "source": [
    "# len(df_train.iloc[1,:].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxYgcR0qVNQu"
   },
   "outputs": [],
   "source": [
    "# tweet = \" We've just 16ï¿½C today&amp;cold wind..  Want it 2b like 25ï¿½ to 30ï¿½! I love hot weather! But I reaped the 1st strawberry yday!\"\n",
    "# clean_tweet = \" We've just 16ï¿½C today&cold wind..  Want it To be like 25ï¿½ to 30ï¿½! I love hot weather! But I reaped the 1st strawberry yday!\"\n",
    "# selected_text = \"We've just 16ï¿½C today&amp;cold wind..  Want it 2b like 25ï¿½ to 30ï¿½! I love hot weather! But I reaped the 1st strawberry yd\"\n",
    "# #char_map = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128]\n",
    "# len(clean_tweet)\n",
    "# #char_map_inverse = pd.Series([max([j for j,k in enumerate(char_map) if k==i], default=None) for i in range(len(clean_tweet))]).fillna(method='backfill').fillna(len(tweet)-1).astype(int).values.tolist()\n",
    "# #print([tweet[e] for e in char_map_inverse])\n",
    "# #print(len(tweet))\n",
    "# print(len(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phz0AqNEphWf"
   },
   "outputs": [],
   "source": [
    "# char_targets = list(range(len(tweet)))\n",
    "# char_targets = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UgYYG99piES"
   },
   "outputs": [],
   "source": [
    "# char_targets[200:210]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlYWaCx0Zn-v"
   },
   "outputs": [],
   "source": [
    "# char_map_inverse = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 51, 51, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130]\n",
    "# len(char_map_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEGLJGHFaliQ"
   },
   "outputs": [],
   "source": [
    "# print(df_valid.pipe(lambda x:x[x.text==\" We've just 16ï¿½C today&amp;cold wind..  Want it 2b like 25ï¿½ to 30ï¿½! I love hot weather! But I reaped the 1st strawberry yday!\"]).iloc[0].selected_text[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkgJmnXJt0e6"
   },
   "outputs": [],
   "source": [
    "# tweet, selected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFU0go8KadR9"
   },
   "outputs": [],
   "source": [
    "# #selected_text = \n",
    "# tweet = data['raw_tweet']\n",
    "# selected_text = data['raw_selected_text']\n",
    "# sentiment = 'neutral'\n",
    "# tokenizer = config.TOKENIZER\n",
    "# max_len = config.MAX_LEN\n",
    "# slang_dict = config.SLANG_DICT\n",
    "# data = process_data(tweet, selected_text, sentiment, tokenizer, max_len, slang_dict)\n",
    "# print(data)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "xlnet-model-0615-7.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
