{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "7jaLISF2magZ"
   },
   "source": [
    "Code adapted from:\n",
    "* https://www.youtube.com/watch?v=U51ranzJBpY [ TOKENISER ]\n",
    "* https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch [ TRAINING ]\n",
    "* https://www.kaggle.com/abhishek/roberta-inference-5-folds [ INFERENCE ] \n",
    "* https://www.kaggle.com/masterscrat/detect-if-notebook-is-running-interactively [ CHECK WHERE NOTEBOOK IS RUNNING ]\n",
    "* https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/141502 [ SUBMISSION ]\n",
    "\n",
    "Notes \n",
    "This model is a basic implementation of XLNet 0508_1 but  \n",
    "* Run with train_folds_2.csv, based on updated input data (June 2nd)\n",
    "* Seeding of random number generator\n",
    "* Generates prediction for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VjKIw2dP1wuj",
    "outputId": "215c9dda-2c75-4e1d-c44f-d4cd41524238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.9.0)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.86)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\r\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\r\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.7.0)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (3.11.4)\r\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.7/site-packages (from protobuf) (1.14.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf) (46.1.3.post20200325)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tokenizers\n",
    "!pip install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1RjWp8i3_R4y",
    "outputId": "30c3cb2b-6ba3-41b2-e756-161bc5b86c04"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    drive.mount('/content/drive')\n",
    "    !wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_pb2.py\n",
    "    !wget https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model\n",
    "\n",
    "\n",
    "\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    \n",
    "    import sys\n",
    "    sys.path.append('/kaggle/input/sentencepiece-pb2/')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "NBiazpTm1wvB"
   },
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vEBAM8Yn1wvC"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tokenizers\n",
    "import string\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import sentencepiece_pb2\n",
    "import random\n",
    "from sklearn import model_selection\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "76RJVzb_1wvF",
    "outputId": "2713cebc-6e1b-400c-9030-eed7b70f8628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "P0NlI3nJ6Kw4"
   },
   "outputs": [],
   "source": [
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, model_name):\n",
    "        self.sp = spm.SentencePieceProcessor()\n",
    "        self.sp.load(model_name)\n",
    "    \n",
    "    def encode(self, sentence):\n",
    "        spt = sentencepiece_pb2.SentencePieceText()\n",
    "        spt.ParseFromString(self.sp.encode_as_serialized_proto(sentence))\n",
    "        offsets = []\n",
    "        ids = []\n",
    "        for piece in spt.pieces:\n",
    "            ids.append(piece.id)\n",
    "            offsets.append((piece.begin, piece.end))\n",
    "        return {'ids' : ids,\n",
    "                'offsets' : offsets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UiUv9iHz1wvu"
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 32 #64\n",
    "    VALID_BATCH_SIZE =  16\n",
    "    EPOCHS = 10\n",
    "    \n",
    "    MODEL_CONFIG = transformers.XLNetConfig\n",
    "    MODEL = transformers.XLNetForQuestionAnswering\n",
    "    if IN_COLAB:\n",
    "        \n",
    "        BASE_PATH = Path.cwd() / \"drive\" / \"My Drive\" / \"kaggle\" / \"tweet_sentiment_extraction\"\n",
    "        PRETRAINED_MODEL_DIR = BASE_PATH / \"input\" / \"xlnetbasecased\"\n",
    "        TOKENIZER = SentencePieceTokenizer(str(PRETRAINED_MODEL_DIR / 'xlnet-base-cased-spiece.model'))\n",
    "        MODEL_PATH = BASE_PATH  / \"model_save\" / \"model_0602_2\"\n",
    "        FOLDED_TRAINING_FILE = BASE_PATH / \"input\" / \"train-5fold\" / \"train_folds.csv\"\n",
    "        TRAINING_FILE = BASE_PATH / \"input\" / \"train.csv\"\n",
    "        TESTING_FILE = BASE_PATH  / \"input\" / \"test.csv\"\n",
    "        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"sample_submission.csv\"\n",
    "        SUBMISSION_FILE = BASE_PATH / \"input\" / \"submission.csv\"\n",
    "    else:\n",
    "        BASE_PATH = Path('/kaggle')\n",
    "        PRETRAINED_MODEL_DIR = BASE_PATH / \"input\" / \"xlnetbasecased\"\n",
    "        TOKENIZER = SentencePieceTokenizer( str(PRETRAINED_MODEL_DIR / \"xlnet-base-cased-spiece.model\"))\n",
    "        MODEL_PATH = BASE_PATH  / \"input\" / \"xlnetmodel06022\"\n",
    "        FOLDED_TRAINING_FILE = BASE_PATH / \"working\" / \"train_folds.csv\"\n",
    "        TRAINING_FILE = BASE_PATH  / \"input\" / \"tweet-sentiment-extraction\" / \"train.csv\"\n",
    "        TESTING_FILE = BASE_PATH  / \"input\" / \"tweet-sentiment-extraction\" / \"test.csv\"\n",
    "        SAMPLE_SUBMISSION_FILE = BASE_PATH / \"input\" / \"tweet-sentiment-extraction\" / \"sample_submission.csv\"\n",
    "        SUBMISSION_FILE = BASE_PATH / \"working\" / \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AciDwLjC8G77",
    "outputId": "2b62904a-a7ea-4d01-823d-852b596d155d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '<cls>',\n",
       " '<sep>',\n",
       " '<pad>',\n",
       " '<mask>',\n",
       " '<eod>',\n",
       " '<eop>',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[config.TOKENIZER.sp.id_to_piece(x) for x in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UMoIyYf39Zct",
    "outputId": "c15021f0-0117-4d2b-a6f7-b9e5e506be51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19036, 25976, 24734]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[config.TOKENIZER.sp.piece_to_id(x) for x in ['positive', 'negative', 'neutral']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cbpVX6MIlynx"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EXBSMEFxl3be",
    "outputId": "5e313c31-bf8a-42e5-ed00-dd2f06198d55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21984 5496\n",
      "21984 5496\n",
      "21984 5496\n",
      "21984 5496\n",
      "21984 5496\n"
     ]
    }
   ],
   "source": [
    "def create_train_folds():\n",
    "    df = pd.read_csv(config.TRAINING_FILE)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    df[\"kfold\"] = -1\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, random_state=seed)\n",
    "\n",
    "    for fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.sentiment.values)):\n",
    "        print(len(trn_), len(val_))\n",
    "        df.loc[val_, 'kfold'] = fold\n",
    "\n",
    "    df.to_csv(config.FOLDED_TRAINING_FILE, index=False)\n",
    "\n",
    "create_train_folds()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "2hXHgCXG1wv2"
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wLmNjNd41wv3"
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    # https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, name):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, name)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, name)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, name):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), name)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "EZ8fCbd41wv6"
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MUCcTAMr1wv7"
   },
   "outputs": [],
   "source": [
    "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Preprocessing the data to the XLNet model formatting\n",
    "    \"\"\"\n",
    "    tweet = \" \" + \" \".join(str(tweet).split())\n",
    "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
    "\n",
    "    # find start and indices of selected_text in tweet\n",
    "    len_st = len(selected_text) - 1\n",
    "    idx0 = None\n",
    "    idx1 = None\n",
    "\n",
    "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
    "            idx0 = ind\n",
    "            idx1 = ind + len_st - 1\n",
    "            break\n",
    "\n",
    "    # create character mask for selected_text in tweet\n",
    "    char_targets = [0] * len(tweet)\n",
    "    if idx0 != None and idx1 != None:\n",
    "        for ct in range(idx0, idx1 + 1):\n",
    "            char_targets[ct] = 1\n",
    "\n",
    "    # replace unicode replacement character with \"***\" so that offset values are returned correctly\n",
    "    tok_tweet = tokenizer.encode(re.sub(r'(Â¡)', 'Ai', re.sub(r'(ï¿½)', \"@@@\", tweet)))\n",
    "    \n",
    "    input_ids_orig = tok_tweet['ids']\n",
    "    tweet_offsets = tok_tweet['offsets']\n",
    "    \n",
    "    target_idx = []\n",
    "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
    "        if sum(char_targets[offset1: offset2]) > 0:\n",
    "            target_idx.append(j)\n",
    "    \n",
    "    targets_start = target_idx[0]\n",
    "    targets_end = target_idx[-1]\n",
    "\n",
    "    #######\n",
    "    sentiment_id = {\n",
    "        'positive': 19036,\n",
    "        'negative': 25976,\n",
    "        'neutral': 24734\n",
    "    }\n",
    "    #######\n",
    "    \n",
    "    # https://huggingface.co/transformers/model_doc/xlnet.html#transformers.XLNetTokenizer.build_inputs_with_special_tokens\n",
    "    input_ids = [sentiment_id[sentiment]] + [4] + input_ids_orig + [4] + [3]\n",
    "    #input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
    "    token_type_ids = [0]*2 + [1] * (len(input_ids_orig)+1) + [2]\n",
    "    mask = [1] * len(token_type_ids)\n",
    "    tweet_offsets = [(0, 0)] * 2 + tweet_offsets + [(0, 0)] * 2\n",
    "    targets_start += 2\n",
    "    targets_end += 2\n",
    "\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([5] * padding_length)\n",
    "        mask = mask + ([0] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
    "    \n",
    "    return {\n",
    "        'ids': input_ids,\n",
    "        'mask': mask,\n",
    "        'token_type_ids': token_type_ids,\n",
    "        'targets_start': targets_start,\n",
    "        'targets_end': targets_end,\n",
    "        'orig_tweet': tweet,\n",
    "        'orig_selected': selected_text,\n",
    "        'sentiment': sentiment,\n",
    "        'offsets': tweet_offsets\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "y-eRe4x11wv-"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4s9Id3hW1wv-"
   },
   "outputs": [],
   "source": [
    "class TweetDataset:\n",
    "    def __init__(self, tweet, sentiment, selected_text):\n",
    "        self.tweet = tweet\n",
    "        self.sentiment = sentiment\n",
    "        self.selected_text = selected_text\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = process_data(\n",
    "            self.tweet[item], \n",
    "            self.selected_text[item], \n",
    "            self.sentiment[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            'orig_tweet': data[\"orig_tweet\"],\n",
    "            'orig_selected': data[\"orig_selected\"],\n",
    "            'sentiment': data[\"sentiment\"],\n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "GXJ2VElJ1wwI"
   },
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "hFNXnE3u1wwJ"
   },
   "outputs": [],
   "source": [
    "# def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "#     loss_fct = nn.CrossEntropyLoss()\n",
    "#     start_loss = loss_fct(start_logits, start_positions)\n",
    "#     end_loss = loss_fct(end_logits, end_positions)\n",
    "#     total_loss = (start_loss + end_loss)\n",
    "#     return total_loss\n",
    "\n",
    "def loss_fn(start_logprobs, end_logprobs, start_positions, end_positions):\n",
    "    loss_fct = nn.NLLLoss()\n",
    "    start_loss = loss_fct(start_logits, start_positions)\n",
    "    end_loss = loss_fct(end_logits, end_positions)\n",
    "    total_loss = (start_loss + end_loss)\n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "xCxxX5Jw1wwM"
   },
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NL1vcjOa1wwN"
   },
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for bi, d in enumerate(tk0):\n",
    "\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # outputs_start, outputs_end = model(\n",
    "        #     ids=ids,\n",
    "        #     mask=mask,\n",
    "        #     token_type_ids=token_type_ids,\n",
    "        # )\n",
    "        # loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
    "        # loss.backward()\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            start_positions=targets_start, \n",
    "            end_positions=targets_end\n",
    "        )\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
    "        # outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
    "        \n",
    "        # jaccard_scores = []\n",
    "        # for px, tweet in enumerate(orig_tweet):\n",
    "        #     selected_tweet = orig_selected[px]\n",
    "        #     tweet_sentiment = sentiment[px]\n",
    "        #     jaccard_score, _ = calculate_jaccard_score(\n",
    "        #         original_tweet=tweet,\n",
    "        #         target_string=selected_tweet,\n",
    "        #         sentiment_val=tweet_sentiment,\n",
    "        #         idx_start=np.argmax(outputs_start[px, :]),\n",
    "        #         idx_end=np.argmax(outputs_end[px, :]),\n",
    "        #         offsets=offsets[px]\n",
    "        #     )\n",
    "        #     jaccard_scores.append(jaccard_score)\n",
    "\n",
    "        # jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "        losses.update(loss.item(), ids.size(0))\n",
    "        tk0.set_postfix(loss=losses.avg)#, jaccard=jaccards.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "Jalt8e9B1wwQ"
   },
   "source": [
    "## Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "chIyiuLB1wwR"
   },
   "outputs": [],
   "source": [
    "def calculate_jaccard_score(\n",
    "    original_tweet, \n",
    "    target_string, \n",
    "    sentiment_val, \n",
    "    idx_start, \n",
    "    idx_end, \n",
    "    offsets,\n",
    "    verbose=False):\n",
    "    \n",
    "    if idx_end < idx_start:\n",
    "        idx_end = idx_start\n",
    "    \n",
    "    filtered_output  = \"\"\n",
    "    for ix in range(idx_start, idx_end + 1):\n",
    "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
    "\n",
    "        # add spacing to output\n",
    "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
    "            filtered_output += \" \"\n",
    "\n",
    "    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n",
    "        filtered_output = original_tweet\n",
    "\n",
    "    if sentiment_val != \"neutral\" and verbose == True:\n",
    "        if filtered_output.strip().lower() != target_string.strip().lower():\n",
    "            print(\"********************************\")\n",
    "            print(f\"Output= {filtered_output.strip()}\")\n",
    "            print(f\"Target= {target_string.strip()}\")\n",
    "            print(f\"Tweet= {original_tweet.strip()}\")\n",
    "            print(\"********************************\")\n",
    "\n",
    "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
    "    return jac, filtered_output\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    jaccards = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "        for bi, d in enumerate(tk0):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            sentiment = d[\"sentiment\"]\n",
    "            orig_selected = d[\"orig_selected\"]\n",
    "            orig_tweet = d[\"orig_tweet\"]\n",
    "            targets_start = d[\"targets_start\"]\n",
    "            targets_end = d[\"targets_end\"]\n",
    "            offsets = d[\"offsets\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets_start = targets_start.to(device, dtype=torch.long)\n",
    "            targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                start_positions=targets_start,\n",
    "                end_positions=targets_end\n",
    "            )\n",
    "            loss = outputs[0]\n",
    "            \n",
    "            # run it again to get the probabilities\n",
    "            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "            outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n",
    "            start_top_probs = outputs[0]  \n",
    "          \n",
    "            # start_top_probs contain those positions' probabilities\n",
    "            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n",
    "            start_top_index = outputs[1] \n",
    "\n",
    "            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "            # this can be verified by checking summation to unity\n",
    "            end_top_probs = outputs[2] \n",
    "            end_top_index = outputs[3] \n",
    "            \n",
    "            # calculate joint probability of start, end position tuples\n",
    "            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n",
    "\n",
    "            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n",
    "            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n",
    "            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n",
    "\n",
    "            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n",
    "                      indices]\n",
    "\n",
    "            # get (flat) position in sequence of highest probability tuple\n",
    "            top_start_end_probs_sorted = start_end_probs_sorted.argmax(dim=1)\n",
    "\n",
    "            # convert flat position to separate start and end positions\n",
    "            start_top_positions = (top_start_end_probs_sorted % torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n",
    "            end_top_positions = (top_start_end_probs_sorted // torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n",
    "            \n",
    "            jaccard_scores = []\n",
    "            for px, tweet in enumerate(orig_tweet):\n",
    "                selected_tweet = orig_selected[px]\n",
    "                tweet_sentiment = sentiment[px]\n",
    "                start_top_position = start_top_positions[px]\n",
    "                end_top_position = end_top_positions[px]\n",
    "            \n",
    "                jaccard_score, _ = calculate_jaccard_score(\n",
    "                    original_tweet=tweet,\n",
    "                    target_string=selected_tweet,\n",
    "                    sentiment_val=tweet_sentiment,\n",
    "                    idx_start=start_top_position,\n",
    "                    idx_end=end_top_position,\n",
    "                    offsets=offsets[px]\n",
    "                )\n",
    "                jaccard_scores.append(jaccard_score)\n",
    "\n",
    "            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n",
    "            losses.update(loss.item(), ids.size(0))\n",
    "            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n",
    "    \n",
    "    print(f\"Jaccard = {jaccards.avg}\")\n",
    "    print(f\"Loss = {losses.avg}\")\n",
    "    return jaccards.avg, losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "sWiozMAq1wwW"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "el1CDathhxr8"
   },
   "outputs": [],
   "source": [
    "def init_model(config):\n",
    "    model_config = config.MODEL_CONFIG.from_pretrained(config.PRETRAINED_MODEL_DIR )#/ \"config.json\")\n",
    "    model_config.output_hidden_states = True\n",
    "    model_config.start_n_top = config.MAX_LEN\n",
    "    model_config.end_n_top = config.MAX_LEN\n",
    "    #'/kaggle/input/xlnet-base-tf/xlnet-base-cased'\n",
    "    model = config.MODEL.from_pretrained(config.PRETRAINED_MODEL_DIR, config=model_config)#, state_dict='/kaggle/input/xlnetmodel05081/model_3.bin')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "v0tHJ24G1wwW"
   },
   "outputs": [],
   "source": [
    "def run_fold(fold):\n",
    "\n",
    "    dfx = pd.read_csv(config.FOLDED_TRAINING_FILE)\n",
    "\n",
    "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = TweetDataset(\n",
    "        tweet=df_train.text.values,\n",
    "        sentiment=df_train.sentiment.values,\n",
    "        selected_text=df_train.selected_text.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = TweetDataset(\n",
    "        tweet=df_valid.text.values,\n",
    "        sentiment=df_valid.sentiment.values,\n",
    "        selected_text=df_valid.selected_text.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # initialise model\n",
    "    model = init_model(config)\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(patience=2, verbose=True)\n",
    "    print(f\"Training is Starting for fold={fold}\")\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n",
    "        jaccard, loss = eval_fn(valid_data_loader, model, device)\n",
    "        print(f\"Jaccard Score = {jaccard}\")\n",
    "        print(f\"Loss score = {loss}\")\n",
    "        es(loss, model, name=config.MODEL_PATH / f\"model_{fold}.bin\")\n",
    "        \n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "  \n",
    "    return es.val_loss_min"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "2uAP683Z1wwZ"
   },
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mUwMNkb1Hq3w"
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "  if not os.path.exists(config.MODEL_PATH):\n",
    "    os.mkdir(config.MODEL_PATH)\n",
    "  val_loss = []\n",
    "  for ifold in range(2,5):\n",
    "      q = run_fold(ifold)\n",
    "      val_loss.append(q)\n",
    "  print(f'Mean val loss: {np.mean(val_loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "Iyui81xMHLqQ"
   },
   "source": [
    "## Predict test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pW0jJHfUkzTs"
   },
   "outputs": [],
   "source": [
    "def predict_test():\n",
    "  df_test = pd.read_csv(config.TESTING_FILE)\n",
    "  df_test.loc[:, \"selected_text\"] = df_test.text.values\n",
    "\n",
    "  models = []\n",
    "\n",
    "  for mf in os.listdir(config.MODEL_PATH):\n",
    "    m = init_model(config)\n",
    "    \n",
    "    m.load_state_dict(torch.load(config.MODEL_PATH / mf))\n",
    "    print(config.MODEL_PATH / mf)\n",
    "    m.eval()\n",
    "    # ensure we get output probabilities for all combinations of start and end position\n",
    "    m.start_n_top = config.MAX_LEN\n",
    "    m.end_n_top = config.MAX_LEN\n",
    "    m.to(device)\n",
    "\n",
    "    models.append(m)\n",
    "\n",
    "  test_dataset = TweetDataset(\n",
    "          tweet=df_test.text.values,\n",
    "          sentiment=df_test.sentiment.values,\n",
    "          selected_text=df_test.selected_text.values\n",
    "      )\n",
    "\n",
    "  test_data_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      shuffle=False,\n",
    "      batch_size=config.VALID_BATCH_SIZE,\n",
    "      num_workers=1\n",
    "  )\n",
    "\n",
    "  final_output = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "      tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
    "      for bi, d in enumerate(tk0):\n",
    "          ids = d[\"ids\"]\n",
    "          token_type_ids = d[\"token_type_ids\"]\n",
    "          mask = d[\"mask\"]\n",
    "          sentiment = d[\"sentiment\"]\n",
    "          orig_selected = d[\"orig_selected\"]\n",
    "          orig_tweet = d[\"orig_tweet\"]\n",
    "          targets_start = d[\"targets_start\"]\n",
    "          targets_end = d[\"targets_end\"]\n",
    "          offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "          ids = ids.to(device, dtype=torch.long)\n",
    "          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "          mask = mask.to(device, dtype=torch.long)\n",
    "          targets_start = targets_start.to(device, dtype=torch.long)\n",
    "          targets_end = targets_end.to(device, dtype=torch.long)\n",
    "          \n",
    "          summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n",
    "\n",
    "          for model in models: \n",
    "            # run it again to get the probabilities\n",
    "            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "            outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n",
    "            start_top_probs = outputs[0]  \n",
    "          \n",
    "            # start_top_probs contain those positions' probabilities\n",
    "            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n",
    "            start_top_index = outputs[1] \n",
    "\n",
    "            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "            # this can be verified by checking summation to unity\n",
    "            end_top_probs = outputs[2] \n",
    "            end_top_index = outputs[3] \n",
    "            \n",
    "            # calculate joint probability of start, end position tuples\n",
    "            start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n",
    "\n",
    "            # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n",
    "            mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n",
    "            _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n",
    "\n",
    "            #start_end_probs_sorted = start_end_probs[torch.arange(start_end_probs.shape[0]), indices]\n",
    "            start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n",
    "                      indices]\n",
    "\n",
    "            summed_start_end_probs_sorted += start_end_probs_sorted\n",
    "\n",
    "          avg_start_end_probs_sorted = summed_start_end_probs_sorted/torch.tensor(len(models))\n",
    "\n",
    "          # get (flat) position in sequence of highest probability tuple\n",
    "          top_avg_start_end_probs_sorted = avg_start_end_probs_sorted.argmax(dim=1)\n",
    "\n",
    "          # convert flat position to separate start and end positions\n",
    "          start_top_positions = (top_avg_start_end_probs_sorted % torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n",
    "          end_top_positions = (top_avg_start_end_probs_sorted // torch.tensor(config.MAX_LEN).to(device)).cpu().detach().numpy()\n",
    "          \n",
    "          jaccard_scores = []\n",
    "          for px, tweet in enumerate(orig_tweet):\n",
    "              selected_tweet = orig_selected[px]\n",
    "              tweet_sentiment = sentiment[px]\n",
    "              _, output_sentence = calculate_jaccard_score(\n",
    "                  original_tweet=tweet,\n",
    "                  target_string=selected_tweet,\n",
    "                  sentiment_val=tweet_sentiment,\n",
    "                  idx_start=start_top_positions[px],\n",
    "                  idx_end=end_top_positions[px],\n",
    "                  offsets=offsets[px],\n",
    "                  verbose=True\n",
    "              )\n",
    "              final_output.append(output_sentence)\n",
    "\n",
    "\n",
    "  sample = pd.read_csv(config.SAMPLE_SUBMISSION_FILE)\n",
    "  sample.loc[:, 'selected_text'] = final_output\n",
    "  sample.to_csv(\"predictions_voting.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_for_voting():\n",
    "    \n",
    "    df_test = pd.read_csv(config.TESTING_FILE)\n",
    "    df_test.loc[:, \"selected_text\"] = df_test.text.values\n",
    "\n",
    "    test_dataset = TweetDataset(\n",
    "          tweet=df_test.text.values,\n",
    "          sentiment=df_test.sentiment.values,\n",
    "          selected_text=df_test.selected_text.values\n",
    "      )\n",
    "\n",
    "    test_data_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      shuffle=False,\n",
    "      batch_size=config.VALID_BATCH_SIZE,\n",
    "      num_workers=1\n",
    "    )\n",
    "    \n",
    "    preds_df = df_test.loc[:, ['textID']]\n",
    "    \n",
    "    for mf in os.listdir(config.MODEL_PATH):\n",
    "        if not mf.endswith('.bin'):\n",
    "            continue\n",
    "            \n",
    "        model = init_model(config)\n",
    "        model.load_state_dict(torch.load(config.MODEL_PATH / mf))\n",
    "        print(config.MODEL_PATH / mf)\n",
    "\n",
    "        model.eval()\n",
    "        # ensure we get output probabilities for all combinations of start and end position\n",
    "        model.start_n_top = config.MAX_LEN\n",
    "        model.end_n_top = config.MAX_LEN\n",
    "        model.to(device)\n",
    "        \n",
    "        final_output = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
    "\n",
    "            for bi, d in enumerate(tk0):\n",
    "                ids = d[\"ids\"]\n",
    "                token_type_ids = d[\"token_type_ids\"]\n",
    "                mask = d[\"mask\"]\n",
    "                sentiment = d[\"sentiment\"]\n",
    "                orig_selected = d[\"orig_selected\"]\n",
    "                orig_tweet = d[\"orig_tweet\"]\n",
    "                targets_start = d[\"targets_start\"]\n",
    "                targets_end = d[\"targets_end\"]\n",
    "                offsets = d[\"offsets\"].numpy()\n",
    "\n",
    "                ids = ids.to(device, dtype=torch.long)\n",
    "                token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "                mask = mask.to(device, dtype=torch.long)\n",
    "                targets_start = targets_start.to(device, dtype=torch.long)\n",
    "                targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "                summed_start_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN*config.MAX_LEN).to(device)\n",
    "\n",
    "                # run it again to get the probabilities\n",
    "                # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "                outputs = model(\n",
    "                    input_ids=ids,\n",
    "                    attention_mask=mask,\n",
    "                    token_type_ids=token_type_ids\n",
    "                )\n",
    "\n",
    "                # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n",
    "                sorted_start_probs = outputs[0]  \n",
    "\n",
    "                # start_top_probs contain those positions' probabilities\n",
    "                # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n",
    "                sorted_start_index = outputs[1] \n",
    "\n",
    "                # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "                # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "                # this can be verified by checking summation to unity\n",
    "                sorted_end_probs = outputs[2] \n",
    "                sorted_end_index = outputs[3] \n",
    "\n",
    "                # calculate joint probability of start, end position tuples\n",
    "                sorted_joint_probs = (sorted_start_probs.repeat(1, model.end_n_top)*sorted_end_probs)\n",
    "                top_joint_index = sorted_joint_probs.argmax(dim=1)\n",
    "                \n",
    "                # convert flat position to separate start and end positions\n",
    "                top_end_index = sorted_end_index[torch.arange(sorted_end_index.shape[0]), top_joint_index]\n",
    "                top_start_index = sorted_start_index[torch.arange(sorted_start_index.shape[0]), top_joint_index % torch.tensor(config.MAX_LEN).to(device)]\n",
    "                \n",
    "                for px, tweet in enumerate(orig_tweet):\n",
    "                    _, output_sentence = calculate_jaccard_score(\n",
    "                        original_tweet=tweet,\n",
    "                        target_string=orig_selected[px],\n",
    "                        sentiment_val=sentiment[px],\n",
    "                        idx_start=top_start_index[px],\n",
    "                        idx_end=top_end_index[px],\n",
    "                        offsets=offsets[px],\n",
    "                        verbose=False\n",
    "                      )\n",
    "        \n",
    "        \n",
    "                    final_output.append(output_sentence)\n",
    "\n",
    "        preds_df.loc[:, mf] = final_output\n",
    "\n",
    "    # reshape output\n",
    "    preds_df = preds_df.melt(id_vars = 'textID', var_name='model', value_name='selected_text')\n",
    "    \n",
    "    preds_df.to_csv('predictions_voting.csv', index=False)\n",
    "    \n",
    "    return preds_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "WzGdGgpPp_Pi"
   },
   "outputs": [],
   "source": [
    "def predict_train(n_sample=None):\n",
    "  df_train = pd.read_csv(config.FOLDED_TRAINING_FILE)\n",
    "  \n",
    "  if n_sample:\n",
    "    df_train = df_train.sample(n_sample)\n",
    "  \n",
    "  final_output = []\n",
    "\n",
    "  for mf in os.listdir(config.MODEL_PATH):\n",
    "    if not mf.endswith('.bin'):\n",
    "      continue\n",
    "\n",
    "    model = init_model(config)\n",
    "    \n",
    "    model.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n",
    "    print(config.MODEL_PATH / mf)\n",
    "    model.eval()\n",
    "    # ensure we get output probabilities for all combinations of start and end position\n",
    "    model.start_n_top = config.MAX_LEN\n",
    "    model.end_n_top = config.MAX_LEN\n",
    "    model.to(device)\n",
    "\n",
    "    fold = int(re.findall('model_(\\d).bin', mf)[0])\n",
    "    \n",
    "    if df_train.pipe(lambda x:x[x.kfold==fold]).shape[0]==0:\n",
    "      continue\n",
    "    \n",
    "    train_dataset = TweetDataset(\n",
    "            tweet=df_train.pipe(lambda x:x[x.kfold==fold]).text.values,\n",
    "            sentiment=df_train.pipe(lambda x:x[x.kfold==fold]).sentiment.values,\n",
    "            selected_text=df_train.pipe(lambda x:x[x.kfold==fold]).selected_text.values\n",
    "        )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=8, #config.VALID_BATCH_SIZE,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    tk0 = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    for bi, d in enumerate(tk0):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        sentiment = d[\"sentiment\"]\n",
    "        orig_selected = d[\"orig_selected\"]\n",
    "        orig_tweet = d[\"orig_tweet\"]\n",
    "        targets_start = d[\"targets_start\"]\n",
    "        targets_end = d[\"targets_end\"]\n",
    "        offsets = d[\"offsets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets_start = targets_start.to(device, dtype=torch.long)\n",
    "        targets_end = targets_end.to(device, dtype=torch.long)\n",
    "\n",
    "        # run it again to get the probabilities\n",
    "        # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        # start_top_index contains the model.start_n_top highest probability starting sequence positions, in decreasing order of probability\n",
    "        start_top_probs = outputs[0]  \n",
    "      \n",
    "        # start_top_probs contain those positions' probabilities\n",
    "        # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1]\n",
    "        start_top_index = outputs[1] \n",
    "\n",
    "        # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end position  \n",
    "        # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "        # this can be verified by checking summation to unity\n",
    "        end_top_probs = outputs[2] \n",
    "        end_top_index = outputs[3] \n",
    "        \n",
    "        # calculate joint probability of start, end position tuples\n",
    "        start_end_probs = (start_top_probs.repeat(1, model.end_n_top)*end_top_probs)\n",
    "\n",
    "        # reshape so that probabilities are ordered by sequence position rather than probability so that we can combine with output of other models\n",
    "        mapping_to_flat_sequence_position = (end_top_index*torch.tensor(model.start_n_top)).add(start_top_index.repeat(1, model.end_n_top))\n",
    "        _, indices = torch.sort(mapping_to_flat_sequence_position, dim=1)\n",
    "\n",
    "        start_end_probs_sorted = start_end_probs[torch.repeat_interleave(torch.arange(start_end_probs.shape[0]), start_end_probs.shape[1]).view(start_end_probs.shape),\n",
    "                  indices]\n",
    "\n",
    "        # get (flat) position in sequence of highest probability tuple\n",
    "        top_start_end_probs_sorted = start_end_probs_sorted.argmax(dim=1)\n",
    "\n",
    "        # convert flat position to separate start and end positions\n",
    "        start_top_positions = (top_start_end_probs_sorted % torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n",
    "        end_top_positions = (top_start_end_probs_sorted // torch.tensor(config.MAX_LEN)).cpu().detach().numpy()\n",
    "        \n",
    "        jaccard_scores = []\n",
    "        for px, tweet in enumerate(orig_tweet):\n",
    "            selected_tweet = orig_selected[px]\n",
    "            tweet_sentiment = sentiment[px]\n",
    "            start_top_position = start_top_positions[px]\n",
    "            end_top_position = end_top_positions[px]\n",
    "            \n",
    "            _, output_sentence = calculate_jaccard_score(\n",
    "                original_tweet=tweet,\n",
    "                target_string=selected_tweet,\n",
    "                sentiment_val=tweet_sentiment,\n",
    "                idx_start=start_top_position,\n",
    "                idx_end=end_top_position,\n",
    "                offsets=offsets[px]\n",
    "            )\n",
    "            final_output.append({'text':tweet, 'prediction':output_sentence})\n",
    "     \n",
    "    del model, train_dataset, train_data_loader\n",
    "    gc.collect()\n",
    "\n",
    "  df_train = df_train.merge(pd.DataFrame(final_output), on='text', how='inner')\n",
    "\n",
    "  return df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_probs_test():\n",
    "  df_test = pd.read_csv(config.TESTING_FILE)#.pipe(lambda x: x[x.text.str.contains('long')])#.head(32)\n",
    "\n",
    "  models = []\n",
    "\n",
    "  for mf in os.listdir(config.MODEL_PATH):#[0:1]:\n",
    "    if not mf.endswith('.bin'):\n",
    "        continue\n",
    "    m = init_model(config)\n",
    "    \n",
    "    m.load_state_dict(torch.load(config.MODEL_PATH / mf, map_location=device))\n",
    "    print(config.MODEL_PATH / mf)\n",
    "    m.eval()\n",
    "    # ensure we get output probabilities for all combinations of start and end position\n",
    "    m.start_n_top = config.MAX_LEN#2#config.MAX_LEN\n",
    "    m.end_n_top = config.MAX_LEN#3#\n",
    "    m.to(device)\n",
    "\n",
    "    models.append(m)\n",
    "\n",
    "  test_dataset = TweetDataset(\n",
    "          tweet=df_test.text.values,\n",
    "          sentiment=df_test.sentiment.values,\n",
    "          selected_text=df_test.text.values\n",
    "      )\n",
    "\n",
    "  test_data_loader = torch.utils.data.DataLoader(\n",
    "      test_dataset,\n",
    "      shuffle=False,\n",
    "      batch_size=config.VALID_BATCH_SIZE,\n",
    "      num_workers=1\n",
    "  )\n",
    "\n",
    "  final_output_start = []\n",
    "  final_output_end = []\n",
    "  final_tweets = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "      tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
    "      for bi, d in enumerate(tk0):\n",
    "          ids = d[\"ids\"]\n",
    "          token_type_ids = d[\"token_type_ids\"]\n",
    "          mask = d[\"mask\"]\n",
    "          sentiment = d[\"sentiment\"]\n",
    "          orig_selected = d[\"orig_selected\"]\n",
    "          orig_tweet = d[\"orig_tweet\"]\n",
    "          targets_start = d[\"targets_start\"]\n",
    "          targets_end = d[\"targets_end\"]\n",
    "          offsets = d[\"offsets\"].numpy().tolist()\n",
    "\n",
    "          ids = ids.to(device, dtype=torch.long)\n",
    "          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "          mask = mask.to(device, dtype=torch.long)\n",
    "          targets_start = targets_start.to(device, dtype=torch.long)\n",
    "          targets_end = targets_end.to(device, dtype=torch.long)\n",
    "          \n",
    "          summed_start_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN).to(device) # config.MAX_LEN\n",
    "          summed_end_probs_sorted = torch.zeros(ids.shape[0], config.MAX_LEN).to(device)  # config.MAX_LEN*config.MAX_LEN\n",
    "\n",
    "          for model in models: \n",
    "            # run it again to get the probabilities\n",
    "            # https://huggingface.co/transformers/model_doc/xlnet.html#xlnetforquestionanswering\n",
    "            outputs = model(\n",
    "                input_ids=ids,\n",
    "                attention_mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "\n",
    "            # start_top_index contains the model.start_n_top highest probability starting token positions, in decreasing order of probability (for each sample)\n",
    "            start_top_probs = outputs[0]  \n",
    "            \n",
    "            # start_top_probs contain those token' probabilities (for each sample)\n",
    "            # the documentation claims that the values are log probabilities, which seems to be incorrect given that the values are in [0-1] \n",
    "            start_top_index = outputs[1] \n",
    "            \n",
    "            # sort start_top_probs so that element (i,j) represents the probability that token j is the start token of tweet i\n",
    "            _, indices = torch.sort(start_top_index, dim=1)\n",
    "            start_top_probs_sorted = start_top_probs[torch.repeat_interleave(torch.arange(start_top_probs.shape[0]), start_top_probs.shape[1]).view(start_top_probs.shape),\n",
    "                      indices]\n",
    "\n",
    "            # the i-th element of start_top_index, start_top_probs are associated with elements j*model.start_n_top+i (j=1...model.end_n_top) of end_top_index, end_top_probs, where j represents the j-th highest probability end token  \n",
    "            # and NOT with the i*END_N_TOP+j elements as used here https://github.com/huggingface/transformers/blob/master/src/transformers/data/metrics/squad_metrics.py#L639\n",
    "            # this can be verified by checking summation to unity\n",
    "            end_top_probs = outputs[2] \n",
    "            end_top_index = outputs[3] \n",
    "\n",
    "            # sort end_top_probs by position of element (rather than by its probability)\n",
    "            # resulting dimensions: n_sample, end_n_top, start_n_top\n",
    "            _, indices = torch.sort(end_top_index, dim=1)\n",
    "            end_top_probs_sorted = end_top_probs[torch.repeat_interleave(torch.arange(end_top_probs.shape[0]), end_top_probs.shape[1]).view(end_top_probs.shape),\n",
    "                      indices]\n",
    "            \n",
    "            # average the end position probabilities across start positions\n",
    "            end_top_probs_sorted = end_top_probs_sorted.view([end_top_probs_sorted.shape[0], model.end_n_top, model.start_n_top]).mean(dim=2)\n",
    "   \n",
    "            summed_start_probs_sorted += start_top_probs_sorted\n",
    "            summed_end_probs_sorted += end_top_probs_sorted\n",
    "\n",
    "          avg_start_probs_sorted = (summed_start_probs_sorted/torch.tensor(len(models))).cpu().detach().numpy().tolist()\n",
    "          avg_end_probs_sorted = (summed_end_probs_sorted/torch.tensor(len(models))).cpu().detach().numpy().tolist()\n",
    "          \n",
    "          # convert starting and ending token probabilities to starting and ending character probabilities\n",
    "          for i, t in enumerate(orig_tweet):\n",
    "            start_char_probs = [0]*len(t)\n",
    "            end_char_probs = [0]*len(t)\n",
    "            for j,o in enumerate(offsets[i]):\n",
    "                if o==[0,0]: continue\n",
    "                try:\n",
    "                    start_char_probs[o[0]] = avg_start_probs_sorted[i][j]\n",
    "                    end_char_probs[o[1]-1] = avg_end_probs_sorted[i][j]\n",
    "                except:\n",
    "                    print('offsets: '+str(o))\n",
    "                    print('len(tweet):'+str(len(t)))\n",
    "                    print('len(start_char_probs): '+str(len(start_char_probs)))\n",
    "                    print('tweet: '+str(t))\n",
    "                    print('segment: '+str(orig_tweet[o[0]:o[1]]))\n",
    "                    raise()\n",
    "                    \n",
    "            \n",
    "            final_output_start.append(start_char_probs)\n",
    "            final_output_end.append(end_char_probs)\n",
    "          final_tweets.extend(orig_tweet)\n",
    "            \n",
    "  df_test.loc[:, 'start_position_probs'] = final_output_start\n",
    "  df_test.loc[:, 'end_position_probs'] = final_output_end\n",
    "  df_test.loc[:, 'orig_tweet'] = final_tweets\n",
    "\n",
    "  df_test.to_csv('start_end_predictions.csv', index=False)\n",
    "\n",
    "  return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = gen_probs_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(df.tail(20).shape[0]):\n",
    "#     start_max = pd.Series(df.iloc[i]['start_position_probs']).idxmax()\n",
    "#     end_max = pd.Series(df.iloc[i]['end_position_probs']).idxmax()\n",
    "#     print(df.iloc[i]['orig_tweet'], df.iloc[i]['orig_tweet'][start_max:end_max+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "sy0b2gnexnQB",
    "outputId": "4a4e39dd-586a-4bfa-e0fb-d9af7eed996d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "IN_KAGGLE_COMMIT = False\n",
    "if (not IN_COLAB) and ('runtime' not in get_ipython().config.IPKernelApp.connection_file):\n",
    "   IN_KAGGLE_COMMIT = True\n",
    "\n",
    "\n",
    "print(IN_KAGGLE_COMMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/221 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlnetmodel06022/model_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:46<00:00,  4.70it/s]\n",
      "  0%|          | 0/221 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlnetmodel06022/model_2.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:46<00:00,  4.78it/s]\n",
      "  0%|          | 0/221 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlnetmodel06022/model_4.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:46<00:00,  4.78it/s]\n",
      "  0%|          | 0/221 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlnetmodel06022/model_1.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:46<00:00,  4.78it/s]\n",
      "  0%|          | 0/221 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/xlnetmodel06022/model_3.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 221/221 [00:46<00:00,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 49s, sys: 1min 23s, total: 4min 12s\n",
      "Wall time: 4min 19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " %%time\n",
    " \n",
    "if IN_COLAB:\n",
    "    run_training()\n",
    "\n",
    "if IN_KAGGLE_COMMIT:\n",
    "    #predict_test()\n",
    "    #gen_probs_test()\n",
    "    predict_test_for_voting()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
